{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "# data loader\n",
    "# kbinsdiscretizer\n",
    "# embeddings\n",
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 10\n",
    "n_heads = 4\n",
    "d_model = 64\n",
    "head_size = d_model//n_heads\n",
    "dropout = 0.3\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.read_csv('./data/train.csv', index_col=0)\n",
    "target = 'FloodProbability'\n",
    "features = [col for col in input.columns if col != target]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    input[features],\n",
    "    input[target],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = KBinsDiscretizer(\n",
    "    n_bins=n_embed,\n",
    "    encode='ordinal',\n",
    "    strategy='uniform',\n",
    "    subsample=None\n",
    ")\n",
    "\n",
    "train_disc = disc.fit_transform(X_train)\n",
    "train_tensor = torch.tensor(train_disc, dtype=torch.int32)\n",
    "val_disc = disc.transform(X_val)\n",
    "val_tensor = torch.tensor(val_disc, dtype=torch.int32)\n",
    "\n",
    "xs = {\n",
    "    'train':train_tensor,\n",
    "    'val':val_tensor\n",
    "}\n",
    "\n",
    "ys = {\n",
    "    'train':torch.tensor(y_train.values, dtype=torch.float32),\n",
    "   \n",
    "    'val':torch.tensor(y_val.values, dtype=torch.float32)\n",
    "}\n",
    "\n",
    "def get_batch(split):\n",
    "    assert split in ['train', 'val']\n",
    "    idx = torch.randint(len(xs[split]), (batch_size,))\n",
    "    x = xs[split][idx]\n",
    "    y = ys[split][idx]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_size)\n",
    "        self.query = nn.Linear(d_model, head_size)\n",
    "        self.value = nn.Linear(d_model, head_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        w = k @ q.transpose(-2, -1) * C**-0.5 # (B, T, T), multiply with C**-0.5 to ensure unit gaussian outputs\n",
    "        w = F.softmax(w, dim=-1) # (B, T, T)\n",
    "        w = self.dropout(w)\n",
    "        out = w @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, n_heads, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, head_size, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(head_size, n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(len(features), d_model)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(head_size, d_model, n_heads, dropout),\n",
    "            Block(head_size, d_model, n_heads, dropout),\n",
    "            Block(head_size, d_model, n_heads, dropout)\n",
    "        )\n",
    "        self.linear = nn.Linear(d_model*len(features), 1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        out = self.embed(x)\n",
    "        out = self.blocks(out).view(-1, d_model*len(features))\n",
    "        out = self.linear(out).squeeze()\n",
    "\n",
    "        if y == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.huber_loss(out, y)\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch('train')\n",
    "m = Model().to(device)\n",
    "m.train()\n",
    "optimizer = optim.AdamW(m.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, loss = m(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e842169b4134aada65fad8614a4a5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: loss 0.2738\n",
      "step 1001: loss 0.0916\n",
      "step 2001: loss 0.0460\n",
      "step 3001: loss 0.0309\n",
      "step 4001: loss 0.0233\n",
      "step 5001: loss 0.0187\n",
      "step 6001: loss 0.0157\n",
      "step 7001: loss 0.0135\n",
      "step 8001: loss 0.0118\n",
      "step 9001: loss 0.0105\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in tqdm(range(10000)):\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = m(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.step()\n",
    "    if i%1000==0:\n",
    "        tqdm.write(f\"step {i+1}: loss {np.mean(losses[-10000:]):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
