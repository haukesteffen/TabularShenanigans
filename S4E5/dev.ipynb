{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 20\n",
    "n_heads = 16\n",
    "d_model = 64\n",
    "head_size = d_model//n_heads\n",
    "dropout = 0.3\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.read_csv('./data/train.csv', index_col=0)\n",
    "target = 'FloodProbability'\n",
    "features = [col for col in input.columns if col != target]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    input[features],\n",
    "    input[target],\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = KBinsDiscretizer(\n",
    "    n_bins=n_embed,\n",
    "    encode='ordinal',\n",
    "    strategy='uniform',\n",
    "    subsample=None\n",
    ")\n",
    "\n",
    "train_disc = disc.fit_transform(X_train)\n",
    "train_tensor = torch.tensor(train_disc, dtype=torch.int32)\n",
    "val_disc = disc.transform(X_val)\n",
    "val_tensor = torch.tensor(val_disc, dtype=torch.int32)\n",
    "\n",
    "xs = {\n",
    "    'train':train_tensor,\n",
    "    'val':val_tensor\n",
    "}\n",
    "\n",
    "ys = {\n",
    "    'train':torch.tensor(y_train.values, dtype=torch.float32),  \n",
    "    'val':torch.tensor(y_val.values, dtype=torch.float32)\n",
    "}\n",
    "\n",
    "def get_batch(split):\n",
    "    assert split in ['train', 'val']\n",
    "    idx = torch.randint(len(xs[split]), (batch_size,))\n",
    "    x = xs[split][idx]\n",
    "    y = ys[split][idx]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_size)\n",
    "        self.query = nn.Linear(d_model, head_size)\n",
    "        self.value = nn.Linear(d_model, head_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        w = k @ q.transpose(-2, -1) * C**-0.5 # (B, T, T), multiply with C**-0.5 to ensure unit gaussian outputs\n",
    "        w = F.softmax(w, dim=-1) # (B, T, T)\n",
    "        w = self.dropout(w)\n",
    "        out = w @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, n_heads, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, head_size, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(head_size, n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(len(features), d_model)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(head_size, d_model, n_heads, dropout),\n",
    "            Block(head_size, d_model, n_heads, dropout),\n",
    "            Block(head_size, d_model, n_heads, dropout)\n",
    "        )\n",
    "        self.linear = nn.Linear(d_model*len(features), 1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        out = self.embed(x)\n",
    "        out = self.blocks(out).view(-1, d_model*len(features))\n",
    "        out = self.linear(out).squeeze()\n",
    "\n",
    "        if y == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.mse_loss(out, y)\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch('train')\n",
    "m = Model().to(device)\n",
    "m.train()\n",
    "optimizer = optim.AdamW(m.parameters(), lr=1e-3)\n",
    "lr_schedule = optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=0.95\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bcd84ecebb4fe796d0bea73fdd5d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: train loss 0.31809, val loss 1.73127, validation r2 score -618.62898, learning rate 0.0010000\n",
      "step 1001: train loss 0.01099, val loss 0.00513, validation r2 score -1.00038, learning rate 0.0009500\n",
      "step 2001: train loss 0.00106, val loss 0.00079, validation r2 score 0.69109, learning rate 0.0009025\n",
      "step 3001: train loss 0.00090, val loss 0.00079, validation r2 score 0.69110, learning rate 0.0008574\n",
      "step 4001: train loss 0.00083, val loss 0.00077, validation r2 score 0.70100, learning rate 0.0008145\n",
      "step 5001: train loss 0.00078, val loss 0.00076, validation r2 score 0.70381, learning rate 0.0007738\n",
      "step 6001: train loss 0.00066, val loss 0.00066, validation r2 score 0.74194, learning rate 0.0007351\n",
      "step 7001: train loss 0.00058, val loss 0.00060, validation r2 score 0.76636, learning rate 0.0006983\n",
      "step 8001: train loss 0.00059, val loss 0.00059, validation r2 score 0.77219, learning rate 0.0006634\n",
      "step 9001: train loss 0.00061, val loss 0.00062, validation r2 score 0.75974, learning rate 0.0006302\n",
      "step 10001: train loss 0.00053, val loss 0.00055, validation r2 score 0.78733, learning rate 0.0005987\n",
      "step 11001: train loss 0.00057, val loss 0.00057, validation r2 score 0.77588, learning rate 0.0005688\n",
      "step 12001: train loss 0.00051, val loss 0.00051, validation r2 score 0.80301, learning rate 0.0005404\n",
      "step 13001: train loss 0.00049, val loss 0.00051, validation r2 score 0.80205, learning rate 0.0005133\n",
      "step 14001: train loss 0.00050, val loss 0.00051, validation r2 score 0.79955, learning rate 0.0004877\n",
      "step 15001: train loss 0.00051, val loss 0.00053, validation r2 score 0.79469, learning rate 0.0004633\n",
      "step 16001: train loss 0.00048, val loss 0.00052, validation r2 score 0.79813, learning rate 0.0004401\n",
      "step 17001: train loss 0.00048, val loss 0.00052, validation r2 score 0.79878, learning rate 0.0004181\n",
      "step 18001: train loss 0.00048, val loss 0.00051, validation r2 score 0.80184, learning rate 0.0003972\n",
      "step 19001: train loss 0.00047, val loss 0.00051, validation r2 score 0.80314, learning rate 0.0003774\n",
      "step 20001: train loss 0.00046, val loss 0.00050, validation r2 score 0.80628, learning rate 0.0003585\n",
      "step 21001: train loss 0.00045, val loss 0.00049, validation r2 score 0.80791, learning rate 0.0003406\n",
      "step 22001: train loss 0.00045, val loss 0.00049, validation r2 score 0.80844, learning rate 0.0003235\n",
      "step 23001: train loss 0.00045, val loss 0.00050, validation r2 score 0.80625, learning rate 0.0003074\n",
      "step 24001: train loss 0.00044, val loss 0.00048, validation r2 score 0.81182, learning rate 0.0002920\n",
      "step 25001: train loss 0.00044, val loss 0.00047, validation r2 score 0.81700, learning rate 0.0002774\n",
      "step 26001: train loss 0.00043, val loss 0.00047, validation r2 score 0.81782, learning rate 0.0002635\n",
      "step 27001: train loss 0.00043, val loss 0.00046, validation r2 score 0.81869, learning rate 0.0002503\n",
      "step 28001: train loss 0.00043, val loss 0.00046, validation r2 score 0.81985, learning rate 0.0002378\n",
      "step 29001: train loss 0.00042, val loss 0.00045, validation r2 score 0.82434, learning rate 0.0002259\n",
      "step 30001: train loss 0.00042, val loss 0.00045, validation r2 score 0.82697, learning rate 0.0002146\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, val_r2_scores = [], [], []\n",
    "n_eval = 1000\n",
    "for i in tqdm(range(30001)):\n",
    "    m.train()\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = m(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    train_losses.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    m.eval()\n",
    "    x, y = get_batch('val')\n",
    "    with torch.no_grad():\n",
    "        logits, loss = m(x, y)\n",
    "        val_losses.append(loss.item())\n",
    "        score = r2_score(y.tolist(), logits.tolist())\n",
    "        val_r2_scores.append(score)\n",
    "    if i%n_eval==0:\n",
    "        tqdm.write(f\"step {i+1}: train loss {np.mean(train_losses[-n_eval:]):.5f}, val loss {np.mean(val_losses[-n_eval:]):.5f}, validation r2 score {np.mean(val_r2_scores[-n_eval:]):.5f}, learning rate {lr_schedule.get_last_lr()[0]:.7f}\")\n",
    "        lr_schedule.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
