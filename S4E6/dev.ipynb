{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from importlib import import_module\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Model_Parameters:\n",
    "    d_model: int = 64 # dimension of model\n",
    "    n_embed: int = 32 # dimension of embedding\n",
    "    n_heads: int = 8 # number of heads\n",
    "    head_size: int = n_embed//n_heads # head size\n",
    "    dropout: float = 0.3 # dropout rate\n",
    "    n_in_cat: int = 141 # number of categorical columns in input tensor\n",
    "    n_in_num: int = 18 # number of numberical columns in input tensor\n",
    "\n",
    "@dataclass\n",
    "class Optim_Parameters:\n",
    "    lr: float = 3e-4 # initial learning rate\n",
    "    gamma: float = 0.9999 # exponential learning rate decay gamma\n",
    "    wd: float = 0.1 # weight decay\n",
    "\n",
    "@dataclass\n",
    "class Train_Parameters:\n",
    "    batch_size: int = 32 # number of examples per batch\n",
    "    test_size: float = 0.1 # relative size of validation split\n",
    "    n_eval: int = 1000 # evaluate model performance every n_eval steps\n",
    "    max_steps: int = 5001 # maximum number of steps in training\n",
    "\n",
    "mparam = Model_Parameters()\n",
    "oparam = Optim_Parameters()\n",
    "tparam = Train_Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.read_csv('./data/train.csv', index_col=0)\n",
    "input = input.rename(columns={\n",
    "    'Marital status': 'c_marital_status',\n",
    "    'Application mode': 'c_application_mode',\n",
    "    'Application order': 'c_application_order',\n",
    "    'Course': 'c_course',\n",
    "    'Daytime/evening attendance': 'c_attendance',\n",
    "    'Previous qualification': 'c_qualification',\n",
    "    'Previous qualification (grade)': 'n_qualification',\n",
    "    'Nacionality': 'c_nationality',\n",
    "    \"Mother's qualification\": 'c_mqual',\n",
    "    \"Father's qualification\": 'c_fqual',\n",
    "    \"Mother's occupation\": 'c_mocup',\n",
    "    \"Father's occupation\": 'c_focup',\n",
    "    'Admission grade': 'n_grade',\n",
    "    'Displaced': 'c_displaced',\n",
    "    'Educational special needs': 'c_special_needs',\n",
    "    'Debtor': 'c_debtor',\n",
    "    'Tuition fees up to date': 'c_fees',\n",
    "    'Gender': 'c_gender',\n",
    "    'Scholarship holder': 'c_scholarship',\n",
    "    'Age at enrollment': 'n_age',\n",
    "    'International': 'c_international',\n",
    "    'Curricular units 1st sem (credited)': 'n_cu1cr',\n",
    "    'Curricular units 1st sem (enrolled)': 'n_cu1en',\n",
    "    'Curricular units 1st sem (evaluations)': 'n_cu1ev',\n",
    "    'Curricular units 1st sem (approved)': 'n_cu1ap',\n",
    "    'Curricular units 1st sem (grade)': 'n_cu1gr',\n",
    "    'Curricular units 1st sem (without evaluations)': 'n_cu1wo',\n",
    "    'Curricular units 2nd sem (credited)': 'n_cu2cr',\n",
    "    'Curricular units 2nd sem (enrolled)': 'n_cu2en',\n",
    "    'Curricular units 2nd sem (evaluations)': 'n_cu2ev',\n",
    "    'Curricular units 2nd sem (approved)': 'n_cu2ap',\n",
    "    'Curricular units 2nd sem (grade)': 'n_cu2gr',\n",
    "    'Curricular units 2nd sem (without evaluations)': 'n_cu2wo',\n",
    "    'Unemployment rate': 'n_unemployment_rate',\n",
    "    'Inflation rate': 'n_inflation_rate',\n",
    "    'GDP': 'n_gdp'\n",
    "    })\n",
    "target = 'Target'\n",
    "features = [col for col in input.columns if col != target]\n",
    "categorical_features = [f for f in features if f.startswith('c_')]\n",
    "numerical_features = [f for f in features if f.startswith('n_')]\n",
    "\n",
    "# remove categorical outliers\n",
    "for c in categorical_features:\n",
    "    temp = input[c].value_counts()/len(input)\n",
    "    below_cutoff = temp[len(temp)*temp<0.01]\n",
    "    if len(below_cutoff.index)>0:\n",
    "        print(f'dropping {len(input[input[c].isin(below_cutoff.index)])} records of category {c}')\n",
    "        input = input[~input[c].isin(below_cutoff.index)]\n",
    "\n",
    "# split train and validation data\n",
    "input_train, input_val, target_train, target_val = train_test_split(\n",
    "    input[features],\n",
    "    input[target],\n",
    "    test_size=tparam.test_size,\n",
    "    random_state=42,\n",
    "    stratify=input[target]\n",
    "    )\n",
    "\n",
    "# one-hot-encode categorical features\n",
    "ohe = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    "    )\n",
    "encoded_categorical_train_data = ohe.fit_transform(input_train[categorical_features])\n",
    "encoded_categorical_val_data = ohe.transform(input_val[categorical_features])\n",
    "encoded_categorical_feature_names = ohe.get_feature_names_out(input_train[categorical_features].columns)\n",
    "encoded_categorical_train_df = pd.DataFrame(encoded_categorical_train_data, columns=encoded_categorical_feature_names)\n",
    "encoded_categorical_val_df = pd.DataFrame(encoded_categorical_val_data, columns=encoded_categorical_feature_names)\n",
    "\n",
    "# scale numerical features\n",
    "disc = KBinsDiscretizer(\n",
    "    n_bins=mparam.n_embed,\n",
    "    encode='ordinal',\n",
    "    strategy='uniform',\n",
    "    subsample=None\n",
    ")\n",
    "discretized_numerical_train_data = disc.fit_transform(input_train[numerical_features])\n",
    "discretized_numerical_val_data = disc.transform(input_val[numerical_features])\n",
    "discretized_numerical_feature_names = disc.get_feature_names_out(input_train[numerical_features].columns)\n",
    "discretized_numerical_train_df = pd.DataFrame(discretized_numerical_train_data, columns=discretized_numerical_feature_names)\n",
    "discretized_numerical_val_df = pd.DataFrame(discretized_numerical_val_data, columns=discretized_numerical_feature_names)\n",
    "\n",
    "# merge categorical and numerical features\n",
    "X_train = pd.merge(encoded_categorical_train_df, discretized_numerical_train_df, left_index=True, right_index=True)\n",
    "X_val = pd.merge(encoded_categorical_val_df, discretized_numerical_val_df, left_index=True, right_index=True)\n",
    "\n",
    "# one-hot-encode target \n",
    "target_ohe = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    "    )\n",
    "encoded_train_target = target_ohe.fit_transform(pd.DataFrame(target_train))\n",
    "encoded_val_target = target_ohe.transform(pd.DataFrame(target_val))\n",
    "encoded_target_names = target_ohe.get_feature_names_out(pd.DataFrame(target_train).columns)\n",
    "y_train = pd.DataFrame(encoded_train_target, columns=encoded_target_names)\n",
    "y_val = pd.DataFrame(encoded_val_target, columns=encoded_target_names)\n",
    "\n",
    "print(f'{len(X_train)} data points in train set')\n",
    "print(f'{len(X_val)} data points in validation set')\n",
    "print(f'{len(X_train.columns)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = {\n",
    "    'train':torch.tensor(X_train.values, dtype=torch.int32),\n",
    "    'val':torch.tensor(X_val.values, dtype=torch.int32)\n",
    "}\n",
    "\n",
    "ys = {\n",
    "    'train':torch.tensor(y_train.values, dtype=torch.float32),  \n",
    "    'val':torch.tensor(y_val.values, dtype=torch.float32)\n",
    "}\n",
    "\n",
    "def get_batch(split):\n",
    "    assert split in ['train', 'val']\n",
    "    idx = torch.randint(len(xs[split]), (tparam.batch_size,))\n",
    "    x = xs[split][idx]\n",
    "    y = ys[split][idx]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mparam.n_in_cat = len(encoded_categorical_feature_names)\n",
    "mparam.n_in_num = len(discretized_numerical_feature_names)\n",
    "\n",
    "# instantiate model\n",
    "architecture = 'Transformer'\n",
    "module_name = f\"architectures.{architecture}\"\n",
    "module = import_module(module_name)\n",
    "model_class = getattr(module, architecture)\n",
    "m = model_class(mparam)\n",
    "m = m.to(device)\n",
    "#m = torch.compile(device)\n",
    "print(f'Number of params in {architecture} model: {sum(p.numel() for p in m.parameters())}')\n",
    "m.train()\n",
    "\n",
    "# define weight decaying parameters\n",
    "param_dict = {pn: p for pn, p in m.named_parameters()}\n",
    "param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "optim_groups = [\n",
    "    {'params': decay_params, 'weight_decay': oparam.wd},\n",
    "    {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "# instantiate optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=oparam.lr\n",
    "    )\n",
    "\n",
    "# instantiate learning rate schedule\n",
    "lr_schedule = optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=oparam.gamma\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_acc_scores = [], [], []\n",
    "print(f'training for {tparam.max_steps} steps, evaluating every {tparam.n_eval} steps.')\n",
    "print(f'number of batches in training set: {len(xs['train'])}, validation')\n",
    "for i in tqdm(range(tparam.max_steps)):\n",
    "    m.train()\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = m(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    train_losses.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    m.eval()\n",
    "    x, y = get_batch('val')\n",
    "    with torch.no_grad():\n",
    "        logits, loss = m(x, y)\n",
    "        val_losses.append(loss.item())\n",
    "        score = accuracy_score(y.tolist(), F.one_hot(torch.argmax(F.softmax(logits, dim=1), dim=1), num_classes=3).tolist())\n",
    "        val_acc_scores.append(score)\n",
    "    if i%tparam.n_eval==0:\n",
    "        tqdm.write(f\"step {i+1}: train loss {np.mean(train_losses[-tparam.n_eval:]):.5f}, val loss {np.mean(val_losses[-tparam.n_eval:]):.5f}, val acc {np.mean(val_acc_scores[-tparam.n_eval:]):.5f}, current lr {lr_schedule.get_last_lr()[0]:.7f}\")\n",
    "    lr_schedule.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = pd.read_csv('./data/test.csv', index_col=0)\n",
    "input_test = input_test.rename(columns={\n",
    "    'Marital status': 'c_marital_status',\n",
    "    'Application mode': 'c_application_mode',\n",
    "    'Application order': 'c_application_order',\n",
    "    'Course': 'c_course',\n",
    "    'Daytime/evening attendance': 'c_attendance',\n",
    "    'Previous qualification': 'c_qualification',\n",
    "    'Previous qualification (grade)': 'n_qualification',\n",
    "    'Nacionality': 'c_nationality',\n",
    "    \"Mother's qualification\": 'c_mqual',\n",
    "    \"Father's qualification\": 'c_fqual',\n",
    "    \"Mother's occupation\": 'c_mocup',\n",
    "    \"Father's occupation\": 'c_focup',\n",
    "    'Admission grade': 'n_grade',\n",
    "    'Displaced': 'c_displaced',\n",
    "    'Educational special needs': 'c_special_needs',\n",
    "    'Debtor': 'c_debtor',\n",
    "    'Tuition fees up to date': 'c_fees',\n",
    "    'Gender': 'c_gender',\n",
    "    'Scholarship holder': 'c_scholarship',\n",
    "    'Age at enrollment': 'n_age',\n",
    "    'International': 'c_international',\n",
    "    'Curricular units 1st sem (credited)': 'n_cu1cr',\n",
    "    'Curricular units 1st sem (enrolled)': 'n_cu1en',\n",
    "    'Curricular units 1st sem (evaluations)': 'n_cu1ev',\n",
    "    'Curricular units 1st sem (approved)': 'n_cu1ap',\n",
    "    'Curricular units 1st sem (grade)': 'n_cu1gr',\n",
    "    'Curricular units 1st sem (without evaluations)': 'n_cu1wo',\n",
    "    'Curricular units 2nd sem (credited)': 'n_cu2cr',\n",
    "    'Curricular units 2nd sem (enrolled)': 'n_cu2en',\n",
    "    'Curricular units 2nd sem (evaluations)': 'n_cu2ev',\n",
    "    'Curricular units 2nd sem (approved)': 'n_cu2ap',\n",
    "    'Curricular units 2nd sem (grade)': 'n_cu2gr',\n",
    "    'Curricular units 2nd sem (without evaluations)': 'n_cu2wo',\n",
    "    'Unemployment rate': 'n_unemployment_rate',\n",
    "    'Inflation rate': 'n_inflation_rate',\n",
    "    'GDP': 'n_gdp'\n",
    "    })\n",
    "\n",
    "\n",
    "# one-hot-encode categorical features\n",
    "encoded_categorical_test_data = ohe.transform(input_test[categorical_features])\n",
    "encoded_categorical_test_df = pd.DataFrame(encoded_categorical_test_data, columns=encoded_categorical_feature_names)\n",
    "\n",
    "# scale numerical features\n",
    "discretized_numerical_test_data = disc.transform(input_test[numerical_features])\n",
    "discretized_numerical_test_df = pd.DataFrame(discretized_numerical_test_data, columns=discretized_numerical_feature_names)\n",
    "\n",
    "# merge categorical and numerical features\n",
    "X_test = pd.merge(encoded_categorical_test_df, discretized_numerical_test_df, left_index=True, right_index=True)\n",
    "\n",
    "print(f'{len(X_test)} data points in test set')\n",
    "print(f'{len(X_test.columns)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "m.eval()\n",
    "for chunk in tqdm(range((len(X_test)//batch_size)+1)):\n",
    "    with torch.no_grad():\n",
    "        x = X_test[(batch_size*chunk):(batch_size*(chunk+1))]\n",
    "        x = torch.tensor(x.values, dtype=torch.int32).to(device)        \n",
    "        logits, _ = m(x)\n",
    "        pred += F.one_hot(torch.argmax(F.softmax(logits, dim=1), dim=1), num_classes=3).tolist()\n",
    "pd.DataFrame(target_ohe.inverse_transform(pred), index=input_test.index, columns=['Target']).to_csv('./data/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
