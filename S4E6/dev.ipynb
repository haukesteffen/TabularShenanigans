{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 30\n",
    "n_heads = 32\n",
    "d_model = 128\n",
    "head_size = d_model//n_heads\n",
    "dropout = 0.3\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 167 records of category c_marital_status\n",
      "dropping 16 records of category c_application_mode\n",
      "dropping 4 records of category c_application_order\n",
      "dropping 2 records of category c_course\n",
      "dropping 48 records of category c_qualification\n",
      "dropping 104 records of category c_nationality\n",
      "dropping 146 records of category c_mqual\n",
      "dropping 115 records of category c_fqual\n",
      "dropping 128 records of category c_mocup\n",
      "dropping 141 records of category c_focup\n",
      "dropping 279 records of category c_special_needs\n",
      "74614 data points in train set\n",
      "754 data points in validation set\n",
      "159 features\n"
     ]
    }
   ],
   "source": [
    "input = pd.read_csv('./data/train.csv', index_col=0)\n",
    "input = input.rename(columns={\n",
    "    'Marital status': 'c_marital_status',\n",
    "    'Application mode': 'c_application_mode',\n",
    "    'Application order': 'c_application_order',\n",
    "    'Course': 'c_course',\n",
    "    'Daytime/evening attendance': 'c_attendance',\n",
    "    'Previous qualification': 'c_qualification',\n",
    "    'Previous qualification (grade)': 'n_qualification',\n",
    "    'Nacionality': 'c_nationality',\n",
    "    \"Mother's qualification\": 'c_mqual',\n",
    "    \"Father's qualification\": 'c_fqual',\n",
    "    \"Mother's occupation\": 'c_mocup',\n",
    "    \"Father's occupation\": 'c_focup',\n",
    "    'Admission grade': 'n_grade',\n",
    "    'Displaced': 'c_displaced',\n",
    "    'Educational special needs': 'c_special_needs',\n",
    "    'Debtor': 'c_debtor',\n",
    "    'Tuition fees up to date': 'c_fees',\n",
    "    'Gender': 'c_gender',\n",
    "    'Scholarship holder': 'c_scholarship',\n",
    "    'Age at enrollment': 'n_age',\n",
    "    'International': 'c_international',\n",
    "    'Curricular units 1st sem (credited)': 'n_cu1cr',\n",
    "    'Curricular units 1st sem (enrolled)': 'n_cu1en',\n",
    "    'Curricular units 1st sem (evaluations)': 'n_cu1ev',\n",
    "    'Curricular units 1st sem (approved)': 'n_cu1ap',\n",
    "    'Curricular units 1st sem (grade)': 'n_cu1gr',\n",
    "    'Curricular units 1st sem (without evaluations)': 'n_cu1wo',\n",
    "    'Curricular units 2nd sem (credited)': 'n_cu2cr',\n",
    "    'Curricular units 2nd sem (enrolled)': 'n_cu2en',\n",
    "    'Curricular units 2nd sem (evaluations)': 'n_cu2ev',\n",
    "    'Curricular units 2nd sem (approved)': 'n_cu2ap',\n",
    "    'Curricular units 2nd sem (grade)': 'n_cu2gr',\n",
    "    'Curricular units 2nd sem (without evaluations)': 'n_cu2wo',\n",
    "    'Unemployment rate': 'n_unemployment_rate',\n",
    "    'Inflation rate': 'n_inflation_rate',\n",
    "    'GDP': 'n_gdp'\n",
    "    })\n",
    "target = 'Target'\n",
    "features = [col for col in input.columns if col != target]\n",
    "categorical_features = [f for f in features if f.startswith('c_')]\n",
    "numerical_features = [f for f in features if f.startswith('n_')]\n",
    "\n",
    "# remove categorical outliers\n",
    "for c in categorical_features:\n",
    "    temp = input[c].value_counts()/len(input)\n",
    "    below_cutoff = temp[len(temp)*temp<0.01]\n",
    "    if len(below_cutoff.index)>0:\n",
    "        print(f'dropping {len(input[input[c].isin(below_cutoff.index)])} records of category {c}')\n",
    "        input = input[~input[c].isin(below_cutoff.index)]\n",
    "\n",
    "# split train and validation data\n",
    "input_train, input_val, target_train, target_val = train_test_split(\n",
    "    input[features],\n",
    "    input[target],\n",
    "    test_size=0.01,\n",
    "    random_state=42,\n",
    "    stratify=input[target]\n",
    "    )\n",
    "\n",
    "# one-hot-encode categorical features\n",
    "ohe = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    "    )\n",
    "encoded_categorical_train_data = ohe.fit_transform(input_train[categorical_features])\n",
    "encoded_categorical_val_data = ohe.transform(input_val[categorical_features])\n",
    "encoded_categorical_feature_names = ohe.get_feature_names_out(input_train[categorical_features].columns)\n",
    "encoded_categorical_train_df = pd.DataFrame(encoded_categorical_train_data, columns=encoded_categorical_feature_names)\n",
    "encoded_categorical_val_df = pd.DataFrame(encoded_categorical_val_data, columns=encoded_categorical_feature_names)\n",
    "\n",
    "# scale numerical features\n",
    "disc = KBinsDiscretizer(\n",
    "    n_bins=n_embed,\n",
    "    encode='ordinal',\n",
    "    strategy='uniform',\n",
    "    subsample=None\n",
    ")\n",
    "discretized_numerical_train_data = disc.fit_transform(input_train[numerical_features])\n",
    "discretized_numerical_val_data = disc.transform(input_val[numerical_features])\n",
    "discretized_numerical_feature_names = disc.get_feature_names_out(input_train[numerical_features].columns)\n",
    "discretized_numerical_train_df = pd.DataFrame(discretized_numerical_train_data, columns=discretized_numerical_feature_names)\n",
    "discretized_numerical_val_df = pd.DataFrame(discretized_numerical_val_data, columns=discretized_numerical_feature_names)\n",
    "\n",
    "# merge categorical and numerical features\n",
    "X_train = pd.merge(encoded_categorical_train_df, discretized_numerical_train_df, left_index=True, right_index=True)\n",
    "X_val = pd.merge(encoded_categorical_val_df, discretized_numerical_val_df, left_index=True, right_index=True)\n",
    "\n",
    "# one-hot-encode target \n",
    "target_ohe = OneHotEncoder(\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    "    )\n",
    "encoded_train_target = target_ohe.fit_transform(pd.DataFrame(target_train))\n",
    "encoded_val_target = target_ohe.transform(pd.DataFrame(target_val))\n",
    "encoded_target_names = target_ohe.get_feature_names_out(pd.DataFrame(target_train).columns)\n",
    "y_train = pd.DataFrame(encoded_train_target, columns=encoded_target_names)\n",
    "y_val = pd.DataFrame(encoded_val_target, columns=encoded_target_names)\n",
    "\n",
    "print(f'{len(X_train)} data points in train set')\n",
    "print(f'{len(X_val)} data points in validation set')\n",
    "print(f'{len(X_train.columns)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = {\n",
    "    'train':torch.tensor(X_train.values, dtype=torch.int32),\n",
    "    'val':torch.tensor(X_val.values, dtype=torch.int32)\n",
    "}\n",
    "\n",
    "ys = {\n",
    "    'train':torch.tensor(y_train.values, dtype=torch.float32),  \n",
    "    'val':torch.tensor(y_val.values, dtype=torch.float32)\n",
    "}\n",
    "\n",
    "def get_batch(split):\n",
    "    assert split in ['train', 'val']\n",
    "    idx = torch.randint(len(xs[split]), (batch_size,))\n",
    "    x = xs[split][idx]\n",
    "    y = ys[split][idx]\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_model, head_size)\n",
    "        self.query = nn.Linear(d_model, head_size)\n",
    "        self.value = nn.Linear(d_model, head_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        w = k @ q.transpose(-2, -1) * C**-0.5 # (B, T, T), multiply with C**-0.5 to ensure unit gaussian outputs\n",
    "        w = F.softmax(w, dim=-1) # (B, T, T)\n",
    "        w = self.dropout(w)\n",
    "        out = w @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, n_heads, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, dropout) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, head_size, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(head_size, n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedcat = nn.Embedding(2, d_model)\n",
    "        self.embednum = nn.Embedding(n_embed, d_model)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(head_size, d_model, n_heads, dropout),\n",
    "            Block(head_size, d_model, n_heads, dropout),\n",
    "            Block(head_size, d_model, n_heads, dropout),\n",
    "            Block(head_size, d_model, n_heads, dropout)\n",
    "        )\n",
    "        self.linear = nn.Linear(d_model*(len(encoded_categorical_feature_names)+len(discretized_numerical_feature_names)), 3)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        xcat = x[:, :len(encoded_categorical_feature_names)]\n",
    "        xnum = x[:, len(encoded_categorical_feature_names):len(encoded_categorical_feature_names)+len(discretized_numerical_feature_names)]\n",
    "        ecat = self.embedcat(xcat)\n",
    "        enum = self.embednum(xnum)\n",
    "        out = torch.cat([ecat, enum], dim=1)\n",
    "        out = self.blocks(out).view(-1, d_model*(len(encoded_categorical_feature_names)+len(discretized_numerical_feature_names)))\n",
    "        out = self.linear(out).squeeze()\n",
    "\n",
    "        if y == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy_with_logits(out, y)\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "m = Model().to(device)\n",
    "m.train()\n",
    "\n",
    "# define weight decaying parameters\n",
    "param_dict = {pn: p for pn, p in m.named_parameters()}\n",
    "param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "optim_groups = [\n",
    "    {'params': decay_params, 'weight_decay': 0.1},\n",
    "    {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "# instantiate optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optim_groups,\n",
    "    lr=5e-4\n",
    "    )\n",
    "\n",
    "# instantiate learning rate schedule\n",
    "lr_schedule = optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=0.99\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ccc96f915342078be5514a97ee0241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: train loss 0.83305, val loss 2.80231, validation accuracy score 0.51562, current learning rate 0.0005000\n",
      "step 101: train loss 0.42741, val loss 0.38140, validation accuracy score 0.77758, current learning rate 0.0001830\n",
      "step 201: train loss 0.30297, val loss 0.27888, validation accuracy score 0.82875, current learning rate 0.0000670\n",
      "step 301: train loss 0.28962, val loss 0.27820, validation accuracy score 0.82281, current learning rate 0.0000245\n",
      "step 401: train loss 0.28807, val loss 0.27622, validation accuracy score 0.82711, current learning rate 0.0000090\n",
      "step 501: train loss 0.29018, val loss 0.27672, validation accuracy score 0.82563, current learning rate 0.0000033\n",
      "step 601: train loss 0.28846, val loss 0.27308, validation accuracy score 0.82719, current learning rate 0.0000012\n",
      "step 701: train loss 0.28921, val loss 0.26921, validation accuracy score 0.83320, current learning rate 0.0000004\n",
      "step 801: train loss 0.28918, val loss 0.27250, validation accuracy score 0.83078, current learning rate 0.0000002\n",
      "step 901: train loss 0.27987, val loss 0.27864, validation accuracy score 0.82586, current learning rate 0.0000001\n",
      "step 1001: train loss 0.28781, val loss 0.27129, validation accuracy score 0.83273, current learning rate 0.0000000\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, val_acc_scores = [], [], []\n",
    "n_eval = 100\n",
    "for i in tqdm(range(1001)):\n",
    "    m.train()\n",
    "    x, y = get_batch('train')\n",
    "    logits, loss = m(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    train_losses.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    m.eval()\n",
    "    x, y = get_batch('val')\n",
    "    with torch.no_grad():\n",
    "        logits, loss = m(x, y)\n",
    "        val_losses.append(loss.item())\n",
    "        score = accuracy_score(y.tolist(), F.one_hot(torch.argmax(F.softmax(logits, dim=1), dim=1), num_classes=3).tolist())\n",
    "        val_acc_scores.append(score)\n",
    "    if i%n_eval==0:\n",
    "        tqdm.write(f\"step {i+1}: train loss {np.mean(train_losses[-n_eval:]):.5f}, val loss {np.mean(val_losses[-n_eval:]):.5f}, validation accuracy score {np.mean(val_acc_scores[-n_eval:]):.5f}, current learning rate {lr_schedule.get_last_lr()[0]:.7f}\")\n",
    "    lr_schedule.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51012 data points in test set\n",
      "159 features\n"
     ]
    }
   ],
   "source": [
    "input_test = pd.read_csv('./data/test.csv', index_col=0)\n",
    "input_test = input_test.rename(columns={\n",
    "    'Marital status': 'c_marital_status',\n",
    "    'Application mode': 'c_application_mode',\n",
    "    'Application order': 'c_application_order',\n",
    "    'Course': 'c_course',\n",
    "    'Daytime/evening attendance': 'c_attendance',\n",
    "    'Previous qualification': 'c_qualification',\n",
    "    'Previous qualification (grade)': 'n_qualification',\n",
    "    'Nacionality': 'c_nationality',\n",
    "    \"Mother's qualification\": 'c_mqual',\n",
    "    \"Father's qualification\": 'c_fqual',\n",
    "    \"Mother's occupation\": 'c_mocup',\n",
    "    \"Father's occupation\": 'c_focup',\n",
    "    'Admission grade': 'n_grade',\n",
    "    'Displaced': 'c_displaced',\n",
    "    'Educational special needs': 'c_special_needs',\n",
    "    'Debtor': 'c_debtor',\n",
    "    'Tuition fees up to date': 'c_fees',\n",
    "    'Gender': 'c_gender',\n",
    "    'Scholarship holder': 'c_scholarship',\n",
    "    'Age at enrollment': 'n_age',\n",
    "    'International': 'c_international',\n",
    "    'Curricular units 1st sem (credited)': 'n_cu1cr',\n",
    "    'Curricular units 1st sem (enrolled)': 'n_cu1en',\n",
    "    'Curricular units 1st sem (evaluations)': 'n_cu1ev',\n",
    "    'Curricular units 1st sem (approved)': 'n_cu1ap',\n",
    "    'Curricular units 1st sem (grade)': 'n_cu1gr',\n",
    "    'Curricular units 1st sem (without evaluations)': 'n_cu1wo',\n",
    "    'Curricular units 2nd sem (credited)': 'n_cu2cr',\n",
    "    'Curricular units 2nd sem (enrolled)': 'n_cu2en',\n",
    "    'Curricular units 2nd sem (evaluations)': 'n_cu2ev',\n",
    "    'Curricular units 2nd sem (approved)': 'n_cu2ap',\n",
    "    'Curricular units 2nd sem (grade)': 'n_cu2gr',\n",
    "    'Curricular units 2nd sem (without evaluations)': 'n_cu2wo',\n",
    "    'Unemployment rate': 'n_unemployment_rate',\n",
    "    'Inflation rate': 'n_inflation_rate',\n",
    "    'GDP': 'n_gdp'\n",
    "    })\n",
    "\n",
    "\n",
    "# one-hot-encode categorical features\n",
    "encoded_categorical_test_data = ohe.transform(input_test[categorical_features])\n",
    "encoded_categorical_test_df = pd.DataFrame(encoded_categorical_test_data, columns=encoded_categorical_feature_names)\n",
    "\n",
    "# scale numerical features\n",
    "discretized_numerical_test_data = disc.transform(input_test[numerical_features])\n",
    "discretized_numerical_test_df = pd.DataFrame(discretized_numerical_test_data, columns=discretized_numerical_feature_names)\n",
    "\n",
    "# merge categorical and numerical features\n",
    "X_test = pd.merge(encoded_categorical_test_df, discretized_numerical_test_df, left_index=True, right_index=True)\n",
    "\n",
    "print(f'{len(X_test)} data points in test set')\n",
    "print(f'{len(X_test.columns)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8565109037d84d97b7b17939082f7f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/511 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunksize = 100\n",
    "pred = []\n",
    "m.eval()\n",
    "for chunk in tqdm(range((len(X_test)//chunksize)+1)):\n",
    "    with torch.no_grad():\n",
    "        x = X_test[(chunksize*chunk):(chunksize*(chunk+1))]\n",
    "        x = torch.tensor(x.values, dtype=torch.int32).to(device)        \n",
    "        logits, _ = m(x)\n",
    "        pred += F.one_hot(torch.argmax(F.softmax(logits, dim=1), dim=1), num_classes=3).tolist()\n",
    "pd.DataFrame(target_ohe.inverse_transform(pred), index=input_test.index, columns=['Target']).to_csv('./data/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
