{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from utils.data_util import CarsDataset\n",
    "from utils.model_util import TabularTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint_dir='checkpoints'):\n",
    "    torch.save(state, os.path.join(checkpoint_dir, 'latest.pth'))\n",
    "    if is_best:\n",
    "        torch.save(state, os.path.join(checkpoint_dir, 'best.pth'))\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, filename='checkpoints/best.pth'):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Checkpoint loaded: {filename} (Epoch {start_epoch})\")\n",
    "        return start_epoch, loss\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filename}\")\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "@dataclass\n",
    "class Train_Parameters:\n",
    "    batch_size: int = 64 # number of examples per batch\n",
    "    val_size: float = 0.2 # relative size of validation split\n",
    "    n_eval: int = 200 # evaluate model performance every n_eval steps\n",
    "    epochs: int = 50 # number of training epochs\n",
    "\n",
    "@dataclass\n",
    "class Model_Parameters:    \n",
    "    num_features: int = 15 # number of features in input data\n",
    "    num_bins: int = 64 # number of bins in k-bins discretizer\n",
    "    d_model: int = 128 # dimension of model\n",
    "    d_ff: int = 256 # dimension of feed forward layer\n",
    "    num_layers: int = 4 # number of decoder layers\n",
    "    num_heads: int = 8 # number of heads\n",
    "    dropout: float = 0.4 # dropout rate\n",
    "\n",
    "tparam = Train_Parameters()\n",
    "mparam = Model_Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 4 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:322: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create dataset and dataloader objects\n",
    "\n",
    "train_data = CarsDataset(n_bins=mparam.num_bins, subset='train', preprocessors=None, val_size=tparam.val_size)\n",
    "val_data = CarsDataset(n_bins=mparam.num_bins, subset='val', preprocessors=[train_data.preprocessor, train_data.scaler], val_size=tparam.val_size)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=tparam.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=tparam.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in model: 921857\n"
     ]
    }
   ],
   "source": [
    "# instantiate model and optimizer\n",
    "\n",
    "model = TabularTransformer(\n",
    "    num_features=mparam.num_features,\n",
    "    num_bins=mparam.num_bins,\n",
    "    d_model=mparam.d_model,\n",
    "    num_layers=mparam.num_layers,\n",
    "    num_heads=mparam.num_heads,\n",
    "    d_ff=mparam.d_ff,\n",
    "    dropout=mparam.dropout\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f'Number of params in model: {sum(p.numel() for p in model.parameters())}')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    max_lr=4e-5,\n",
    "    epochs=tparam.epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.1,\n",
    "    div_factor=1e2,\n",
    "    final_div_factor=1e3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_model(model, optimizer, scheduler, train_loader, val_loader, training_parameters, checkpoint_path):\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch, loss = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n",
    "\n",
    "    for epoch in range(start_epoch, training_parameters.epochs):\n",
    "        model.train()\n",
    "        print(f'epoch {epoch+1}:')\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred, loss = model(X, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            scheduler.step()\n",
    "            if batch % training_parameters.n_eval == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    loss_val = []\n",
    "                    current = batch * training_parameters.batch_size + len(X)\n",
    "                    for Xval, yval in val_loader:\n",
    "                        Xval, yval = Xval.to(device), yval.to(device)\n",
    "                        pred, loss = model(Xval, yval)\n",
    "                        loss_val.append(loss.item())\n",
    "                    losses_val.append(np.mean(loss_val))\n",
    "                    print(f\"loss: {np.mean(losses[-training_parameters.n_eval:]):>7f}  val loss: {losses_val[-1]:>7f}  current lr: {scheduler.get_last_lr()[0]:>7f}  [{current:>7d}/{len(train_loader.dataset):>7d}]\")\n",
    "                    save_checkpoint({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                    }, losses_val[-1] == np.min(losses_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:\n",
      "loss: 0.222776  val loss: 0.303354  current lr: 0.000000  [     64/ 150826]\n",
      "loss: 0.253211  val loss: 0.233876  current lr: 0.000000  [  12864/ 150826]\n",
      "loss: 0.211925  val loss: 0.215194  current lr: 0.000001  [  25664/ 150826]\n",
      "loss: 0.201455  val loss: 0.198381  current lr: 0.000001  [  38464/ 150826]\n",
      "loss: 0.171933  val loss: 0.181971  current lr: 0.000001  [  51264/ 150826]\n",
      "loss: 0.164995  val loss: 0.166959  current lr: 0.000001  [  64064/ 150826]\n",
      "loss: 0.164163  val loss: 0.154128  current lr: 0.000001  [  76864/ 150826]\n",
      "loss: 0.139669  val loss: 0.143819  current lr: 0.000002  [  89664/ 150826]\n",
      "loss: 0.126784  val loss: 0.135963  current lr: 0.000002  [ 102464/ 150826]\n",
      "loss: 0.135581  val loss: 0.129478  current lr: 0.000003  [ 115264/ 150826]\n",
      "loss: 0.135520  val loss: 0.124156  current lr: 0.000003  [ 128064/ 150826]\n",
      "loss: 0.124092  val loss: 0.119732  current lr: 0.000004  [ 140864/ 150826]\n",
      "epoch 2:\n",
      "loss: 0.113701  val loss: 0.117102  current lr: 0.000004  [     64/ 150826]\n",
      "loss: 0.122121  val loss: 0.114221  current lr: 0.000005  [  12864/ 150826]\n",
      "loss: 0.108991  val loss: 0.111320  current lr: 0.000006  [  25664/ 150826]\n",
      "loss: 0.106330  val loss: 0.110419  current lr: 0.000006  [  38464/ 150826]\n",
      "loss: 0.107017  val loss: 0.107936  current lr: 0.000007  [  51264/ 150826]\n",
      "loss: 0.098752  val loss: 0.106259  current lr: 0.000008  [  64064/ 150826]\n",
      "loss: 0.090785  val loss: 0.105295  current lr: 0.000009  [  76864/ 150826]\n",
      "loss: 0.096614  val loss: 0.104392  current lr: 0.000010  [  89664/ 150826]\n",
      "loss: 0.088562  val loss: 0.103675  current lr: 0.000010  [ 102464/ 150826]\n",
      "loss: 0.102329  val loss: 0.103060  current lr: 0.000011  [ 115264/ 150826]\n",
      "loss: 0.104661  val loss: 0.102750  current lr: 0.000012  [ 128064/ 150826]\n",
      "loss: 0.105543  val loss: 0.102321  current lr: 0.000013  [ 140864/ 150826]\n",
      "epoch 3:\n",
      "loss: 0.104339  val loss: 0.101827  current lr: 0.000014  [     64/ 150826]\n",
      "loss: 0.098973  val loss: 0.102314  current lr: 0.000015  [  12864/ 150826]\n",
      "loss: 0.103002  val loss: 0.101344  current lr: 0.000016  [  25664/ 150826]\n",
      "loss: 0.095477  val loss: 0.101022  current lr: 0.000017  [  38464/ 150826]\n",
      "loss: 0.099532  val loss: 0.101624  current lr: 0.000018  [  51264/ 150826]\n",
      "loss: 0.089441  val loss: 0.100596  current lr: 0.000019  [  64064/ 150826]\n",
      "loss: 0.101116  val loss: 0.100792  current lr: 0.000020  [  76864/ 150826]\n",
      "loss: 0.088301  val loss: 0.102179  current lr: 0.000021  [  89664/ 150826]\n",
      "loss: 0.092069  val loss: 0.100267  current lr: 0.000022  [ 102464/ 150826]\n",
      "loss: 0.102092  val loss: 0.103640  current lr: 0.000023  [ 115264/ 150826]\n",
      "loss: 0.103451  val loss: 0.101161  current lr: 0.000025  [ 128064/ 150826]\n",
      "loss: 0.084843  val loss: 0.100535  current lr: 0.000026  [ 140864/ 150826]\n",
      "epoch 4:\n",
      "loss: 0.104304  val loss: 0.100155  current lr: 0.000026  [     64/ 150826]\n",
      "loss: 0.094191  val loss: 0.099748  current lr: 0.000027  [  12864/ 150826]\n",
      "loss: 0.087973  val loss: 0.099401  current lr: 0.000028  [  25664/ 150826]\n",
      "loss: 0.099269  val loss: 0.099418  current lr: 0.000029  [  38464/ 150826]\n",
      "loss: 0.096589  val loss: 0.099535  current lr: 0.000030  [  51264/ 150826]\n",
      "loss: 0.103056  val loss: 0.100105  current lr: 0.000031  [  64064/ 150826]\n",
      "loss: 0.093193  val loss: 0.099185  current lr: 0.000032  [  76864/ 150826]\n",
      "loss: 0.098973  val loss: 0.100089  current lr: 0.000033  [  89664/ 150826]\n",
      "loss: 0.091583  val loss: 0.099445  current lr: 0.000034  [ 102464/ 150826]\n",
      "loss: 0.089823  val loss: 0.099235  current lr: 0.000034  [ 115264/ 150826]\n",
      "loss: 0.107883  val loss: 0.105900  current lr: 0.000035  [ 128064/ 150826]\n",
      "loss: 0.096548  val loss: 0.099376  current lr: 0.000036  [ 140864/ 150826]\n",
      "epoch 5:\n",
      "loss: 0.087080  val loss: 0.099200  current lr: 0.000036  [     64/ 150826]\n",
      "loss: 0.090890  val loss: 0.102449  current lr: 0.000037  [  12864/ 150826]\n",
      "loss: 0.098518  val loss: 0.100554  current lr: 0.000037  [  25664/ 150826]\n",
      "loss: 0.096757  val loss: 0.101035  current lr: 0.000038  [  38464/ 150826]\n",
      "loss: 0.098921  val loss: 0.100019  current lr: 0.000038  [  51264/ 150826]\n",
      "loss: 0.088561  val loss: 0.099872  current lr: 0.000039  [  64064/ 150826]\n",
      "loss: 0.091865  val loss: 0.099305  current lr: 0.000039  [  76864/ 150826]\n",
      "loss: 0.093849  val loss: 0.098730  current lr: 0.000039  [  89664/ 150826]\n",
      "loss: 0.096991  val loss: 0.099147  current lr: 0.000040  [ 102464/ 150826]\n",
      "loss: 0.097210  val loss: 0.099162  current lr: 0.000040  [ 115264/ 150826]\n",
      "loss: 0.087005  val loss: 0.098635  current lr: 0.000040  [ 128064/ 150826]\n",
      "loss: 0.102980  val loss: 0.100074  current lr: 0.000040  [ 140864/ 150826]\n",
      "epoch 6:\n",
      "loss: 0.096423  val loss: 0.099105  current lr: 0.000040  [     64/ 150826]\n",
      "loss: 0.088376  val loss: 0.101935  current lr: 0.000040  [  12864/ 150826]\n",
      "loss: 0.092700  val loss: 0.099856  current lr: 0.000040  [  25664/ 150826]\n",
      "loss: 0.092160  val loss: 0.099398  current lr: 0.000040  [  38464/ 150826]\n",
      "loss: 0.094816  val loss: 0.101843  current lr: 0.000040  [  51264/ 150826]\n",
      "loss: 0.090421  val loss: 0.098934  current lr: 0.000040  [  64064/ 150826]\n",
      "loss: 0.102035  val loss: 0.099624  current lr: 0.000040  [  76864/ 150826]\n",
      "loss: 0.092932  val loss: 0.098429  current lr: 0.000040  [  89664/ 150826]\n",
      "loss: 0.095428  val loss: 0.098655  current lr: 0.000040  [ 102464/ 150826]\n",
      "loss: 0.090797  val loss: 0.098878  current lr: 0.000040  [ 115264/ 150826]\n",
      "loss: 0.088736  val loss: 0.098387  current lr: 0.000040  [ 128064/ 150826]\n",
      "loss: 0.104497  val loss: 0.100760  current lr: 0.000040  [ 140864/ 150826]\n",
      "epoch 7:\n",
      "loss: 0.098867  val loss: 0.101789  current lr: 0.000040  [     64/ 150826]\n",
      "loss: 0.089156  val loss: 0.099661  current lr: 0.000040  [  12864/ 150826]\n",
      "loss: 0.082399  val loss: 0.098993  current lr: 0.000040  [  25664/ 150826]\n",
      "loss: 0.098576  val loss: 0.098909  current lr: 0.000040  [  38464/ 150826]\n",
      "loss: 0.102329  val loss: 0.098843  current lr: 0.000040  [  51264/ 150826]\n",
      "loss: 0.082857  val loss: 0.099155  current lr: 0.000040  [  64064/ 150826]\n",
      "loss: 0.113580  val loss: 0.099652  current lr: 0.000040  [  76864/ 150826]\n",
      "loss: 0.081122  val loss: 0.098659  current lr: 0.000040  [  89664/ 150826]\n",
      "loss: 0.088179  val loss: 0.098309  current lr: 0.000040  [ 102464/ 150826]\n",
      "loss: 0.096086  val loss: 0.098448  current lr: 0.000040  [ 115264/ 150826]\n",
      "loss: 0.102188  val loss: 0.098687  current lr: 0.000040  [ 128064/ 150826]\n",
      "loss: 0.106619  val loss: 0.098846  current lr: 0.000040  [ 140864/ 150826]\n",
      "epoch 8:\n",
      "loss: 0.089482  val loss: 0.098346  current lr: 0.000040  [     64/ 150826]\n",
      "loss: 0.091081  val loss: 0.098425  current lr: 0.000040  [  12864/ 150826]\n",
      "loss: 0.090343  val loss: 0.098911  current lr: 0.000040  [  25664/ 150826]\n",
      "loss: 0.093021  val loss: 0.098472  current lr: 0.000040  [  38464/ 150826]\n",
      "loss: 0.090687  val loss: 0.098254  current lr: 0.000040  [  51264/ 150826]\n",
      "loss: 0.106755  val loss: 0.098454  current lr: 0.000040  [  64064/ 150826]\n",
      "loss: 0.094160  val loss: 0.098347  current lr: 0.000040  [  76864/ 150826]\n",
      "loss: 0.093374  val loss: 0.099820  current lr: 0.000040  [  89664/ 150826]\n",
      "loss: 0.090161  val loss: 0.098049  current lr: 0.000040  [ 102464/ 150826]\n",
      "loss: 0.090566  val loss: 0.098111  current lr: 0.000040  [ 115264/ 150826]\n",
      "loss: 0.082064  val loss: 0.098044  current lr: 0.000040  [ 128064/ 150826]\n",
      "loss: 0.099601  val loss: 0.098674  current lr: 0.000040  [ 140864/ 150826]\n",
      "epoch 9:\n",
      "loss: 0.109209  val loss: 0.098629  current lr: 0.000040  [     64/ 150826]\n",
      "loss: 0.095350  val loss: 0.098780  current lr: 0.000040  [  12864/ 150826]\n",
      "loss: 0.104548  val loss: 0.098548  current lr: 0.000040  [  25664/ 150826]\n",
      "loss: 0.073401  val loss: 0.098390  current lr: 0.000039  [  38464/ 150826]\n",
      "loss: 0.093145  val loss: 0.098237  current lr: 0.000039  [  51264/ 150826]\n",
      "loss: 0.089334  val loss: 0.098494  current lr: 0.000039  [  64064/ 150826]\n",
      "loss: 0.100143  val loss: 0.098074  current lr: 0.000039  [  76864/ 150826]\n",
      "loss: 0.112163  val loss: 0.098562  current lr: 0.000039  [  89664/ 150826]\n",
      "loss: 0.096458  val loss: 0.098445  current lr: 0.000039  [ 102464/ 150826]\n",
      "loss: 0.085627  val loss: 0.098827  current lr: 0.000039  [ 115264/ 150826]\n",
      "loss: 0.102211  val loss: 0.098060  current lr: 0.000039  [ 128064/ 150826]\n",
      "loss: 0.084239  val loss: 0.098657  current lr: 0.000039  [ 140864/ 150826]\n",
      "epoch 10:\n",
      "loss: 0.079685  val loss: 0.098663  current lr: 0.000039  [     64/ 150826]\n",
      "loss: 0.088207  val loss: 0.098459  current lr: 0.000039  [  12864/ 150826]\n",
      "loss: 0.086673  val loss: 0.098163  current lr: 0.000039  [  25664/ 150826]\n",
      "loss: 0.081859  val loss: 0.098599  current lr: 0.000039  [  38464/ 150826]\n",
      "loss: 0.098912  val loss: 0.097922  current lr: 0.000039  [  51264/ 150826]\n",
      "loss: 0.092065  val loss: 0.098229  current lr: 0.000039  [  64064/ 150826]\n",
      "loss: 0.093774  val loss: 0.098960  current lr: 0.000039  [  76864/ 150826]\n",
      "loss: 0.086800  val loss: 0.098103  current lr: 0.000039  [  89664/ 150826]\n",
      "loss: 0.094115  val loss: 0.099209  current lr: 0.000039  [ 102464/ 150826]\n",
      "loss: 0.096190  val loss: 0.098054  current lr: 0.000039  [ 115264/ 150826]\n",
      "loss: 0.100139  val loss: 0.098058  current lr: 0.000039  [ 128064/ 150826]\n",
      "loss: 0.106946  val loss: 0.098715  current lr: 0.000039  [ 140864/ 150826]\n",
      "epoch 11:\n",
      "loss: 0.091746  val loss: 0.097975  current lr: 0.000039  [     64/ 150826]\n",
      "loss: 0.085347  val loss: 0.098205  current lr: 0.000039  [  12864/ 150826]\n",
      "loss: 0.092908  val loss: 0.098258  current lr: 0.000039  [  25664/ 150826]\n",
      "loss: 0.104391  val loss: 0.098173  current lr: 0.000039  [  38464/ 150826]\n",
      "loss: 0.093704  val loss: 0.099858  current lr: 0.000039  [  51264/ 150826]\n",
      "loss: 0.085558  val loss: 0.097927  current lr: 0.000039  [  64064/ 150826]\n",
      "loss: 0.107233  val loss: 0.098553  current lr: 0.000039  [  76864/ 150826]\n",
      "loss: 0.095307  val loss: 0.098294  current lr: 0.000038  [  89664/ 150826]\n",
      "loss: 0.092394  val loss: 0.098578  current lr: 0.000038  [ 102464/ 150826]\n",
      "loss: 0.086490  val loss: 0.098239  current lr: 0.000038  [ 115264/ 150826]\n",
      "loss: 0.096489  val loss: 0.098067  current lr: 0.000038  [ 128064/ 150826]\n",
      "loss: 0.086893  val loss: 0.097940  current lr: 0.000038  [ 140864/ 150826]\n",
      "epoch 12:\n",
      "loss: 0.075539  val loss: 0.098360  current lr: 0.000038  [     64/ 150826]\n",
      "loss: 0.091976  val loss: 0.098049  current lr: 0.000038  [  12864/ 150826]\n",
      "loss: 0.097741  val loss: 0.099830  current lr: 0.000038  [  25664/ 150826]\n",
      "loss: 0.096511  val loss: 0.098324  current lr: 0.000038  [  38464/ 150826]\n",
      "loss: 0.092616  val loss: 0.098145  current lr: 0.000038  [  51264/ 150826]\n",
      "loss: 0.087731  val loss: 0.098329  current lr: 0.000038  [  64064/ 150826]\n",
      "loss: 0.093381  val loss: 0.098202  current lr: 0.000038  [  76864/ 150826]\n",
      "loss: 0.086413  val loss: 0.098337  current lr: 0.000038  [  89664/ 150826]\n",
      "loss: 0.091251  val loss: 0.098464  current lr: 0.000038  [ 102464/ 150826]\n",
      "loss: 0.092863  val loss: 0.098596  current lr: 0.000038  [ 115264/ 150826]\n",
      "loss: 0.082800  val loss: 0.098142  current lr: 0.000038  [ 128064/ 150826]\n",
      "loss: 0.090025  val loss: 0.098398  current lr: 0.000038  [ 140864/ 150826]\n",
      "epoch 13:\n",
      "loss: 0.104377  val loss: 0.098160  current lr: 0.000038  [     64/ 150826]\n",
      "loss: 0.079949  val loss: 0.098346  current lr: 0.000038  [  12864/ 150826]\n",
      "loss: 0.085957  val loss: 0.098368  current lr: 0.000038  [  25664/ 150826]\n",
      "loss: 0.105981  val loss: 0.098358  current lr: 0.000037  [  38464/ 150826]\n",
      "loss: 0.087701  val loss: 0.098826  current lr: 0.000037  [  51264/ 150826]\n",
      "loss: 0.085252  val loss: 0.098726  current lr: 0.000037  [  64064/ 150826]\n",
      "loss: 0.095133  val loss: 0.098290  current lr: 0.000037  [  76864/ 150826]\n",
      "loss: 0.100659  val loss: 0.098011  current lr: 0.000037  [  89664/ 150826]\n",
      "loss: 0.083564  val loss: 0.098188  current lr: 0.000037  [ 102464/ 150826]\n",
      "loss: 0.088100  val loss: 0.100172  current lr: 0.000037  [ 115264/ 150826]\n",
      "loss: 0.106169  val loss: 0.099057  current lr: 0.000037  [ 128064/ 150826]\n",
      "loss: 0.094149  val loss: 0.099249  current lr: 0.000037  [ 140864/ 150826]\n",
      "epoch 14:\n",
      "loss: 0.085522  val loss: 0.098426  current lr: 0.000037  [     64/ 150826]\n",
      "loss: 0.100825  val loss: 0.098890  current lr: 0.000037  [  12864/ 150826]\n",
      "loss: 0.087544  val loss: 0.098613  current lr: 0.000037  [  25664/ 150826]\n",
      "loss: 0.086413  val loss: 0.098775  current lr: 0.000037  [  38464/ 150826]\n",
      "loss: 0.083573  val loss: 0.098292  current lr: 0.000037  [  51264/ 150826]\n",
      "loss: 0.089414  val loss: 0.098785  current lr: 0.000037  [  64064/ 150826]\n",
      "loss: 0.081440  val loss: 0.098362  current lr: 0.000037  [  76864/ 150826]\n",
      "loss: 0.081409  val loss: 0.099527  current lr: 0.000037  [  89664/ 150826]\n",
      "loss: 0.105579  val loss: 0.098117  current lr: 0.000036  [ 102464/ 150826]\n",
      "loss: 0.094537  val loss: 0.098921  current lr: 0.000036  [ 115264/ 150826]\n",
      "loss: 0.086022  val loss: 0.098124  current lr: 0.000036  [ 128064/ 150826]\n",
      "loss: 0.098831  val loss: 0.098173  current lr: 0.000036  [ 140864/ 150826]\n",
      "epoch 15:\n",
      "loss: 0.102761  val loss: 0.098502  current lr: 0.000036  [     64/ 150826]\n",
      "loss: 0.090073  val loss: 0.099040  current lr: 0.000036  [  12864/ 150826]\n",
      "loss: 0.081337  val loss: 0.099071  current lr: 0.000036  [  25664/ 150826]\n",
      "loss: 0.089668  val loss: 0.098269  current lr: 0.000036  [  38464/ 150826]\n",
      "loss: 0.083772  val loss: 0.099109  current lr: 0.000036  [  51264/ 150826]\n",
      "loss: 0.084396  val loss: 0.098314  current lr: 0.000036  [  64064/ 150826]\n",
      "loss: 0.098499  val loss: 0.099079  current lr: 0.000036  [  76864/ 150826]\n",
      "loss: 0.098521  val loss: 0.098318  current lr: 0.000036  [  89664/ 150826]\n",
      "loss: 0.086210  val loss: 0.098510  current lr: 0.000036  [ 102464/ 150826]\n",
      "loss: 0.089906  val loss: 0.098765  current lr: 0.000036  [ 115264/ 150826]\n",
      "loss: 0.100358  val loss: 0.098859  current lr: 0.000035  [ 128064/ 150826]\n",
      "loss: 0.092389  val loss: 0.098938  current lr: 0.000035  [ 140864/ 150826]\n",
      "epoch 16:\n",
      "loss: 0.094675  val loss: 0.098353  current lr: 0.000035  [     64/ 150826]\n",
      "loss: 0.085189  val loss: 0.098750  current lr: 0.000035  [  12864/ 150826]\n",
      "loss: 0.084247  val loss: 0.098459  current lr: 0.000035  [  25664/ 150826]\n",
      "loss: 0.089694  val loss: 0.099129  current lr: 0.000035  [  38464/ 150826]\n",
      "loss: 0.100216  val loss: 0.099154  current lr: 0.000035  [  51264/ 150826]\n",
      "loss: 0.088941  val loss: 0.098212  current lr: 0.000035  [  64064/ 150826]\n",
      "loss: 0.094572  val loss: 0.098527  current lr: 0.000035  [  76864/ 150826]\n",
      "loss: 0.082214  val loss: 0.098815  current lr: 0.000035  [  89664/ 150826]\n",
      "loss: 0.094737  val loss: 0.098424  current lr: 0.000035  [ 102464/ 150826]\n",
      "loss: 0.092217  val loss: 0.098186  current lr: 0.000035  [ 115264/ 150826]\n",
      "loss: 0.090357  val loss: 0.099917  current lr: 0.000035  [ 128064/ 150826]\n",
      "loss: 0.090002  val loss: 0.098253  current lr: 0.000034  [ 140864/ 150826]\n",
      "epoch 17:\n",
      "loss: 0.091458  val loss: 0.098250  current lr: 0.000034  [     64/ 150826]\n",
      "loss: 0.096421  val loss: 0.098350  current lr: 0.000034  [  12864/ 150826]\n",
      "loss: 0.079371  val loss: 0.098595  current lr: 0.000034  [  25664/ 150826]\n",
      "loss: 0.090216  val loss: 0.098516  current lr: 0.000034  [  38464/ 150826]\n",
      "loss: 0.083573  val loss: 0.098298  current lr: 0.000034  [  51264/ 150826]\n",
      "loss: 0.088955  val loss: 0.099643  current lr: 0.000034  [  64064/ 150826]\n",
      "loss: 0.089364  val loss: 0.098644  current lr: 0.000034  [  76864/ 150826]\n",
      "loss: 0.105947  val loss: 0.098566  current lr: 0.000034  [  89664/ 150826]\n",
      "loss: 0.085412  val loss: 0.098197  current lr: 0.000034  [ 102464/ 150826]\n",
      "loss: 0.087931  val loss: 0.098461  current lr: 0.000034  [ 115264/ 150826]\n",
      "loss: 0.086247  val loss: 0.098290  current lr: 0.000034  [ 128064/ 150826]\n",
      "loss: 0.100669  val loss: 0.098709  current lr: 0.000033  [ 140864/ 150826]\n",
      "epoch 18:\n",
      "loss: 0.087229  val loss: 0.098911  current lr: 0.000033  [     64/ 150826]\n",
      "loss: 0.087503  val loss: 0.098607  current lr: 0.000033  [  12864/ 150826]\n",
      "loss: 0.103704  val loss: 0.100020  current lr: 0.000033  [  25664/ 150826]\n",
      "loss: 0.082902  val loss: 0.098560  current lr: 0.000033  [  38464/ 150826]\n",
      "loss: 0.092750  val loss: 0.099180  current lr: 0.000033  [  51264/ 150826]\n",
      "loss: 0.099504  val loss: 0.098987  current lr: 0.000033  [  64064/ 150826]\n",
      "loss: 0.088176  val loss: 0.098417  current lr: 0.000033  [  76864/ 150826]\n",
      "loss: 0.088732  val loss: 0.098805  current lr: 0.000033  [  89664/ 150826]\n",
      "loss: 0.080165  val loss: 0.098507  current lr: 0.000033  [ 102464/ 150826]\n",
      "loss: 0.088207  val loss: 0.098545  current lr: 0.000033  [ 115264/ 150826]\n",
      "loss: 0.083379  val loss: 0.099260  current lr: 0.000032  [ 128064/ 150826]\n",
      "loss: 0.091989  val loss: 0.098594  current lr: 0.000032  [ 140864/ 150826]\n",
      "epoch 19:\n",
      "loss: 0.087330  val loss: 0.098483  current lr: 0.000032  [     64/ 150826]\n",
      "loss: 0.091062  val loss: 0.098477  current lr: 0.000032  [  12864/ 150826]\n",
      "loss: 0.081128  val loss: 0.100485  current lr: 0.000032  [  25664/ 150826]\n",
      "loss: 0.101052  val loss: 0.099407  current lr: 0.000032  [  38464/ 150826]\n",
      "loss: 0.089301  val loss: 0.098639  current lr: 0.000032  [  51264/ 150826]\n",
      "loss: 0.096621  val loss: 0.098807  current lr: 0.000032  [  64064/ 150826]\n",
      "loss: 0.090032  val loss: 0.098792  current lr: 0.000032  [  76864/ 150826]\n",
      "loss: 0.079229  val loss: 0.098960  current lr: 0.000032  [  89664/ 150826]\n",
      "loss: 0.086050  val loss: 0.098578  current lr: 0.000032  [ 102464/ 150826]\n",
      "loss: 0.090012  val loss: 0.098892  current lr: 0.000031  [ 115264/ 150826]\n",
      "loss: 0.095043  val loss: 0.098795  current lr: 0.000031  [ 128064/ 150826]\n",
      "loss: 0.089000  val loss: 0.099033  current lr: 0.000031  [ 140864/ 150826]\n",
      "epoch 20:\n",
      "loss: 0.080402  val loss: 0.098514  current lr: 0.000031  [     64/ 150826]\n",
      "loss: 0.080637  val loss: 0.099015  current lr: 0.000031  [  12864/ 150826]\n",
      "loss: 0.091671  val loss: 0.100402  current lr: 0.000031  [  25664/ 150826]\n",
      "loss: 0.091823  val loss: 0.099220  current lr: 0.000031  [  38464/ 150826]\n",
      "loss: 0.097248  val loss: 0.100437  current lr: 0.000031  [  51264/ 150826]\n",
      "loss: 0.079105  val loss: 0.099419  current lr: 0.000031  [  64064/ 150826]\n",
      "loss: 0.092041  val loss: 0.098825  current lr: 0.000031  [  76864/ 150826]\n",
      "loss: 0.098314  val loss: 0.098665  current lr: 0.000030  [  89664/ 150826]\n",
      "loss: 0.091146  val loss: 0.099402  current lr: 0.000030  [ 102464/ 150826]\n",
      "loss: 0.095203  val loss: 0.099390  current lr: 0.000030  [ 115264/ 150826]\n",
      "loss: 0.072455  val loss: 0.098827  current lr: 0.000030  [ 128064/ 150826]\n",
      "loss: 0.081566  val loss: 0.099164  current lr: 0.000030  [ 140864/ 150826]\n",
      "epoch 21:\n",
      "loss: 0.097756  val loss: 0.099270  current lr: 0.000030  [     64/ 150826]\n",
      "loss: 0.087350  val loss: 0.099495  current lr: 0.000030  [  12864/ 150826]\n",
      "loss: 0.084693  val loss: 0.099110  current lr: 0.000030  [  25664/ 150826]\n",
      "loss: 0.087616  val loss: 0.098880  current lr: 0.000030  [  38464/ 150826]\n",
      "loss: 0.085786  val loss: 0.099173  current lr: 0.000030  [  51264/ 150826]\n",
      "loss: 0.085509  val loss: 0.099018  current lr: 0.000029  [  64064/ 150826]\n",
      "loss: 0.084609  val loss: 0.099672  current lr: 0.000029  [  76864/ 150826]\n",
      "loss: 0.091084  val loss: 0.099236  current lr: 0.000029  [  89664/ 150826]\n",
      "loss: 0.097289  val loss: 0.099601  current lr: 0.000029  [ 102464/ 150826]\n",
      "loss: 0.079501  val loss: 0.098975  current lr: 0.000029  [ 115264/ 150826]\n",
      "loss: 0.087876  val loss: 0.099100  current lr: 0.000029  [ 128064/ 150826]\n",
      "loss: 0.087935  val loss: 0.100479  current lr: 0.000029  [ 140864/ 150826]\n",
      "epoch 22:\n",
      "loss: 0.099093  val loss: 0.099193  current lr: 0.000029  [     64/ 150826]\n",
      "loss: 0.086900  val loss: 0.099338  current lr: 0.000029  [  12864/ 150826]\n",
      "loss: 0.086290  val loss: 0.099306  current lr: 0.000029  [  25664/ 150826]\n",
      "loss: 0.077595  val loss: 0.099390  current lr: 0.000028  [  38464/ 150826]\n",
      "loss: 0.082131  val loss: 0.099247  current lr: 0.000028  [  51264/ 150826]\n",
      "loss: 0.087175  val loss: 0.099507  current lr: 0.000028  [  64064/ 150826]\n",
      "loss: 0.097017  val loss: 0.099487  current lr: 0.000028  [  76864/ 150826]\n",
      "loss: 0.077509  val loss: 0.099315  current lr: 0.000028  [  89664/ 150826]\n",
      "loss: 0.092057  val loss: 0.099482  current lr: 0.000028  [ 102464/ 150826]\n",
      "loss: 0.074295  val loss: 0.100209  current lr: 0.000028  [ 115264/ 150826]\n",
      "loss: 0.094038  val loss: 0.099254  current lr: 0.000028  [ 128064/ 150826]\n",
      "loss: 0.104933  val loss: 0.099619  current lr: 0.000028  [ 140864/ 150826]\n",
      "epoch 23:\n",
      "loss: 0.100273  val loss: 0.099784  current lr: 0.000027  [     64/ 150826]\n",
      "loss: 0.083558  val loss: 0.099947  current lr: 0.000027  [  12864/ 150826]\n",
      "loss: 0.077698  val loss: 0.099687  current lr: 0.000027  [  25664/ 150826]\n",
      "loss: 0.083065  val loss: 0.099308  current lr: 0.000027  [  38464/ 150826]\n",
      "loss: 0.080283  val loss: 0.099507  current lr: 0.000027  [  51264/ 150826]\n",
      "loss: 0.098992  val loss: 0.099757  current lr: 0.000027  [  64064/ 150826]\n",
      "loss: 0.083618  val loss: 0.099489  current lr: 0.000027  [  76864/ 150826]\n",
      "loss: 0.084516  val loss: 0.099011  current lr: 0.000027  [  89664/ 150826]\n",
      "loss: 0.089738  val loss: 0.099371  current lr: 0.000027  [ 102464/ 150826]\n",
      "loss: 0.084346  val loss: 0.099471  current lr: 0.000026  [ 115264/ 150826]\n",
      "loss: 0.098658  val loss: 0.100161  current lr: 0.000026  [ 128064/ 150826]\n",
      "loss: 0.083408  val loss: 0.099641  current lr: 0.000026  [ 140864/ 150826]\n",
      "epoch 24:\n",
      "loss: 0.101226  val loss: 0.099774  current lr: 0.000026  [     64/ 150826]\n",
      "loss: 0.087328  val loss: 0.099712  current lr: 0.000026  [  12864/ 150826]\n",
      "loss: 0.078881  val loss: 0.099905  current lr: 0.000026  [  25664/ 150826]\n",
      "loss: 0.086953  val loss: 0.099337  current lr: 0.000026  [  38464/ 150826]\n",
      "loss: 0.078002  val loss: 0.099282  current lr: 0.000026  [  51264/ 150826]\n",
      "loss: 0.094543  val loss: 0.099446  current lr: 0.000026  [  64064/ 150826]\n",
      "loss: 0.081476  val loss: 0.100192  current lr: 0.000025  [  76864/ 150826]\n",
      "loss: 0.086657  val loss: 0.099959  current lr: 0.000025  [  89664/ 150826]\n",
      "loss: 0.093430  val loss: 0.099976  current lr: 0.000025  [ 102464/ 150826]\n",
      "loss: 0.085060  val loss: 0.099582  current lr: 0.000025  [ 115264/ 150826]\n",
      "loss: 0.091797  val loss: 0.099692  current lr: 0.000025  [ 128064/ 150826]\n",
      "loss: 0.086899  val loss: 0.099971  current lr: 0.000025  [ 140864/ 150826]\n",
      "epoch 25:\n",
      "loss: 0.091604  val loss: 0.100354  current lr: 0.000025  [     64/ 150826]\n",
      "loss: 0.076832  val loss: 0.100519  current lr: 0.000025  [  12864/ 150826]\n",
      "loss: 0.095015  val loss: 0.100486  current lr: 0.000025  [  25664/ 150826]\n",
      "loss: 0.090119  val loss: 0.100742  current lr: 0.000024  [  38464/ 150826]\n",
      "loss: 0.096080  val loss: 0.100111  current lr: 0.000024  [  51264/ 150826]\n",
      "loss: 0.082850  val loss: 0.099947  current lr: 0.000024  [  64064/ 150826]\n",
      "loss: 0.084688  val loss: 0.099782  current lr: 0.000024  [  76864/ 150826]\n",
      "loss: 0.086713  val loss: 0.099920  current lr: 0.000024  [  89664/ 150826]\n",
      "loss: 0.083053  val loss: 0.100143  current lr: 0.000024  [ 102464/ 150826]\n",
      "loss: 0.093090  val loss: 0.099866  current lr: 0.000024  [ 115264/ 150826]\n",
      "loss: 0.080545  val loss: 0.100238  current lr: 0.000024  [ 128064/ 150826]\n",
      "loss: 0.079981  val loss: 0.100103  current lr: 0.000024  [ 140864/ 150826]\n",
      "epoch 26:\n",
      "loss: 0.088195  val loss: 0.100525  current lr: 0.000023  [     64/ 150826]\n",
      "loss: 0.085453  val loss: 0.100508  current lr: 0.000023  [  12864/ 150826]\n",
      "loss: 0.089310  val loss: 0.100242  current lr: 0.000023  [  25664/ 150826]\n",
      "loss: 0.077336  val loss: 0.100044  current lr: 0.000023  [  38464/ 150826]\n",
      "loss: 0.088679  val loss: 0.099918  current lr: 0.000023  [  51264/ 150826]\n",
      "loss: 0.083893  val loss: 0.100377  current lr: 0.000023  [  64064/ 150826]\n",
      "loss: 0.082909  val loss: 0.100531  current lr: 0.000023  [  76864/ 150826]\n",
      "loss: 0.077772  val loss: 0.100383  current lr: 0.000023  [  89664/ 150826]\n",
      "loss: 0.085326  val loss: 0.100741  current lr: 0.000023  [ 102464/ 150826]\n",
      "loss: 0.094021  val loss: 0.100915  current lr: 0.000022  [ 115264/ 150826]\n",
      "loss: 0.091344  val loss: 0.100205  current lr: 0.000022  [ 128064/ 150826]\n",
      "loss: 0.086469  val loss: 0.100668  current lr: 0.000022  [ 140864/ 150826]\n",
      "epoch 27:\n",
      "loss: 0.096115  val loss: 0.100334  current lr: 0.000022  [     64/ 150826]\n",
      "loss: 0.080209  val loss: 0.100836  current lr: 0.000022  [  12864/ 150826]\n",
      "loss: 0.093285  val loss: 0.101452  current lr: 0.000022  [  25664/ 150826]\n",
      "loss: 0.084057  val loss: 0.100827  current lr: 0.000022  [  38464/ 150826]\n",
      "loss: 0.078979  val loss: 0.099757  current lr: 0.000022  [  51264/ 150826]\n",
      "loss: 0.070187  val loss: 0.101686  current lr: 0.000021  [  64064/ 150826]\n",
      "loss: 0.076488  val loss: 0.100923  current lr: 0.000021  [  76864/ 150826]\n",
      "loss: 0.079861  val loss: 0.100006  current lr: 0.000021  [  89664/ 150826]\n",
      "loss: 0.099648  val loss: 0.100563  current lr: 0.000021  [ 102464/ 150826]\n",
      "loss: 0.095876  val loss: 0.100658  current lr: 0.000021  [ 115264/ 150826]\n",
      "loss: 0.089788  val loss: 0.100343  current lr: 0.000021  [ 128064/ 150826]\n",
      "loss: 0.085291  val loss: 0.100289  current lr: 0.000021  [ 140864/ 150826]\n",
      "epoch 28:\n",
      "loss: 0.097956  val loss: 0.101539  current lr: 0.000021  [     64/ 150826]\n",
      "loss: 0.091049  val loss: 0.100454  current lr: 0.000021  [  12864/ 150826]\n",
      "loss: 0.088655  val loss: 0.100572  current lr: 0.000020  [  25664/ 150826]\n",
      "loss: 0.076000  val loss: 0.100196  current lr: 0.000020  [  38464/ 150826]\n",
      "loss: 0.084591  val loss: 0.100920  current lr: 0.000020  [  51264/ 150826]\n",
      "loss: 0.086405  val loss: 0.100731  current lr: 0.000020  [  64064/ 150826]\n",
      "loss: 0.090866  val loss: 0.100707  current lr: 0.000020  [  76864/ 150826]\n",
      "loss: 0.087858  val loss: 0.100637  current lr: 0.000020  [  89664/ 150826]\n",
      "loss: 0.081057  val loss: 0.101094  current lr: 0.000020  [ 102464/ 150826]\n",
      "loss: 0.077094  val loss: 0.100728  current lr: 0.000020  [ 115264/ 150826]\n",
      "loss: 0.097884  val loss: 0.100585  current lr: 0.000020  [ 128064/ 150826]\n",
      "loss: 0.076331  val loss: 0.100728  current lr: 0.000019  [ 140864/ 150826]\n",
      "epoch 29:\n",
      "loss: 0.078272  val loss: 0.101199  current lr: 0.000019  [     64/ 150826]\n",
      "loss: 0.083747  val loss: 0.101024  current lr: 0.000019  [  12864/ 150826]\n",
      "loss: 0.085829  val loss: 0.100721  current lr: 0.000019  [  25664/ 150826]\n",
      "loss: 0.087640  val loss: 0.101162  current lr: 0.000019  [  38464/ 150826]\n",
      "loss: 0.086351  val loss: 0.101032  current lr: 0.000019  [  51264/ 150826]\n",
      "loss: 0.087339  val loss: 0.100754  current lr: 0.000019  [  64064/ 150826]\n",
      "loss: 0.082541  val loss: 0.101559  current lr: 0.000019  [  76864/ 150826]\n",
      "loss: 0.092545  val loss: 0.102063  current lr: 0.000018  [  89664/ 150826]\n",
      "loss: 0.082236  val loss: 0.101481  current lr: 0.000018  [ 102464/ 150826]\n",
      "loss: 0.086457  val loss: 0.101543  current lr: 0.000018  [ 115264/ 150826]\n",
      "loss: 0.083247  val loss: 0.101114  current lr: 0.000018  [ 128064/ 150826]\n",
      "loss: 0.069398  val loss: 0.100663  current lr: 0.000018  [ 140864/ 150826]\n",
      "epoch 30:\n",
      "loss: 0.083869  val loss: 0.100835  current lr: 0.000018  [     64/ 150826]\n",
      "loss: 0.085215  val loss: 0.101641  current lr: 0.000018  [  12864/ 150826]\n",
      "loss: 0.077742  val loss: 0.101038  current lr: 0.000018  [  25664/ 150826]\n",
      "loss: 0.073258  val loss: 0.101317  current lr: 0.000018  [  38464/ 150826]\n",
      "loss: 0.087701  val loss: 0.101293  current lr: 0.000017  [  51264/ 150826]\n",
      "loss: 0.083623  val loss: 0.102126  current lr: 0.000017  [  64064/ 150826]\n",
      "loss: 0.084226  val loss: 0.101794  current lr: 0.000017  [  76864/ 150826]\n",
      "loss: 0.085735  val loss: 0.101465  current lr: 0.000017  [  89664/ 150826]\n",
      "loss: 0.086402  val loss: 0.101539  current lr: 0.000017  [ 102464/ 150826]\n",
      "loss: 0.088291  val loss: 0.100966  current lr: 0.000017  [ 115264/ 150826]\n",
      "loss: 0.094504  val loss: 0.100919  current lr: 0.000017  [ 128064/ 150826]\n",
      "loss: 0.083468  val loss: 0.101610  current lr: 0.000017  [ 140864/ 150826]\n",
      "epoch 31:\n",
      "loss: 0.085750  val loss: 0.101590  current lr: 0.000017  [     64/ 150826]\n",
      "loss: 0.081470  val loss: 0.101842  current lr: 0.000016  [  12864/ 150826]\n",
      "loss: 0.085699  val loss: 0.102249  current lr: 0.000016  [  25664/ 150826]\n",
      "loss: 0.075482  val loss: 0.101395  current lr: 0.000016  [  38464/ 150826]\n",
      "loss: 0.086442  val loss: 0.101255  current lr: 0.000016  [  51264/ 150826]\n",
      "loss: 0.098658  val loss: 0.101922  current lr: 0.000016  [  64064/ 150826]\n",
      "loss: 0.077779  val loss: 0.101310  current lr: 0.000016  [  76864/ 150826]\n",
      "loss: 0.085926  val loss: 0.101826  current lr: 0.000016  [  89664/ 150826]\n",
      "loss: 0.073150  val loss: 0.101129  current lr: 0.000016  [ 102464/ 150826]\n",
      "loss: 0.092073  val loss: 0.101613  current lr: 0.000015  [ 115264/ 150826]\n",
      "loss: 0.091786  val loss: 0.101998  current lr: 0.000015  [ 128064/ 150826]\n",
      "loss: 0.070770  val loss: 0.101557  current lr: 0.000015  [ 140864/ 150826]\n",
      "epoch 32:\n",
      "loss: 0.080774  val loss: 0.102198  current lr: 0.000015  [     64/ 150826]\n",
      "loss: 0.084082  val loss: 0.101891  current lr: 0.000015  [  12864/ 150826]\n",
      "loss: 0.087394  val loss: 0.101629  current lr: 0.000015  [  25664/ 150826]\n",
      "loss: 0.089247  val loss: 0.102292  current lr: 0.000015  [  38464/ 150826]\n",
      "loss: 0.080620  val loss: 0.102204  current lr: 0.000015  [  51264/ 150826]\n",
      "loss: 0.083103  val loss: 0.101923  current lr: 0.000015  [  64064/ 150826]\n",
      "loss: 0.085338  val loss: 0.102302  current lr: 0.000014  [  76864/ 150826]\n",
      "loss: 0.088645  val loss: 0.101467  current lr: 0.000014  [  89664/ 150826]\n",
      "loss: 0.081116  val loss: 0.102405  current lr: 0.000014  [ 102464/ 150826]\n",
      "loss: 0.086012  val loss: 0.101449  current lr: 0.000014  [ 115264/ 150826]\n",
      "loss: 0.077327  val loss: 0.101668  current lr: 0.000014  [ 128064/ 150826]\n",
      "loss: 0.074888  val loss: 0.102441  current lr: 0.000014  [ 140864/ 150826]\n",
      "epoch 33:\n",
      "loss: 0.081243  val loss: 0.101922  current lr: 0.000014  [     64/ 150826]\n",
      "loss: 0.068787  val loss: 0.102441  current lr: 0.000014  [  12864/ 150826]\n",
      "loss: 0.087828  val loss: 0.101990  current lr: 0.000014  [  25664/ 150826]\n",
      "loss: 0.082397  val loss: 0.102335  current lr: 0.000013  [  38464/ 150826]\n",
      "loss: 0.087309  val loss: 0.101365  current lr: 0.000013  [  51264/ 150826]\n",
      "loss: 0.097559  val loss: 0.102386  current lr: 0.000013  [  64064/ 150826]\n",
      "loss: 0.077319  val loss: 0.102175  current lr: 0.000013  [  76864/ 150826]\n",
      "loss: 0.090164  val loss: 0.101994  current lr: 0.000013  [  89664/ 150826]\n",
      "loss: 0.076842  val loss: 0.101926  current lr: 0.000013  [ 102464/ 150826]\n",
      "loss: 0.089574  val loss: 0.102419  current lr: 0.000013  [ 115264/ 150826]\n",
      "loss: 0.076100  val loss: 0.101683  current lr: 0.000013  [ 128064/ 150826]\n",
      "loss: 0.083978  val loss: 0.101884  current lr: 0.000013  [ 140864/ 150826]\n",
      "epoch 34:\n",
      "loss: 0.072199  val loss: 0.102306  current lr: 0.000013  [     64/ 150826]\n",
      "loss: 0.077018  val loss: 0.102231  current lr: 0.000012  [  12864/ 150826]\n",
      "loss: 0.085835  val loss: 0.102286  current lr: 0.000012  [  25664/ 150826]\n",
      "loss: 0.076207  val loss: 0.101992  current lr: 0.000012  [  38464/ 150826]\n",
      "loss: 0.079143  val loss: 0.102080  current lr: 0.000012  [  51264/ 150826]\n",
      "loss: 0.073679  val loss: 0.102956  current lr: 0.000012  [  64064/ 150826]\n",
      "loss: 0.078181  val loss: 0.102412  current lr: 0.000012  [  76864/ 150826]\n",
      "loss: 0.086149  val loss: 0.102475  current lr: 0.000012  [  89664/ 150826]\n",
      "loss: 0.083091  val loss: 0.102475  current lr: 0.000012  [ 102464/ 150826]\n",
      "loss: 0.096323  val loss: 0.102403  current lr: 0.000012  [ 115264/ 150826]\n",
      "loss: 0.080966  val loss: 0.102894  current lr: 0.000011  [ 128064/ 150826]\n",
      "loss: 0.077247  val loss: 0.101946  current lr: 0.000011  [ 140864/ 150826]\n",
      "epoch 35:\n",
      "loss: 0.091022  val loss: 0.102537  current lr: 0.000011  [     64/ 150826]\n",
      "loss: 0.083572  val loss: 0.102757  current lr: 0.000011  [  12864/ 150826]\n",
      "loss: 0.083699  val loss: 0.102521  current lr: 0.000011  [  25664/ 150826]\n",
      "loss: 0.078890  val loss: 0.102590  current lr: 0.000011  [  38464/ 150826]\n",
      "loss: 0.089670  val loss: 0.102183  current lr: 0.000011  [  51264/ 150826]\n",
      "loss: 0.093598  val loss: 0.103602  current lr: 0.000011  [  64064/ 150826]\n",
      "loss: 0.075608  val loss: 0.102626  current lr: 0.000011  [  76864/ 150826]\n",
      "loss: 0.091784  val loss: 0.102228  current lr: 0.000010  [  89664/ 150826]\n",
      "loss: 0.074424  val loss: 0.102623  current lr: 0.000010  [ 102464/ 150826]\n",
      "loss: 0.084979  val loss: 0.102557  current lr: 0.000010  [ 115264/ 150826]\n",
      "loss: 0.073854  val loss: 0.102840  current lr: 0.000010  [ 128064/ 150826]\n",
      "loss: 0.082359  val loss: 0.102893  current lr: 0.000010  [ 140864/ 150826]\n",
      "epoch 36:\n",
      "loss: 0.073060  val loss: 0.101976  current lr: 0.000010  [     64/ 150826]\n",
      "loss: 0.087604  val loss: 0.102673  current lr: 0.000010  [  12864/ 150826]\n",
      "loss: 0.081206  val loss: 0.103067  current lr: 0.000010  [  25664/ 150826]\n",
      "loss: 0.076256  val loss: 0.102838  current lr: 0.000010  [  38464/ 150826]\n",
      "loss: 0.092181  val loss: 0.102847  current lr: 0.000010  [  51264/ 150826]\n",
      "loss: 0.088568  val loss: 0.102368  current lr: 0.000009  [  64064/ 150826]\n",
      "loss: 0.082843  val loss: 0.103193  current lr: 0.000009  [  76864/ 150826]\n",
      "loss: 0.080969  val loss: 0.103052  current lr: 0.000009  [  89664/ 150826]\n",
      "loss: 0.064471  val loss: 0.102832  current lr: 0.000009  [ 102464/ 150826]\n",
      "loss: 0.090847  val loss: 0.102413  current lr: 0.000009  [ 115264/ 150826]\n",
      "loss: 0.078610  val loss: 0.103046  current lr: 0.000009  [ 128064/ 150826]\n",
      "loss: 0.076494  val loss: 0.102600  current lr: 0.000009  [ 140864/ 150826]\n",
      "epoch 37:\n",
      "loss: 0.077855  val loss: 0.103249  current lr: 0.000009  [     64/ 150826]\n",
      "loss: 0.082642  val loss: 0.102748  current lr: 0.000009  [  12864/ 150826]\n",
      "loss: 0.090108  val loss: 0.103264  current lr: 0.000009  [  25664/ 150826]\n",
      "loss: 0.077459  val loss: 0.102895  current lr: 0.000009  [  38464/ 150826]\n",
      "loss: 0.070057  val loss: 0.103481  current lr: 0.000008  [  51264/ 150826]\n",
      "loss: 0.083765  val loss: 0.103315  current lr: 0.000008  [  64064/ 150826]\n",
      "loss: 0.080961  val loss: 0.103198  current lr: 0.000008  [  76864/ 150826]\n",
      "loss: 0.074086  val loss: 0.103719  current lr: 0.000008  [  89664/ 150826]\n",
      "loss: 0.078323  val loss: 0.103004  current lr: 0.000008  [ 102464/ 150826]\n",
      "loss: 0.084680  val loss: 0.102801  current lr: 0.000008  [ 115264/ 150826]\n",
      "loss: 0.077954  val loss: 0.102716  current lr: 0.000008  [ 128064/ 150826]\n",
      "loss: 0.091424  val loss: 0.103064  current lr: 0.000008  [ 140864/ 150826]\n",
      "epoch 38:\n",
      "loss: 0.086981  val loss: 0.102692  current lr: 0.000008  [     64/ 150826]\n",
      "loss: 0.078983  val loss: 0.102692  current lr: 0.000008  [  12864/ 150826]\n",
      "loss: 0.088469  val loss: 0.103134  current lr: 0.000008  [  25664/ 150826]\n",
      "loss: 0.084760  val loss: 0.102971  current lr: 0.000007  [  38464/ 150826]\n",
      "loss: 0.081532  val loss: 0.103479  current lr: 0.000007  [  51264/ 150826]\n",
      "loss: 0.072561  val loss: 0.103332  current lr: 0.000007  [  64064/ 150826]\n",
      "loss: 0.078221  val loss: 0.103097  current lr: 0.000007  [  76864/ 150826]\n",
      "loss: 0.078393  val loss: 0.103059  current lr: 0.000007  [  89664/ 150826]\n",
      "loss: 0.085171  val loss: 0.103011  current lr: 0.000007  [ 102464/ 150826]\n",
      "loss: 0.072055  val loss: 0.103885  current lr: 0.000007  [ 115264/ 150826]\n",
      "loss: 0.073546  val loss: 0.102674  current lr: 0.000007  [ 128064/ 150826]\n",
      "loss: 0.103017  val loss: 0.103652  current lr: 0.000007  [ 140864/ 150826]\n",
      "epoch 39:\n",
      "loss: 0.077020  val loss: 0.103089  current lr: 0.000007  [     64/ 150826]\n",
      "loss: 0.069913  val loss: 0.102815  current lr: 0.000007  [  12864/ 150826]\n",
      "loss: 0.076506  val loss: 0.103546  current lr: 0.000006  [  25664/ 150826]\n",
      "loss: 0.084669  val loss: 0.103508  current lr: 0.000006  [  38464/ 150826]\n",
      "loss: 0.079380  val loss: 0.103959  current lr: 0.000006  [  51264/ 150826]\n",
      "loss: 0.080086  val loss: 0.102936  current lr: 0.000006  [  64064/ 150826]\n",
      "loss: 0.080902  val loss: 0.103218  current lr: 0.000006  [  76864/ 150826]\n",
      "loss: 0.079571  val loss: 0.103615  current lr: 0.000006  [  89664/ 150826]\n",
      "loss: 0.090761  val loss: 0.103536  current lr: 0.000006  [ 102464/ 150826]\n",
      "loss: 0.083284  val loss: 0.103092  current lr: 0.000006  [ 115264/ 150826]\n",
      "loss: 0.081416  val loss: 0.103409  current lr: 0.000006  [ 128064/ 150826]\n",
      "loss: 0.082818  val loss: 0.103143  current lr: 0.000006  [ 140864/ 150826]\n",
      "epoch 40:\n",
      "loss: 0.075998  val loss: 0.103190  current lr: 0.000006  [     64/ 150826]\n",
      "loss: 0.092061  val loss: 0.102967  current lr: 0.000006  [  12864/ 150826]\n",
      "loss: 0.075493  val loss: 0.103270  current lr: 0.000005  [  25664/ 150826]\n",
      "loss: 0.091281  val loss: 0.103767  current lr: 0.000005  [  38464/ 150826]\n",
      "loss: 0.068212  val loss: 0.103426  current lr: 0.000005  [  51264/ 150826]\n",
      "loss: 0.084305  val loss: 0.103258  current lr: 0.000005  [  64064/ 150826]\n",
      "loss: 0.093311  val loss: 0.103205  current lr: 0.000005  [  76864/ 150826]\n",
      "loss: 0.090196  val loss: 0.103290  current lr: 0.000005  [  89664/ 150826]\n",
      "loss: 0.070647  val loss: 0.103370  current lr: 0.000005  [ 102464/ 150826]\n",
      "loss: 0.077270  val loss: 0.103295  current lr: 0.000005  [ 115264/ 150826]\n",
      "loss: 0.087637  val loss: 0.103681  current lr: 0.000005  [ 128064/ 150826]\n",
      "loss: 0.064183  val loss: 0.103222  current lr: 0.000005  [ 140864/ 150826]\n",
      "epoch 41:\n",
      "loss: 0.066504  val loss: 0.103217  current lr: 0.000005  [     64/ 150826]\n",
      "loss: 0.072578  val loss: 0.103982  current lr: 0.000005  [  12864/ 150826]\n",
      "loss: 0.086122  val loss: 0.103469  current lr: 0.000005  [  25664/ 150826]\n",
      "loss: 0.083524  val loss: 0.103243  current lr: 0.000004  [  38464/ 150826]\n",
      "loss: 0.096001  val loss: 0.103397  current lr: 0.000004  [  51264/ 150826]\n",
      "loss: 0.076845  val loss: 0.103481  current lr: 0.000004  [  64064/ 150826]\n",
      "loss: 0.060410  val loss: 0.103587  current lr: 0.000004  [  76864/ 150826]\n",
      "loss: 0.083437  val loss: 0.103823  current lr: 0.000004  [  89664/ 150826]\n",
      "loss: 0.077291  val loss: 0.103986  current lr: 0.000004  [ 102464/ 150826]\n",
      "loss: 0.070207  val loss: 0.103762  current lr: 0.000004  [ 115264/ 150826]\n",
      "loss: 0.083680  val loss: 0.103549  current lr: 0.000004  [ 128064/ 150826]\n",
      "loss: 0.097379  val loss: 0.103376  current lr: 0.000004  [ 140864/ 150826]\n",
      "epoch 42:\n",
      "loss: 0.078127  val loss: 0.103457  current lr: 0.000004  [     64/ 150826]\n",
      "loss: 0.078781  val loss: 0.103165  current lr: 0.000004  [  12864/ 150826]\n",
      "loss: 0.090512  val loss: 0.103559  current lr: 0.000004  [  25664/ 150826]\n",
      "loss: 0.092101  val loss: 0.103819  current lr: 0.000004  [  38464/ 150826]\n",
      "loss: 0.075700  val loss: 0.103530  current lr: 0.000004  [  51264/ 150826]\n",
      "loss: 0.072219  val loss: 0.104052  current lr: 0.000003  [  64064/ 150826]\n",
      "loss: 0.079870  val loss: 0.103508  current lr: 0.000003  [  76864/ 150826]\n",
      "loss: 0.074858  val loss: 0.104027  current lr: 0.000003  [  89664/ 150826]\n",
      "loss: 0.082652  val loss: 0.103570  current lr: 0.000003  [ 102464/ 150826]\n",
      "loss: 0.078273  val loss: 0.103689  current lr: 0.000003  [ 115264/ 150826]\n",
      "loss: 0.078677  val loss: 0.103433  current lr: 0.000003  [ 128064/ 150826]\n",
      "loss: 0.071253  val loss: 0.103576  current lr: 0.000003  [ 140864/ 150826]\n",
      "epoch 43:\n",
      "loss: 0.082407  val loss: 0.103788  current lr: 0.000003  [     64/ 150826]\n",
      "loss: 0.071528  val loss: 0.103875  current lr: 0.000003  [  12864/ 150826]\n",
      "loss: 0.078943  val loss: 0.103785  current lr: 0.000003  [  25664/ 150826]\n",
      "loss: 0.072390  val loss: 0.103611  current lr: 0.000003  [  38464/ 150826]\n",
      "loss: 0.086513  val loss: 0.103578  current lr: 0.000003  [  51264/ 150826]\n",
      "loss: 0.081080  val loss: 0.103957  current lr: 0.000003  [  64064/ 150826]\n",
      "loss: 0.079095  val loss: 0.104055  current lr: 0.000003  [  76864/ 150826]\n",
      "loss: 0.083032  val loss: 0.103786  current lr: 0.000003  [  89664/ 150826]\n",
      "loss: 0.082167  val loss: 0.103675  current lr: 0.000003  [ 102464/ 150826]\n",
      "loss: 0.080106  val loss: 0.103866  current lr: 0.000002  [ 115264/ 150826]\n",
      "loss: 0.083856  val loss: 0.103723  current lr: 0.000002  [ 128064/ 150826]\n",
      "loss: 0.075372  val loss: 0.103448  current lr: 0.000002  [ 140864/ 150826]\n",
      "epoch 44:\n",
      "loss: 0.081945  val loss: 0.103537  current lr: 0.000002  [     64/ 150826]\n",
      "loss: 0.069458  val loss: 0.103444  current lr: 0.000002  [  12864/ 150826]\n",
      "loss: 0.088399  val loss: 0.103870  current lr: 0.000002  [  25664/ 150826]\n",
      "loss: 0.080362  val loss: 0.103731  current lr: 0.000002  [  38464/ 150826]\n",
      "loss: 0.085686  val loss: 0.103727  current lr: 0.000002  [  51264/ 150826]\n",
      "loss: 0.075469  val loss: 0.103737  current lr: 0.000002  [  64064/ 150826]\n",
      "loss: 0.079097  val loss: 0.103880  current lr: 0.000002  [  76864/ 150826]\n",
      "loss: 0.071723  val loss: 0.103682  current lr: 0.000002  [  89664/ 150826]\n",
      "loss: 0.083033  val loss: 0.103911  current lr: 0.000002  [ 102464/ 150826]\n",
      "loss: 0.078494  val loss: 0.104011  current lr: 0.000002  [ 115264/ 150826]\n",
      "loss: 0.084356  val loss: 0.103912  current lr: 0.000002  [ 128064/ 150826]\n",
      "loss: 0.083369  val loss: 0.103945  current lr: 0.000002  [ 140864/ 150826]\n",
      "epoch 45:\n",
      "loss: 0.077683  val loss: 0.103747  current lr: 0.000002  [     64/ 150826]\n",
      "loss: 0.086198  val loss: 0.103830  current lr: 0.000002  [  12864/ 150826]\n",
      "loss: 0.077545  val loss: 0.103853  current lr: 0.000002  [  25664/ 150826]\n",
      "loss: 0.089106  val loss: 0.103792  current lr: 0.000002  [  38464/ 150826]\n",
      "loss: 0.073905  val loss: 0.103508  current lr: 0.000002  [  51264/ 150826]\n",
      "loss: 0.066893  val loss: 0.103643  current lr: 0.000001  [  64064/ 150826]\n",
      "loss: 0.095982  val loss: 0.103950  current lr: 0.000001  [  76864/ 150826]\n",
      "loss: 0.090117  val loss: 0.103810  current lr: 0.000001  [  89664/ 150826]\n",
      "loss: 0.077018  val loss: 0.103934  current lr: 0.000001  [ 102464/ 150826]\n",
      "loss: 0.073216  val loss: 0.103767  current lr: 0.000001  [ 115264/ 150826]\n",
      "loss: 0.076339  val loss: 0.103813  current lr: 0.000001  [ 128064/ 150826]\n",
      "loss: 0.076938  val loss: 0.103918  current lr: 0.000001  [ 140864/ 150826]\n",
      "epoch 46:\n",
      "loss: 0.072013  val loss: 0.103918  current lr: 0.000001  [     64/ 150826]\n",
      "loss: 0.072369  val loss: 0.103931  current lr: 0.000001  [  12864/ 150826]\n",
      "loss: 0.081124  val loss: 0.103985  current lr: 0.000001  [  25664/ 150826]\n",
      "loss: 0.086378  val loss: 0.103886  current lr: 0.000001  [  38464/ 150826]\n",
      "loss: 0.077533  val loss: 0.103918  current lr: 0.000001  [  51264/ 150826]\n",
      "loss: 0.077331  val loss: 0.103786  current lr: 0.000001  [  64064/ 150826]\n",
      "loss: 0.090008  val loss: 0.103912  current lr: 0.000001  [  76864/ 150826]\n",
      "loss: 0.074142  val loss: 0.103840  current lr: 0.000001  [  89664/ 150826]\n",
      "loss: 0.076941  val loss: 0.103845  current lr: 0.000001  [ 102464/ 150826]\n",
      "loss: 0.079690  val loss: 0.103861  current lr: 0.000001  [ 115264/ 150826]\n",
      "loss: 0.084904  val loss: 0.103954  current lr: 0.000001  [ 128064/ 150826]\n",
      "loss: 0.076241  val loss: 0.103914  current lr: 0.000001  [ 140864/ 150826]\n",
      "epoch 47:\n",
      "loss: 0.079211  val loss: 0.103930  current lr: 0.000001  [     64/ 150826]\n",
      "loss: 0.063212  val loss: 0.103861  current lr: 0.000001  [  12864/ 150826]\n",
      "loss: 0.083041  val loss: 0.103843  current lr: 0.000001  [  25664/ 150826]\n",
      "loss: 0.082676  val loss: 0.103921  current lr: 0.000001  [  38464/ 150826]\n",
      "loss: 0.082573  val loss: 0.103891  current lr: 0.000001  [  51264/ 150826]\n",
      "loss: 0.084674  val loss: 0.103929  current lr: 0.000001  [  64064/ 150826]\n",
      "loss: 0.073564  val loss: 0.103965  current lr: 0.000001  [  76864/ 150826]\n",
      "loss: 0.085250  val loss: 0.103961  current lr: 0.000001  [  89664/ 150826]\n",
      "loss: 0.079478  val loss: 0.103891  current lr: 0.000001  [ 102464/ 150826]\n",
      "loss: 0.077686  val loss: 0.103847  current lr: 0.000001  [ 115264/ 150826]\n",
      "loss: 0.086316  val loss: 0.103973  current lr: 0.000000  [ 128064/ 150826]\n",
      "loss: 0.081143  val loss: 0.104003  current lr: 0.000000  [ 140864/ 150826]\n",
      "epoch 48:\n",
      "loss: 0.073167  val loss: 0.103941  current lr: 0.000000  [     64/ 150826]\n",
      "loss: 0.072389  val loss: 0.103905  current lr: 0.000000  [  12864/ 150826]\n",
      "loss: 0.080983  val loss: 0.103908  current lr: 0.000000  [  25664/ 150826]\n",
      "loss: 0.078118  val loss: 0.103924  current lr: 0.000000  [  38464/ 150826]\n",
      "loss: 0.078253  val loss: 0.103951  current lr: 0.000000  [  51264/ 150826]\n",
      "loss: 0.085780  val loss: 0.103881  current lr: 0.000000  [  64064/ 150826]\n",
      "loss: 0.069757  val loss: 0.103911  current lr: 0.000000  [  76864/ 150826]\n",
      "loss: 0.087533  val loss: 0.103942  current lr: 0.000000  [  89664/ 150826]\n",
      "loss: 0.094553  val loss: 0.103964  current lr: 0.000000  [ 102464/ 150826]\n",
      "loss: 0.070624  val loss: 0.103948  current lr: 0.000000  [ 115264/ 150826]\n",
      "loss: 0.077264  val loss: 0.103924  current lr: 0.000000  [ 128064/ 150826]\n",
      "loss: 0.085861  val loss: 0.103943  current lr: 0.000000  [ 140864/ 150826]\n",
      "epoch 49:\n",
      "loss: 0.069331  val loss: 0.103924  current lr: 0.000000  [     64/ 150826]\n",
      "loss: 0.073010  val loss: 0.103900  current lr: 0.000000  [  12864/ 150826]\n",
      "loss: 0.086665  val loss: 0.103880  current lr: 0.000000  [  25664/ 150826]\n",
      "loss: 0.075649  val loss: 0.103891  current lr: 0.000000  [  38464/ 150826]\n",
      "loss: 0.067192  val loss: 0.103908  current lr: 0.000000  [  51264/ 150826]\n",
      "loss: 0.082307  val loss: 0.103920  current lr: 0.000000  [  64064/ 150826]\n",
      "loss: 0.072902  val loss: 0.103923  current lr: 0.000000  [  76864/ 150826]\n",
      "loss: 0.078060  val loss: 0.103928  current lr: 0.000000  [  89664/ 150826]\n",
      "loss: 0.081349  val loss: 0.103942  current lr: 0.000000  [ 102464/ 150826]\n",
      "loss: 0.081140  val loss: 0.103939  current lr: 0.000000  [ 115264/ 150826]\n",
      "loss: 0.080239  val loss: 0.103938  current lr: 0.000000  [ 128064/ 150826]\n",
      "loss: 0.082133  val loss: 0.103932  current lr: 0.000000  [ 140864/ 150826]\n",
      "epoch 50:\n",
      "loss: 0.089258  val loss: 0.103939  current lr: 0.000000  [     64/ 150826]\n",
      "loss: 0.079976  val loss: 0.103936  current lr: 0.000000  [  12864/ 150826]\n",
      "loss: 0.087417  val loss: 0.103937  current lr: 0.000000  [  25664/ 150826]\n",
      "loss: 0.076673  val loss: 0.103935  current lr: 0.000000  [  38464/ 150826]\n",
      "loss: 0.061948  val loss: 0.103937  current lr: 0.000000  [  51264/ 150826]\n",
      "loss: 0.082035  val loss: 0.103936  current lr: 0.000000  [  64064/ 150826]\n",
      "loss: 0.071481  val loss: 0.103933  current lr: 0.000000  [  76864/ 150826]\n",
      "loss: 0.077427  val loss: 0.103936  current lr: 0.000000  [  89664/ 150826]\n",
      "loss: 0.079460  val loss: 0.103936  current lr: 0.000000  [ 102464/ 150826]\n",
      "loss: 0.090819  val loss: 0.103936  current lr: 0.000000  [ 115264/ 150826]\n",
      "loss: 0.077412  val loss: 0.103936  current lr: 0.000000  [ 128064/ 150826]\n",
      "loss: 0.081884  val loss: 0.103936  current lr: 0.000000  [ 140864/ 150826]\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    training_parameters=tparam,\n",
    "    checkpoint_path='checkpoints/best.pth'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
