{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from utils.data_util import MushroomDataset\n",
    "from utils.model_util import TabularTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint_dir='checkpoints'):\n",
    "    torch.save(state, os.path.join(checkpoint_dir, 'latest.pth'))\n",
    "    if is_best:\n",
    "        torch.save(state, os.path.join(checkpoint_dir, 'best.pth'))\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, filename='checkpoints/best.pth'):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Checkpoint loaded: {filename} (Epoch {start_epoch})\")\n",
    "        return start_epoch, loss\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filename}\")\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "@dataclass\n",
    "class Train_Parameters:\n",
    "    batch_size: int = 128 # number of examples per batch\n",
    "    val_size: float = 0.2 # relative size of validation split\n",
    "    n_eval: int = 1000 # evaluate model performance every n_eval steps\n",
    "    epochs: int = 20 # number of training epochs\n",
    "\n",
    "@dataclass\n",
    "class Model_Parameters:    \n",
    "    num_features: int = 20 # number of features in input data\n",
    "    num_bins: int = 16 # number of bins in k-bins discretizer\n",
    "    d_model: int = 64 # dimension of model\n",
    "    d_ff: int = 128 # dimension of feed forward layer\n",
    "    num_layers: int = 4 # number of decoder layers\n",
    "    num_heads: int = 8 # number of heads\n",
    "    dropout: float = 0.3 # dropout rate\n",
    "\n",
    "tparam = Train_Parameters()\n",
    "mparam = Model_Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader objects\n",
    "\n",
    "train_data = MushroomDataset(n_bins=mparam.num_bins, subset='train', preprocessors=None, val_size=tparam.val_size)\n",
    "val_data = MushroomDataset(n_bins=mparam.num_bins, subset='val', preprocessors=[train_data.preprocessor, train_data.label_enc], val_size=tparam.val_size)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=tparam.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=tparam.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in model: 224001\n"
     ]
    }
   ],
   "source": [
    "# instantiate model and optimizer\n",
    "\n",
    "model = TabularTransformer(\n",
    "    num_features=mparam.num_features,\n",
    "    num_bins=mparam.num_bins,\n",
    "    d_model=mparam.d_model,\n",
    "    num_layers=mparam.num_layers,\n",
    "    num_heads=mparam.num_heads,\n",
    "    d_ff=mparam.d_ff,\n",
    "    dropout=mparam.dropout\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f'Number of params in model: {sum(p.numel() for p in model.parameters())}')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    max_lr=4e-3,\n",
    "    epochs=tparam.epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.1,\n",
    "    div_factor=1e2,\n",
    "    final_div_factor=1e3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_model(model, optimizer, scheduler, train_loader, val_loader, training_parameters, checkpoint_path):\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch, loss = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n",
    "\n",
    "    for epoch in range(start_epoch, training_parameters.epochs):\n",
    "        model.train()\n",
    "        print(f'epoch {epoch+1}:')\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred, loss = model(X, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            scheduler.step()\n",
    "            if batch % training_parameters.n_eval == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    loss_val = []\n",
    "                    current = batch * training_parameters.batch_size + len(X)\n",
    "                    for Xval, yval in val_loader:\n",
    "                        Xval, yval = Xval.to(device), yval.to(device)\n",
    "                        pred, loss = model(Xval, yval)\n",
    "                        loss_val.append(loss.item())\n",
    "                    losses_val.append(np.mean(loss_val))\n",
    "                    print(f\"loss: {np.mean(losses[-training_parameters.n_eval:]):>7f}  val loss: {losses_val[-1]:>7f}  current lr: {scheduler.get_last_lr()[0]:>7f}  [{current:>7d}/{len(train_loader.dataset):>7d}]\")\n",
    "                    save_checkpoint({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                    }, losses_val[-1] == np.min(losses_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:\n",
      "loss: 0.700886  val loss: 0.733996  current lr: 0.000040  [    128/2493556]\n",
      "loss: 0.403431  val loss: 0.229578  current lr: 0.000046  [ 128128/2493556]\n",
      "loss: 0.132397  val loss: 0.075370  current lr: 0.000066  [ 256128/2493556]\n",
      "loss: 0.071698  val loss: 0.061263  current lr: 0.000098  [ 384128/2493556]\n",
      "loss: 0.061488  val loss: 0.057201  current lr: 0.000142  [ 512128/2493556]\n",
      "loss: 0.057911  val loss: 0.052744  current lr: 0.000199  [ 640128/2493556]\n",
      "loss: 0.053713  val loss: 0.048775  current lr: 0.000267  [ 768128/2493556]\n",
      "loss: 0.053073  val loss: 0.047904  current lr: 0.000347  [ 896128/2493556]\n",
      "loss: 0.049654  val loss: 0.050179  current lr: 0.000438  [1024128/2493556]\n",
      "loss: 0.047023  val loss: 0.046762  current lr: 0.000539  [1152128/2493556]\n",
      "loss: 0.046543  val loss: 0.049181  current lr: 0.000650  [1280128/2493556]\n",
      "loss: 0.046981  val loss: 0.057246  current lr: 0.000769  [1408128/2493556]\n",
      "loss: 0.050369  val loss: 0.044877  current lr: 0.000897  [1536128/2493556]\n",
      "loss: 0.050141  val loss: 0.048918  current lr: 0.001032  [1664128/2493556]\n",
      "loss: 0.047722  val loss: 0.048781  current lr: 0.001173  [1792128/2493556]\n",
      "loss: 0.051263  val loss: 0.046697  current lr: 0.001320  [1920128/2493556]\n",
      "loss: 0.051426  val loss: 0.051553  current lr: 0.001472  [2048128/2493556]\n",
      "loss: 0.051138  val loss: 0.055993  current lr: 0.001627  [2176128/2493556]\n",
      "loss: 0.050559  val loss: 0.050774  current lr: 0.001784  [2304128/2493556]\n",
      "loss: 0.050840  val loss: 0.051226  current lr: 0.001943  [2432128/2493556]\n",
      "epoch 2:\n",
      "loss: 0.053389  val loss: 0.049098  current lr: 0.002020  [    128/2493556]\n",
      "loss: 0.053253  val loss: 0.059686  current lr: 0.002180  [ 128128/2493556]\n",
      "loss: 0.053760  val loss: 0.052151  current lr: 0.002338  [ 256128/2493556]\n",
      "loss: 0.056612  val loss: 0.054137  current lr: 0.002495  [ 384128/2493556]\n",
      "loss: 0.060330  val loss: 0.057320  current lr: 0.002648  [ 512128/2493556]\n",
      "loss: 0.076975  val loss: 0.066041  current lr: 0.002797  [ 640128/2493556]\n",
      "loss: 0.066360  val loss: 0.058714  current lr: 0.002941  [ 768128/2493556]\n",
      "loss: 0.064758  val loss: 0.058718  current lr: 0.003079  [ 896128/2493556]\n",
      "loss: 0.059860  val loss: 0.058887  current lr: 0.003211  [1024128/2493556]\n",
      "loss: 0.058789  val loss: 0.057687  current lr: 0.003334  [1152128/2493556]\n",
      "loss: 0.055537  val loss: 0.055009  current lr: 0.003449  [1280128/2493556]\n",
      "loss: 0.056641  val loss: 0.055458  current lr: 0.003555  [1408128/2493556]\n",
      "loss: 0.057186  val loss: 0.053030  current lr: 0.003651  [1536128/2493556]\n",
      "loss: 0.055277  val loss: 0.055419  current lr: 0.003736  [1664128/2493556]\n",
      "loss: 0.055470  val loss: 0.054437  current lr: 0.003810  [1792128/2493556]\n",
      "loss: 0.056750  val loss: 0.054967  current lr: 0.003872  [1920128/2493556]\n",
      "loss: 0.055657  val loss: 0.055309  current lr: 0.003923  [2048128/2493556]\n",
      "loss: 0.055032  val loss: 0.053375  current lr: 0.003961  [2176128/2493556]\n",
      "loss: 0.054888  val loss: 0.051903  current lr: 0.003986  [2304128/2493556]\n",
      "loss: 0.052025  val loss: 0.050880  current lr: 0.003999  [2432128/2493556]\n",
      "epoch 3:\n",
      "loss: 0.052854  val loss: 0.050973  current lr: 0.004000  [    128/2493556]\n",
      "loss: 0.052456  val loss: 0.050432  current lr: 0.004000  [ 128128/2493556]\n",
      "loss: 0.054557  val loss: 0.049497  current lr: 0.004000  [ 256128/2493556]\n",
      "loss: 0.053047  val loss: 0.050691  current lr: 0.003999  [ 384128/2493556]\n",
      "loss: 0.052467  val loss: 0.049938  current lr: 0.003999  [ 512128/2493556]\n",
      "loss: 0.051667  val loss: 0.049931  current lr: 0.003998  [ 640128/2493556]\n",
      "loss: 0.049271  val loss: 0.050182  current lr: 0.003997  [ 768128/2493556]\n",
      "loss: 0.051166  val loss: 0.051010  current lr: 0.003996  [ 896128/2493556]\n",
      "loss: 0.049324  val loss: 0.054325  current lr: 0.003995  [1024128/2493556]\n",
      "loss: 0.049400  val loss: 0.050519  current lr: 0.003993  [1152128/2493556]\n",
      "loss: 0.047768  val loss: 0.052146  current lr: 0.003992  [1280128/2493556]\n",
      "loss: 0.047522  val loss: 0.049726  current lr: 0.003990  [1408128/2493556]\n",
      "loss: 0.049184  val loss: 0.050204  current lr: 0.003988  [1536128/2493556]\n",
      "loss: 0.052345  val loss: 0.047560  current lr: 0.003986  [1664128/2493556]\n",
      "loss: 0.050281  val loss: 0.050901  current lr: 0.003984  [1792128/2493556]\n",
      "loss: 0.050346  val loss: 0.048301  current lr: 0.003982  [1920128/2493556]\n",
      "loss: 0.048912  val loss: 0.049207  current lr: 0.003979  [2048128/2493556]\n",
      "loss: 0.049433  val loss: 0.049998  current lr: 0.003977  [2176128/2493556]\n",
      "loss: 0.048665  val loss: 0.048422  current lr: 0.003974  [2304128/2493556]\n",
      "loss: 0.048185  val loss: 0.046702  current lr: 0.003971  [2432128/2493556]\n",
      "epoch 4:\n",
      "loss: 0.048402  val loss: 0.048727  current lr: 0.003970  [    128/2493556]\n",
      "loss: 0.049558  val loss: 0.049266  current lr: 0.003966  [ 128128/2493556]\n",
      "loss: 0.048288  val loss: 0.046757  current lr: 0.003963  [ 256128/2493556]\n",
      "loss: 0.048400  val loss: 0.049030  current lr: 0.003960  [ 384128/2493556]\n",
      "loss: 0.048510  val loss: 0.047912  current lr: 0.003956  [ 512128/2493556]\n",
      "loss: 0.048640  val loss: 0.046482  current lr: 0.003952  [ 640128/2493556]\n",
      "loss: 0.049178  val loss: 0.048878  current lr: 0.003948  [ 768128/2493556]\n",
      "loss: 0.047706  val loss: 0.046487  current lr: 0.003944  [ 896128/2493556]\n",
      "loss: 0.047803  val loss: 0.046498  current lr: 0.003940  [1024128/2493556]\n",
      "loss: 0.047863  val loss: 0.046510  current lr: 0.003935  [1152128/2493556]\n",
      "loss: 0.047650  val loss: 0.047961  current lr: 0.003931  [1280128/2493556]\n",
      "loss: 0.048989  val loss: 0.045492  current lr: 0.003926  [1408128/2493556]\n",
      "loss: 0.048215  val loss: 0.046249  current lr: 0.003921  [1536128/2493556]\n",
      "loss: 0.045294  val loss: 0.049267  current lr: 0.003916  [1664128/2493556]\n",
      "loss: 0.049171  val loss: 0.045343  current lr: 0.003911  [1792128/2493556]\n",
      "loss: 0.047047  val loss: 0.048768  current lr: 0.003905  [1920128/2493556]\n",
      "loss: 0.046729  val loss: 0.046139  current lr: 0.003900  [2048128/2493556]\n",
      "loss: 0.049092  val loss: 0.046172  current lr: 0.003894  [2176128/2493556]\n",
      "loss: 0.048462  val loss: 0.046558  current lr: 0.003888  [2304128/2493556]\n",
      "loss: 0.047851  val loss: 0.047062  current lr: 0.003882  [2432128/2493556]\n",
      "epoch 5:\n",
      "loss: 0.048553  val loss: 0.044965  current lr: 0.003879  [    128/2493556]\n",
      "loss: 0.046594  val loss: 0.047048  current lr: 0.003873  [ 128128/2493556]\n",
      "loss: 0.046046  val loss: 0.050227  current lr: 0.003867  [ 256128/2493556]\n",
      "loss: 0.047708  val loss: 0.047127  current lr: 0.003860  [ 384128/2493556]\n",
      "loss: 0.049087  val loss: 0.048718  current lr: 0.003854  [ 512128/2493556]\n",
      "loss: 0.046807  val loss: 0.047615  current lr: 0.003847  [ 640128/2493556]\n",
      "loss: 0.047581  val loss: 0.045581  current lr: 0.003840  [ 768128/2493556]\n",
      "loss: 0.045878  val loss: 0.044934  current lr: 0.003833  [ 896128/2493556]\n",
      "loss: 0.046141  val loss: 0.048790  current lr: 0.003826  [1024128/2493556]\n",
      "loss: 0.048545  val loss: 0.045854  current lr: 0.003818  [1152128/2493556]\n",
      "loss: 0.047564  val loss: 0.045929  current lr: 0.003811  [1280128/2493556]\n",
      "loss: 0.049425  val loss: 0.047025  current lr: 0.003803  [1408128/2493556]\n",
      "loss: 0.045309  val loss: 0.049121  current lr: 0.003795  [1536128/2493556]\n",
      "loss: 0.048382  val loss: 0.046128  current lr: 0.003787  [1664128/2493556]\n",
      "loss: 0.046363  val loss: 0.048465  current lr: 0.003779  [1792128/2493556]\n",
      "loss: 0.047271  val loss: 0.046598  current lr: 0.003771  [1920128/2493556]\n",
      "loss: 0.046315  val loss: 0.045624  current lr: 0.003762  [2048128/2493556]\n",
      "loss: 0.045113  val loss: 0.047231  current lr: 0.003754  [2176128/2493556]\n",
      "loss: 0.047290  val loss: 0.044755  current lr: 0.003745  [2304128/2493556]\n",
      "loss: 0.047030  val loss: 0.046661  current lr: 0.003736  [2432128/2493556]\n",
      "epoch 6:\n",
      "loss: 0.047199  val loss: 0.046077  current lr: 0.003732  [    128/2493556]\n",
      "loss: 0.045386  val loss: 0.048997  current lr: 0.003723  [ 128128/2493556]\n",
      "loss: 0.046904  val loss: 0.048060  current lr: 0.003714  [ 256128/2493556]\n",
      "loss: 0.045401  val loss: 0.045351  current lr: 0.003705  [ 384128/2493556]\n",
      "loss: 0.046705  val loss: 0.050270  current lr: 0.003695  [ 512128/2493556]\n",
      "loss: 0.045918  val loss: 0.049226  current lr: 0.003686  [ 640128/2493556]\n",
      "loss: 0.046419  val loss: 0.045224  current lr: 0.003676  [ 768128/2493556]\n",
      "loss: 0.047952  val loss: 0.044346  current lr: 0.003666  [ 896128/2493556]\n",
      "loss: 0.046236  val loss: 0.044895  current lr: 0.003656  [1024128/2493556]\n",
      "loss: 0.044917  val loss: 0.047280  current lr: 0.003646  [1152128/2493556]\n",
      "loss: 0.046197  val loss: 0.045485  current lr: 0.003636  [1280128/2493556]\n",
      "loss: 0.046184  val loss: 0.044464  current lr: 0.003625  [1408128/2493556]\n",
      "loss: 0.044510  val loss: 0.045166  current lr: 0.003615  [1536128/2493556]\n",
      "loss: 0.048763  val loss: 0.044538  current lr: 0.003604  [1664128/2493556]\n",
      "loss: 0.046128  val loss: 0.045006  current lr: 0.003593  [1792128/2493556]\n",
      "loss: 0.044365  val loss: 0.045056  current lr: 0.003582  [1920128/2493556]\n",
      "loss: 0.045942  val loss: 0.045928  current lr: 0.003571  [2048128/2493556]\n",
      "loss: 0.046679  val loss: 0.045027  current lr: 0.003560  [2176128/2493556]\n",
      "loss: 0.045211  val loss: 0.045642  current lr: 0.003549  [2304128/2493556]\n",
      "loss: 0.047336  val loss: 0.044946  current lr: 0.003538  [2432128/2493556]\n",
      "epoch 7:\n",
      "loss: 0.046118  val loss: 0.045933  current lr: 0.003532  [    128/2493556]\n",
      "loss: 0.044868  val loss: 0.043835  current lr: 0.003520  [ 128128/2493556]\n",
      "loss: 0.045018  val loss: 0.046991  current lr: 0.003509  [ 256128/2493556]\n",
      "loss: 0.048490  val loss: 0.045337  current lr: 0.003497  [ 384128/2493556]\n",
      "loss: 0.047300  val loss: 0.044792  current lr: 0.003485  [ 512128/2493556]\n",
      "loss: 0.045486  val loss: 0.044221  current lr: 0.003473  [ 640128/2493556]\n",
      "loss: 0.044880  val loss: 0.044050  current lr: 0.003461  [ 768128/2493556]\n",
      "loss: 0.045422  val loss: 0.045443  current lr: 0.003448  [ 896128/2493556]\n",
      "loss: 0.045782  val loss: 0.045972  current lr: 0.003436  [1024128/2493556]\n",
      "loss: 0.043273  val loss: 0.045531  current lr: 0.003424  [1152128/2493556]\n",
      "loss: 0.046274  val loss: 0.047142  current lr: 0.003411  [1280128/2493556]\n",
      "loss: 0.046061  val loss: 0.044633  current lr: 0.003398  [1408128/2493556]\n",
      "loss: 0.045060  val loss: 0.045255  current lr: 0.003385  [1536128/2493556]\n",
      "loss: 0.044326  val loss: 0.045530  current lr: 0.003372  [1664128/2493556]\n",
      "loss: 0.044713  val loss: 0.043914  current lr: 0.003359  [1792128/2493556]\n",
      "loss: 0.046930  val loss: 0.044562  current lr: 0.003346  [1920128/2493556]\n",
      "loss: 0.047527  val loss: 0.043369  current lr: 0.003333  [2048128/2493556]\n",
      "loss: 0.044306  val loss: 0.044889  current lr: 0.003319  [2176128/2493556]\n",
      "loss: 0.045536  val loss: 0.044844  current lr: 0.003306  [2304128/2493556]\n",
      "loss: 0.045171  val loss: 0.044717  current lr: 0.003292  [2432128/2493556]\n",
      "epoch 8:\n",
      "loss: 0.045203  val loss: 0.044404  current lr: 0.003286  [    128/2493556]\n",
      "loss: 0.045952  val loss: 0.047654  current lr: 0.003272  [ 128128/2493556]\n",
      "loss: 0.043127  val loss: 0.044977  current lr: 0.003258  [ 256128/2493556]\n",
      "loss: 0.045484  val loss: 0.044163  current lr: 0.003244  [ 384128/2493556]\n",
      "loss: 0.043909  val loss: 0.046308  current lr: 0.003230  [ 512128/2493556]\n",
      "loss: 0.045281  val loss: 0.045674  current lr: 0.003216  [ 640128/2493556]\n",
      "loss: 0.045290  val loss: 0.046053  current lr: 0.003201  [ 768128/2493556]\n",
      "loss: 0.043963  val loss: 0.044796  current lr: 0.003187  [ 896128/2493556]\n",
      "loss: 0.044845  val loss: 0.044392  current lr: 0.003173  [1024128/2493556]\n",
      "loss: 0.043770  val loss: 0.043318  current lr: 0.003158  [1152128/2493556]\n",
      "loss: 0.045614  val loss: 0.044830  current lr: 0.003143  [1280128/2493556]\n",
      "loss: 0.042498  val loss: 0.045368  current lr: 0.003129  [1408128/2493556]\n",
      "loss: 0.046219  val loss: 0.044101  current lr: 0.003114  [1536128/2493556]\n",
      "loss: 0.045418  val loss: 0.043800  current lr: 0.003099  [1664128/2493556]\n",
      "loss: 0.044226  val loss: 0.045888  current lr: 0.003084  [1792128/2493556]\n",
      "loss: 0.044646  val loss: 0.044536  current lr: 0.003069  [1920128/2493556]\n",
      "loss: 0.045555  val loss: 0.044352  current lr: 0.003054  [2048128/2493556]\n",
      "loss: 0.045151  val loss: 0.044113  current lr: 0.003038  [2176128/2493556]\n",
      "loss: 0.045021  val loss: 0.044301  current lr: 0.003023  [2304128/2493556]\n",
      "loss: 0.044439  val loss: 0.042703  current lr: 0.003007  [2432128/2493556]\n",
      "epoch 9:\n",
      "loss: 0.044468  val loss: 0.047701  current lr: 0.003000  [    128/2493556]\n",
      "loss: 0.043189  val loss: 0.044018  current lr: 0.002984  [ 128128/2493556]\n",
      "loss: 0.044673  val loss: 0.043855  current lr: 0.002969  [ 256128/2493556]\n",
      "loss: 0.043680  val loss: 0.044430  current lr: 0.002953  [ 384128/2493556]\n",
      "loss: 0.045097  val loss: 0.043728  current lr: 0.002937  [ 512128/2493556]\n",
      "loss: 0.044914  val loss: 0.042499  current lr: 0.002921  [ 640128/2493556]\n",
      "loss: 0.043715  val loss: 0.043266  current lr: 0.002905  [ 768128/2493556]\n",
      "loss: 0.044138  val loss: 0.043428  current lr: 0.002889  [ 896128/2493556]\n",
      "loss: 0.043752  val loss: 0.044574  current lr: 0.002873  [1024128/2493556]\n",
      "loss: 0.044418  val loss: 0.043215  current lr: 0.002857  [1152128/2493556]\n",
      "loss: 0.044495  val loss: 0.042603  current lr: 0.002841  [1280128/2493556]\n",
      "loss: 0.044266  val loss: 0.043211  current lr: 0.002825  [1408128/2493556]\n",
      "loss: 0.046453  val loss: 0.043174  current lr: 0.002808  [1536128/2493556]\n",
      "loss: 0.043166  val loss: 0.044404  current lr: 0.002792  [1664128/2493556]\n",
      "loss: 0.042942  val loss: 0.045469  current lr: 0.002775  [1792128/2493556]\n",
      "loss: 0.042823  val loss: 0.043114  current lr: 0.002759  [1920128/2493556]\n",
      "loss: 0.044410  val loss: 0.042422  current lr: 0.002742  [2048128/2493556]\n",
      "loss: 0.044747  val loss: 0.043678  current lr: 0.002726  [2176128/2493556]\n",
      "loss: 0.044725  val loss: 0.043441  current lr: 0.002709  [2304128/2493556]\n",
      "loss: 0.042953  val loss: 0.045722  current lr: 0.002692  [2432128/2493556]\n",
      "epoch 10:\n",
      "loss: 0.041781  val loss: 0.043272  current lr: 0.002684  [    128/2493556]\n",
      "loss: 0.042961  val loss: 0.042971  current lr: 0.002667  [ 128128/2493556]\n",
      "loss: 0.042709  val loss: 0.043422  current lr: 0.002650  [ 256128/2493556]\n",
      "loss: 0.044393  val loss: 0.042571  current lr: 0.002633  [ 384128/2493556]\n",
      "loss: 0.041423  val loss: 0.043351  current lr: 0.002616  [ 512128/2493556]\n",
      "loss: 0.042994  val loss: 0.043525  current lr: 0.002599  [ 640128/2493556]\n",
      "loss: 0.043386  val loss: 0.043454  current lr: 0.002582  [ 768128/2493556]\n",
      "loss: 0.041909  val loss: 0.041877  current lr: 0.002565  [ 896128/2493556]\n",
      "loss: 0.044597  val loss: 0.043329  current lr: 0.002548  [1024128/2493556]\n",
      "loss: 0.045130  val loss: 0.042803  current lr: 0.002530  [1152128/2493556]\n",
      "loss: 0.044366  val loss: 0.044354  current lr: 0.002513  [1280128/2493556]\n",
      "loss: 0.042738  val loss: 0.042531  current lr: 0.002496  [1408128/2493556]\n",
      "loss: 0.044256  val loss: 0.043843  current lr: 0.002478  [1536128/2493556]\n",
      "loss: 0.043289  val loss: 0.042907  current lr: 0.002461  [1664128/2493556]\n",
      "loss: 0.044763  val loss: 0.042014  current lr: 0.002444  [1792128/2493556]\n",
      "loss: 0.044137  val loss: 0.042417  current lr: 0.002426  [1920128/2493556]\n",
      "loss: 0.042417  val loss: 0.042438  current lr: 0.002409  [2048128/2493556]\n",
      "loss: 0.044068  val loss: 0.045100  current lr: 0.002391  [2176128/2493556]\n",
      "loss: 0.042408  val loss: 0.042424  current lr: 0.002373  [2304128/2493556]\n",
      "loss: 0.043021  val loss: 0.042635  current lr: 0.002356  [2432128/2493556]\n",
      "epoch 11:\n",
      "loss: 0.042616  val loss: 0.042308  current lr: 0.002347  [    128/2493556]\n",
      "loss: 0.040074  val loss: 0.041714  current lr: 0.002330  [ 128128/2493556]\n",
      "loss: 0.042269  val loss: 0.042343  current lr: 0.002312  [ 256128/2493556]\n",
      "loss: 0.042465  val loss: 0.042110  current lr: 0.002294  [ 384128/2493556]\n",
      "loss: 0.043195  val loss: 0.041788  current lr: 0.002276  [ 512128/2493556]\n",
      "loss: 0.043650  val loss: 0.042246  current lr: 0.002259  [ 640128/2493556]\n",
      "loss: 0.044209  val loss: 0.042379  current lr: 0.002241  [ 768128/2493556]\n",
      "loss: 0.043384  val loss: 0.041987  current lr: 0.002223  [ 896128/2493556]\n",
      "loss: 0.041906  val loss: 0.042239  current lr: 0.002205  [1024128/2493556]\n",
      "loss: 0.041851  val loss: 0.042254  current lr: 0.002188  [1152128/2493556]\n",
      "loss: 0.040509  val loss: 0.041703  current lr: 0.002170  [1280128/2493556]\n",
      "loss: 0.043419  val loss: 0.044311  current lr: 0.002152  [1408128/2493556]\n",
      "loss: 0.043932  val loss: 0.041959  current lr: 0.002134  [1536128/2493556]\n",
      "loss: 0.043544  val loss: 0.042602  current lr: 0.002116  [1664128/2493556]\n",
      "loss: 0.044579  val loss: 0.041352  current lr: 0.002098  [1792128/2493556]\n",
      "loss: 0.042781  val loss: 0.042774  current lr: 0.002080  [1920128/2493556]\n",
      "loss: 0.042785  val loss: 0.042344  current lr: 0.002062  [2048128/2493556]\n",
      "loss: 0.043295  val loss: 0.041548  current lr: 0.002044  [2176128/2493556]\n",
      "loss: 0.041296  val loss: 0.042431  current lr: 0.002027  [2304128/2493556]\n",
      "loss: 0.041530  val loss: 0.041781  current lr: 0.002009  [2432128/2493556]\n",
      "epoch 12:\n",
      "loss: 0.042321  val loss: 0.042140  current lr: 0.002000  [    128/2493556]\n",
      "loss: 0.042265  val loss: 0.042142  current lr: 0.001982  [ 128128/2493556]\n",
      "loss: 0.041908  val loss: 0.043078  current lr: 0.001964  [ 256128/2493556]\n",
      "loss: 0.040921  val loss: 0.042041  current lr: 0.001946  [ 384128/2493556]\n",
      "loss: 0.041890  val loss: 0.042024  current lr: 0.001928  [ 512128/2493556]\n",
      "loss: 0.041598  val loss: 0.041782  current lr: 0.001910  [ 640128/2493556]\n",
      "loss: 0.042383  val loss: 0.041806  current lr: 0.001893  [ 768128/2493556]\n",
      "loss: 0.043492  val loss: 0.041464  current lr: 0.001875  [ 896128/2493556]\n",
      "loss: 0.042878  val loss: 0.041419  current lr: 0.001857  [1024128/2493556]\n",
      "loss: 0.044019  val loss: 0.043129  current lr: 0.001839  [1152128/2493556]\n",
      "loss: 0.040795  val loss: 0.042234  current lr: 0.001821  [1280128/2493556]\n",
      "loss: 0.043205  val loss: 0.041695  current lr: 0.001803  [1408128/2493556]\n",
      "loss: 0.040353  val loss: 0.041353  current lr: 0.001785  [1536128/2493556]\n",
      "loss: 0.041704  val loss: 0.041265  current lr: 0.001768  [1664128/2493556]\n",
      "loss: 0.041305  val loss: 0.042168  current lr: 0.001750  [1792128/2493556]\n",
      "loss: 0.040823  val loss: 0.041651  current lr: 0.001732  [1920128/2493556]\n",
      "loss: 0.043171  val loss: 0.041825  current lr: 0.001714  [2048128/2493556]\n",
      "loss: 0.041918  val loss: 0.041725  current lr: 0.001697  [2176128/2493556]\n",
      "loss: 0.041944  val loss: 0.041016  current lr: 0.001679  [2304128/2493556]\n",
      "loss: 0.039732  val loss: 0.041684  current lr: 0.001661  [2432128/2493556]\n",
      "epoch 13:\n",
      "loss: 0.041905  val loss: 0.042293  current lr: 0.001653  [    128/2493556]\n",
      "loss: 0.042899  val loss: 0.041073  current lr: 0.001635  [ 128128/2493556]\n",
      "loss: 0.042244  val loss: 0.040968  current lr: 0.001617  [ 256128/2493556]\n",
      "loss: 0.041054  val loss: 0.041179  current lr: 0.001600  [ 384128/2493556]\n",
      "loss: 0.041305  val loss: 0.041012  current lr: 0.001582  [ 512128/2493556]\n",
      "loss: 0.042030  val loss: 0.041003  current lr: 0.001565  [ 640128/2493556]\n",
      "loss: 0.040379  val loss: 0.040968  current lr: 0.001547  [ 768128/2493556]\n",
      "loss: 0.040943  val loss: 0.040808  current lr: 0.001530  [ 896128/2493556]\n",
      "loss: 0.041094  val loss: 0.040890  current lr: 0.001513  [1024128/2493556]\n",
      "loss: 0.042406  val loss: 0.042318  current lr: 0.001495  [1152128/2493556]\n",
      "loss: 0.040435  val loss: 0.041846  current lr: 0.001478  [1280128/2493556]\n",
      "loss: 0.042201  val loss: 0.041423  current lr: 0.001461  [1408128/2493556]\n",
      "loss: 0.041450  val loss: 0.041173  current lr: 0.001443  [1536128/2493556]\n",
      "loss: 0.039918  val loss: 0.040839  current lr: 0.001426  [1664128/2493556]\n",
      "loss: 0.041412  val loss: 0.041127  current lr: 0.001409  [1792128/2493556]\n",
      "loss: 0.042070  val loss: 0.041638  current lr: 0.001392  [1920128/2493556]\n",
      "loss: 0.038880  val loss: 0.041035  current lr: 0.001375  [2048128/2493556]\n",
      "loss: 0.040163  val loss: 0.040810  current lr: 0.001358  [2176128/2493556]\n",
      "loss: 0.039707  val loss: 0.040970  current lr: 0.001341  [2304128/2493556]\n",
      "loss: 0.041474  val loss: 0.042031  current lr: 0.001324  [2432128/2493556]\n",
      "epoch 14:\n",
      "loss: 0.042869  val loss: 0.041310  current lr: 0.001316  [    128/2493556]\n",
      "loss: 0.041688  val loss: 0.041502  current lr: 0.001299  [ 128128/2493556]\n",
      "loss: 0.040899  val loss: 0.041004  current lr: 0.001282  [ 256128/2493556]\n",
      "loss: 0.040902  val loss: 0.040906  current lr: 0.001266  [ 384128/2493556]\n",
      "loss: 0.039783  val loss: 0.040286  current lr: 0.001249  [ 512128/2493556]\n",
      "loss: 0.039494  val loss: 0.040500  current lr: 0.001232  [ 640128/2493556]\n",
      "loss: 0.041327  val loss: 0.040258  current lr: 0.001216  [ 768128/2493556]\n",
      "loss: 0.039537  val loss: 0.040476  current lr: 0.001200  [ 896128/2493556]\n",
      "loss: 0.043698  val loss: 0.040080  current lr: 0.001183  [1024128/2493556]\n",
      "loss: 0.040344  val loss: 0.040655  current lr: 0.001167  [1152128/2493556]\n",
      "loss: 0.039716  val loss: 0.040725  current lr: 0.001151  [1280128/2493556]\n",
      "loss: 0.040237  val loss: 0.040252  current lr: 0.001134  [1408128/2493556]\n",
      "loss: 0.040160  val loss: 0.040017  current lr: 0.001118  [1536128/2493556]\n",
      "loss: 0.040791  val loss: 0.040262  current lr: 0.001102  [1664128/2493556]\n",
      "loss: 0.039856  val loss: 0.040511  current lr: 0.001086  [1792128/2493556]\n",
      "loss: 0.040938  val loss: 0.040399  current lr: 0.001070  [1920128/2493556]\n",
      "loss: 0.040682  val loss: 0.040280  current lr: 0.001054  [2048128/2493556]\n",
      "loss: 0.039061  val loss: 0.040221  current lr: 0.001039  [2176128/2493556]\n",
      "loss: 0.039178  val loss: 0.040148  current lr: 0.001023  [2304128/2493556]\n",
      "loss: 0.040781  val loss: 0.040656  current lr: 0.001007  [2432128/2493556]\n",
      "epoch 15:\n",
      "loss: 0.040458  val loss: 0.039987  current lr: 0.001000  [    128/2493556]\n",
      "loss: 0.038909  val loss: 0.040125  current lr: 0.000985  [ 128128/2493556]\n",
      "loss: 0.040772  val loss: 0.040258  current lr: 0.000969  [ 256128/2493556]\n",
      "loss: 0.041460  val loss: 0.040428  current lr: 0.000954  [ 384128/2493556]\n",
      "loss: 0.039247  val loss: 0.040467  current lr: 0.000939  [ 512128/2493556]\n",
      "loss: 0.041125  val loss: 0.040335  current lr: 0.000923  [ 640128/2493556]\n",
      "loss: 0.039120  val loss: 0.040231  current lr: 0.000908  [ 768128/2493556]\n",
      "loss: 0.040870  val loss: 0.040458  current lr: 0.000893  [ 896128/2493556]\n",
      "loss: 0.039306  val loss: 0.039643  current lr: 0.000879  [1024128/2493556]\n",
      "loss: 0.039482  val loss: 0.040322  current lr: 0.000864  [1152128/2493556]\n",
      "loss: 0.038762  val loss: 0.039416  current lr: 0.000849  [1280128/2493556]\n",
      "loss: 0.037958  val loss: 0.039768  current lr: 0.000834  [1408128/2493556]\n",
      "loss: 0.039639  val loss: 0.040048  current lr: 0.000820  [1536128/2493556]\n",
      "loss: 0.039780  val loss: 0.040583  current lr: 0.000806  [1664128/2493556]\n",
      "loss: 0.038516  val loss: 0.039638  current lr: 0.000791  [1792128/2493556]\n",
      "loss: 0.040343  val loss: 0.039585  current lr: 0.000777  [1920128/2493556]\n",
      "loss: 0.039647  val loss: 0.039723  current lr: 0.000763  [2048128/2493556]\n",
      "loss: 0.040487  val loss: 0.039430  current lr: 0.000749  [2176128/2493556]\n",
      "loss: 0.039208  val loss: 0.039790  current lr: 0.000735  [2304128/2493556]\n",
      "loss: 0.038971  val loss: 0.039371  current lr: 0.000721  [2432128/2493556]\n",
      "epoch 16:\n",
      "loss: 0.041391  val loss: 0.039644  current lr: 0.000714  [    128/2493556]\n",
      "loss: 0.037365  val loss: 0.039550  current lr: 0.000701  [ 128128/2493556]\n",
      "loss: 0.039765  val loss: 0.039476  current lr: 0.000687  [ 256128/2493556]\n",
      "loss: 0.039318  val loss: 0.039739  current lr: 0.000674  [ 384128/2493556]\n",
      "loss: 0.038819  val loss: 0.039561  current lr: 0.000660  [ 512128/2493556]\n",
      "loss: 0.039351  val loss: 0.039354  current lr: 0.000647  [ 640128/2493556]\n",
      "loss: 0.037610  val loss: 0.039442  current lr: 0.000634  [ 768128/2493556]\n",
      "loss: 0.039356  val loss: 0.039547  current lr: 0.000621  [ 896128/2493556]\n",
      "loss: 0.041223  val loss: 0.039860  current lr: 0.000608  [1024128/2493556]\n",
      "loss: 0.038291  val loss: 0.039478  current lr: 0.000595  [1152128/2493556]\n",
      "loss: 0.040596  val loss: 0.039031  current lr: 0.000583  [1280128/2493556]\n",
      "loss: 0.038433  val loss: 0.039279  current lr: 0.000570  [1408128/2493556]\n",
      "loss: 0.038594  val loss: 0.039180  current lr: 0.000557  [1536128/2493556]\n",
      "loss: 0.038068  val loss: 0.039163  current lr: 0.000545  [1664128/2493556]\n",
      "loss: 0.038419  val loss: 0.039973  current lr: 0.000533  [1792128/2493556]\n",
      "loss: 0.039220  val loss: 0.039258  current lr: 0.000521  [1920128/2493556]\n",
      "loss: 0.039190  val loss: 0.039336  current lr: 0.000509  [2048128/2493556]\n",
      "loss: 0.040306  val loss: 0.038914  current lr: 0.000497  [2176128/2493556]\n",
      "loss: 0.038304  val loss: 0.039260  current lr: 0.000485  [2304128/2493556]\n",
      "loss: 0.038269  val loss: 0.039102  current lr: 0.000473  [2432128/2493556]\n",
      "epoch 17:\n",
      "loss: 0.039967  val loss: 0.039020  current lr: 0.000468  [    128/2493556]\n",
      "loss: 0.038662  val loss: 0.038934  current lr: 0.000456  [ 128128/2493556]\n",
      "loss: 0.038338  val loss: 0.038946  current lr: 0.000445  [ 256128/2493556]\n",
      "loss: 0.038129  val loss: 0.039141  current lr: 0.000434  [ 384128/2493556]\n",
      "loss: 0.038048  val loss: 0.038937  current lr: 0.000423  [ 512128/2493556]\n",
      "loss: 0.037421  val loss: 0.038918  current lr: 0.000412  [ 640128/2493556]\n",
      "loss: 0.039619  val loss: 0.038861  current lr: 0.000401  [ 768128/2493556]\n",
      "loss: 0.040831  val loss: 0.038993  current lr: 0.000390  [ 896128/2493556]\n",
      "loss: 0.036856  val loss: 0.038970  current lr: 0.000380  [1024128/2493556]\n",
      "loss: 0.038191  val loss: 0.038975  current lr: 0.000369  [1152128/2493556]\n",
      "loss: 0.038609  val loss: 0.038789  current lr: 0.000359  [1280128/2493556]\n",
      "loss: 0.037430  val loss: 0.038762  current lr: 0.000349  [1408128/2493556]\n",
      "loss: 0.037587  val loss: 0.038778  current lr: 0.000339  [1536128/2493556]\n",
      "loss: 0.039378  val loss: 0.038652  current lr: 0.000329  [1664128/2493556]\n",
      "loss: 0.037428  val loss: 0.038731  current lr: 0.000319  [1792128/2493556]\n",
      "loss: 0.037723  val loss: 0.038672  current lr: 0.000309  [1920128/2493556]\n",
      "loss: 0.037264  val loss: 0.038808  current lr: 0.000300  [2048128/2493556]\n",
      "loss: 0.037567  val loss: 0.038586  current lr: 0.000291  [2176128/2493556]\n",
      "loss: 0.039539  val loss: 0.038626  current lr: 0.000281  [2304128/2493556]\n",
      "loss: 0.038485  val loss: 0.038631  current lr: 0.000272  [2432128/2493556]\n",
      "epoch 18:\n",
      "loss: 0.037858  val loss: 0.038633  current lr: 0.000268  [    128/2493556]\n",
      "loss: 0.036944  val loss: 0.038526  current lr: 0.000259  [ 128128/2493556]\n",
      "loss: 0.037913  val loss: 0.038562  current lr: 0.000250  [ 256128/2493556]\n",
      "loss: 0.037198  val loss: 0.038499  current lr: 0.000242  [ 384128/2493556]\n",
      "loss: 0.037553  val loss: 0.038450  current lr: 0.000233  [ 512128/2493556]\n",
      "loss: 0.039524  val loss: 0.038597  current lr: 0.000225  [ 640128/2493556]\n",
      "loss: 0.038063  val loss: 0.038453  current lr: 0.000217  [ 768128/2493556]\n",
      "loss: 0.037493  val loss: 0.038474  current lr: 0.000209  [ 896128/2493556]\n",
      "loss: 0.037849  val loss: 0.038373  current lr: 0.000201  [1024128/2493556]\n",
      "loss: 0.037412  val loss: 0.038653  current lr: 0.000193  [1152128/2493556]\n",
      "loss: 0.039559  val loss: 0.038463  current lr: 0.000185  [1280128/2493556]\n",
      "loss: 0.036086  val loss: 0.038384  current lr: 0.000178  [1408128/2493556]\n",
      "loss: 0.036813  val loss: 0.038314  current lr: 0.000171  [1536128/2493556]\n",
      "loss: 0.037605  val loss: 0.038445  current lr: 0.000164  [1664128/2493556]\n",
      "loss: 0.036438  val loss: 0.038395  current lr: 0.000156  [1792128/2493556]\n",
      "loss: 0.036815  val loss: 0.038281  current lr: 0.000150  [1920128/2493556]\n",
      "loss: 0.037953  val loss: 0.038349  current lr: 0.000143  [2048128/2493556]\n",
      "loss: 0.037335  val loss: 0.038403  current lr: 0.000136  [2176128/2493556]\n",
      "loss: 0.037517  val loss: 0.038252  current lr: 0.000130  [2304128/2493556]\n",
      "loss: 0.037967  val loss: 0.038364  current lr: 0.000124  [2432128/2493556]\n",
      "epoch 19:\n",
      "loss: 0.039023  val loss: 0.038320  current lr: 0.000121  [    128/2493556]\n",
      "loss: 0.036177  val loss: 0.038226  current lr: 0.000115  [ 128128/2493556]\n",
      "loss: 0.038756  val loss: 0.038350  current lr: 0.000109  [ 256128/2493556]\n",
      "loss: 0.036926  val loss: 0.038255  current lr: 0.000103  [ 384128/2493556]\n",
      "loss: 0.036894  val loss: 0.038277  current lr: 0.000097  [ 512128/2493556]\n",
      "loss: 0.036053  val loss: 0.038253  current lr: 0.000092  [ 640128/2493556]\n",
      "loss: 0.038637  val loss: 0.038211  current lr: 0.000087  [ 768128/2493556]\n",
      "loss: 0.036215  val loss: 0.038233  current lr: 0.000081  [ 896128/2493556]\n",
      "loss: 0.036565  val loss: 0.038288  current lr: 0.000076  [1024128/2493556]\n",
      "loss: 0.035523  val loss: 0.038232  current lr: 0.000072  [1152128/2493556]\n",
      "loss: 0.038047  val loss: 0.038152  current lr: 0.000067  [1280128/2493556]\n",
      "loss: 0.037691  val loss: 0.038170  current lr: 0.000062  [1408128/2493556]\n",
      "loss: 0.035486  val loss: 0.038172  current lr: 0.000058  [1536128/2493556]\n",
      "loss: 0.036815  val loss: 0.038158  current lr: 0.000054  [1664128/2493556]\n",
      "loss: 0.038045  val loss: 0.038158  current lr: 0.000050  [1792128/2493556]\n",
      "loss: 0.036393  val loss: 0.038138  current lr: 0.000046  [1920128/2493556]\n",
      "loss: 0.038428  val loss: 0.038083  current lr: 0.000042  [2048128/2493556]\n",
      "loss: 0.037069  val loss: 0.038125  current lr: 0.000039  [2176128/2493556]\n",
      "loss: 0.037810  val loss: 0.038085  current lr: 0.000035  [2304128/2493556]\n",
      "loss: 0.036460  val loss: 0.038079  current lr: 0.000032  [2432128/2493556]\n",
      "epoch 20:\n",
      "loss: 0.037902  val loss: 0.038092  current lr: 0.000030  [    128/2493556]\n",
      "loss: 0.036697  val loss: 0.038084  current lr: 0.000027  [ 128128/2493556]\n",
      "loss: 0.036753  val loss: 0.038090  current lr: 0.000025  [ 256128/2493556]\n",
      "loss: 0.039138  val loss: 0.038099  current lr: 0.000022  [ 384128/2493556]\n",
      "loss: 0.036699  val loss: 0.038094  current lr: 0.000019  [ 512128/2493556]\n",
      "loss: 0.035185  val loss: 0.038108  current lr: 0.000017  [ 640128/2493556]\n",
      "loss: 0.036868  val loss: 0.038109  current lr: 0.000015  [ 768128/2493556]\n",
      "loss: 0.036136  val loss: 0.038088  current lr: 0.000013  [ 896128/2493556]\n",
      "loss: 0.035584  val loss: 0.038085  current lr: 0.000011  [1024128/2493556]\n",
      "loss: 0.036823  val loss: 0.038071  current lr: 0.000009  [1152128/2493556]\n",
      "loss: 0.037569  val loss: 0.038071  current lr: 0.000007  [1280128/2493556]\n",
      "loss: 0.036938  val loss: 0.038072  current lr: 0.000006  [1408128/2493556]\n",
      "loss: 0.037242  val loss: 0.038068  current lr: 0.000005  [1536128/2493556]\n",
      "loss: 0.038690  val loss: 0.038064  current lr: 0.000003  [1664128/2493556]\n",
      "loss: 0.037129  val loss: 0.038063  current lr: 0.000002  [1792128/2493556]\n",
      "loss: 0.036420  val loss: 0.038065  current lr: 0.000002  [1920128/2493556]\n",
      "loss: 0.036697  val loss: 0.038064  current lr: 0.000001  [2048128/2493556]\n",
      "loss: 0.036784  val loss: 0.038064  current lr: 0.000001  [2176128/2493556]\n",
      "loss: 0.034780  val loss: 0.038064  current lr: 0.000000  [2304128/2493556]\n",
      "loss: 0.035917  val loss: 0.038064  current lr: 0.000000  [2432128/2493556]\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    training_parameters=tparam,\n",
    "    checkpoint_path='checkpoints/best.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader for submission\n",
    "test_data = MushroomDataset(n_bins=mparam.num_bins, subset='test', preprocessors=[train_data.preprocessor])\n",
    "test_loader = DataLoader(test_data, batch_size=tparam.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(model, test_loader, submission_path='submission.csv'):\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in test_loader:\n",
    "            X = X.to(device)\n",
    "            logits, _ = model(X, None)\n",
    "            probs = F.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long()\n",
    "            predicted_labels += train_data.label_enc.inverse_transform(preds.cpu().squeeze()).tolist()\n",
    "\n",
    "    submission_df = pd.DataFrame({'class':predicted_labels})\n",
    "    submission_df['id'] = pd.read_csv('test.csv', usecols=['id'])\n",
    "    submission_df[['id', 'class']].to_csv(submission_path, index=False)\n",
    "\n",
    "    print(f'Submission file created at {submission_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created at submission.csv\n"
     ]
    }
   ],
   "source": [
    "make_submission(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    submission_path='submission.csv'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
