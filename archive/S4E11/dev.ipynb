{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import optuna\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', index_col=0)\n",
    "df.rename(columns={\n",
    "    'Have you ever had suicidal thoughts ?':'suicidal_thoughts',\n",
    "    'Family History of Mental Illness':'family_history'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.read_csv('test.csv', index_col=0)\n",
    "dftest.rename(columns={\n",
    "    'Have you ever had suicidal thoughts ?':'suicidal_thoughts',\n",
    "    'Family History of Mental Illness':'family_history'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(df):\n",
    "    df['job'] = df.apply(lambda row: 'Student' if row['Working Professional or Student'] == 'Student' else row['Profession'], axis=1)\n",
    "    df = df.fillna({'job':'Unemployed'})\n",
    "    df['pressure'] = df.apply(lambda row: row['Work Pressure'] if row['Working Professional or Student'] == 'Working Professional' else row['Academic Pressure'], axis=1)\n",
    "    df['satisfaction'] = df.apply(lambda row: row['Job Satisfaction'] if row['Working Professional or Student'] == 'Working Professional' else row['Study Satisfaction'], axis=1)\n",
    "    degree_mapping = {\n",
    "        'Class 12': 'High School',\n",
    "        'B.Ed': 'Undergraduate',\n",
    "        'B.Arch': 'Undergraduate',\n",
    "        'B.Com': 'Undergraduate',\n",
    "        'B.Pharm': 'Undergraduate',\n",
    "        'BCA': 'Undergraduate',\n",
    "        'BBA': 'Undergraduate',\n",
    "        'BSc': 'Undergraduate',\n",
    "        'B.Tech': 'Undergraduate',\n",
    "        'LLB': 'Undergraduate',\n",
    "        'BHM': 'Undergraduate',\n",
    "        'BA': 'Undergraduate',\n",
    "        'BE': 'Undergraduate',\n",
    "        'MBBS': 'Undergraduate',\n",
    "        'M.Ed': 'Postgraduate',\n",
    "        'MCA': 'Postgraduate',\n",
    "        'MSc': 'Postgraduate',\n",
    "        'LLM': 'Postgraduate',\n",
    "        'M.Pharm': 'Postgraduate',\n",
    "        'M.Tech': 'Postgraduate',\n",
    "        'MBA': 'Postgraduate',\n",
    "        'ME': 'Postgraduate',\n",
    "        'MHM': 'Postgraduate',\n",
    "        'M.Com': 'Postgraduate',\n",
    "        'MA': 'Postgraduate',\n",
    "        'PhD': 'Doctorate',\n",
    "        'MD': 'Doctorate'\n",
    "    }\n",
    "    df['deg'] = df['Degree'].map(degree_mapping).fillna('other')\n",
    "    return df\n",
    "\n",
    "df = prep(df)\n",
    "dftest = prep(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_prep.csv')\n",
    "dftest.to_csv('test_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape (112560, 12)\n",
      "Xte shape (28140, 12)\n",
      "ytr shape (112560,)\n",
      "yte shape (28140,)\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    'Gender',\n",
    "    'job',\n",
    "    'Sleep Duration',\n",
    "    'Dietary Habits',\n",
    "    'deg',\n",
    "    'suicidal_thoughts',\n",
    "    'family_history'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'Age',\n",
    "    'pressure',\n",
    "    'satisfaction',\n",
    "    'Work/Study Hours',\n",
    "    'Financial Stress'\n",
    "]\n",
    "\n",
    "target = 'Depression'\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    df[numerical_features + categorical_features],\n",
    "    df[target],\n",
    "    stratify=df['Depression'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'Xtr shape {Xtr.shape}')\n",
    "print(f'Xte shape {Xte.shape}')\n",
    "print(f'ytr shape {ytr.shape}')\n",
    "print(f'yte shape {yte.shape}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM Benchmark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(min_frequency=0.1, handle_unknown='infrequent_if_exist'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear'])\n",
    "    if kernel == 'rbf' or kernel == 'poly':\n",
    "        gamma = trial.suggest_float('gamma', 1e-4, 1e0, log=True)\n",
    "    else:\n",
    "        gamma = 'scale'  # Not used for linear kernel\n",
    "    \n",
    "    pipeline.set_params(\n",
    "        classifier__C=C,\n",
    "        classifier__kernel=kernel,\n",
    "        classifier__gamma=gamma\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(pipeline, Xtr, ytr, cv=3, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(study.best_params)\n",
    "\n",
    "best_params = study.best_params\n",
    "pipeline.set_params(\n",
    "    classifier__C=best_params['C'],\n",
    "    classifier__kernel=best_params['kernel'],\n",
    "    classifier__gamma=best_params['gamma']\n",
    ")\n",
    "\n",
    "pipeline.fit(Xtr, ytr)\n",
    "\n",
    "test_accuracy = pipeline.score(Xte, yte)\n",
    "print(f\"Test accuracy: {test_accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    df[numerical_features + categorical_features],\n",
    "    df[target],\n",
    "    stratify=df['Depression'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'Xtr shape {Xtr.shape}')\n",
    "print(f'Xte shape {Xte.shape}')\n",
    "print(f'ytr shape {ytr.shape}')\n",
    "print(f'yte shape {yte.shape}') \n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(min_frequency=0.1, handle_unknown='infrequent_if_exist'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 1500)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 25)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "\n",
    "    pipeline.set_params(\n",
    "        classifier__n_estimators=n_estimators,\n",
    "        classifier__max_depth=max_depth,\n",
    "        classifier__learning_rate=learning_rate,\n",
    "        classifier__subsample=subsample,\n",
    "        classifier__colsample_bytree=colsample_bytree,\n",
    "        classifier__min_child_weight=min_child_weight\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(pipeline, Xtr, ytr, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "\n",
    "best_params = study.best_params\n",
    "pipeline.set_params(\n",
    "    classifier__n_estimators=best_params['n_estimators'],\n",
    "    classifier__max_depth=best_params['max_depth'],\n",
    "    classifier__learning_rate=best_params['learning_rate'],\n",
    "    classifier__subsample=best_params['subsample'],\n",
    "    classifier__colsample_bytree=best_params['colsample_bytree'],\n",
    "    classifier__min_child_weight=best_params['min_child_weight']\n",
    ")\n",
    "\n",
    "pipeline.fit(Xtr, ytr)\n",
    "\n",
    "test_accuracy = pipeline.score(Xte, yte)\n",
    "print(f\"Test accuracy: {test_accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TabPFN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9317073170731708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "#X, y = load_breast_cancer(return_X_y=True)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "testdf = df.sample(1024)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    testdf[numerical_features + categorical_features],\n",
    "    testdf[target],\n",
    "    stratify=testdf[target],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# N_ensemble_configurations controls the number of model predictions that are ensembled with feature and class rotations (See our work for details).\n",
    "# When N_ensemble_configurations > #features * #classes, no further averaging is applied.\n",
    "\n",
    "\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(min_frequency=0.1, handle_unknown='infrequent_if_exist'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "labenc = LabelEncoder()\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "y_train = labenc.fit_transform(y_train)\n",
    "\n",
    "classifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=32)\n",
    "classifier.fit(X_train, y_train, overwrite_warning=True)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "y_eval, p_eval = classifier.predict(X_test, return_winning_probability=True)\n",
    "\n",
    "print('Accuracy', accuracy_score(y_test, y_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AutoGluon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241204_124838\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.2\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.0.0: Mon Aug 12 20:51:54 PDT 2024; root:xnu-11215.1.10~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Memory Avail:       4.78 GB / 16.00 GB (29.9%)\n",
      "Disk Space Avail:   221.66 GB / 460.43 GB (48.1%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Loaded data from: train_prep.csv | Columns = 24 / 24 | Rows = 140700 -> 140700\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "python(8358) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(8359) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(8360) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(8361) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(8362) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(8363) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "2024-12-04 13:48:44,906\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/Users/haukesteffen/dev/TabularShenanigans/S4E11/AutogluonModels/ag-20241204_124838/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Beginning AutoGluon training ... Time limit = 893s\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m AutoGluon will save models to \"/Users/haukesteffen/dev/TabularShenanigans/S4E11/AutogluonModels/ag-20241204_124838/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Train Data Rows:    125066\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Train Data Columns: 23\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Label Column:       Depression\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tAvailable Memory:                    4846.92 MB\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tTrain Data (Original)  Memory Usage: 91.49 MB (1.9% of available memory)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t('float', [])  : 10 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t('int', [])    :  1 | ['id']\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t('object', []) : 12 | ['Name', 'Gender', 'City', 'Working Professional or Student', 'Profession', ...]\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t('category', [])  :  8 | ['Name', 'City', 'Profession', 'Sleep Duration', 'Dietary Habits', ...]\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t('float', [])     : 10 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t('int', [])       :  1 | ['id']\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t\t('int', ['bool']) :  4 | ['Gender', 'Working Professional or Student', 'suicidal_thoughts', 'family_history']\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.5s = Fit runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t23 features in original data used to generate 23 features in processed data.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tTrain Data (Processed) Memory Usage: 12.05 MB (0.3% of available memory)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 594.86s of the 892.51s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.8466\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 593.56s of the 891.21s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.8485\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.13s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 593.22s of the 890.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.98%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9397\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t4.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t1.64s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 586.38s of the 884.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.81%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.939\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t3.86s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t1.48s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 580.81s of the 878.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9356\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t5.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.3s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 572.96s of the 870.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9358\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t6.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.45s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 564.12s of the 861.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.50%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9398\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t112.49s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 449.64s of the 747.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9346\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t3.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.64s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 443.48s of the 741.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9347\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.89s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.81s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 437.47s of the 735.12s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.40%)\n",
      "\u001b[36m(_ray_fit pid=8499)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_ray_fit pid=8498)\u001b[0m No improvement since epoch 4: early stopping\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8495)\u001b[0m No improvement since epoch 7: early stopping\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9384\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t68.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.87s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 366.32s of the 663.97s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.54%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9396\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t114.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.79s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 249.12s of the 546.77s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=8501)\u001b[0m No improvement since epoch 9: early stopping\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.52%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9382\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t45.46s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 201.80s of the 499.45s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.90%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9389\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t5.48s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t1.6s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 193.76s of the 491.41s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.00%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9399\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t58.24s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 134.09s of the 431.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.68%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.939\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t66.77s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.56s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 65.14s of the 362.79s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.32%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.8183\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t1.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 61.92s of the 359.57s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.99%)\n",
      "\u001b[36m(_ray_fit pid=8824)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9389\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t50.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t1.68s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 9.56s of the 307.21s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.42%)\n",
      "\u001b[36m(_ray_fit pid=8866)\u001b[0m \tRan out of time, early stopping on iteration 38.\n",
      "\u001b[36m(_ray_fit pid=8818)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 8)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9345\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t7.39s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 0.34s of the 297.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.36%)\n",
      "\u001b[36m(_ray_fit pid=8877)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=8877)\u001b[0m \t[1]\tvalid_set's binary_error: 0.181667\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.8183\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.51s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 295.57s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tEnsemble Weights: {'CatBoost_r177_BAG_L1': 0.286, 'NeuralNetTorch_r79_BAG_L1': 0.286, 'LightGBMXT_BAG_L1': 0.143, 'NeuralNetFastAI_BAG_L1': 0.143, 'XGBoost_BAG_L1': 0.143}\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9403\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=8861)\u001b[0m \tRan out of time, early stopping on iteration 39.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 292.95s of the 292.90s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.63%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9399\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t3.16s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 288.16s of the 288.12s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=8874)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8874)\u001b[0m \t[1]\tvalid_set's binary_error: 0.181731\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.02%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9401\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.43s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 284.25s of the 284.20s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9401\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t16.38s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.47s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 265.21s of the 265.17s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9394\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t15.57s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.54s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 246.91s of the 246.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.53%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9405\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t20.68s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 224.67s of the 224.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9396\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t3.79s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.74s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 217.89s of the 217.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9397\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t3.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t2.77s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 211.07s of the 211.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.85%)\n",
      "\u001b[36m(_ray_fit pid=8976)\u001b[0m No improvement since epoch 1: early stopping\n",
      "\u001b[36m(_ray_fit pid=8982)\u001b[0m No improvement since epoch 6: early stopping\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9404\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t73.54s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.97s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 135.53s of the 135.48s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.22%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9402\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t56.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.43s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 77.55s of the 77.50s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=8980)\u001b[0m No improvement since epoch 8: early stopping\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.58%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9392\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t49.2s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.83s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 26.42s of the 26.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.90%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9397\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t3.86s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.53s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 20.35s of the 20.30s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.80%)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9404\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t15.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 2.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L2': 0.96, 'XGBoost_BAG_L2': 0.04}\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.9405\t = Validation score   (accuracy)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t4.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m AutoGluon training complete, total runtime = 894.28s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1374.0 rows/s (15634 batch size)\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Calibrating decision threshold to optimize metric accuracy | Checking 51 thresholds...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tBase Threshold: 0.500\t| val: 0.9405\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m \tBest Threshold: 0.500\t| val: 0.9405\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/haukesteffen/dev/TabularShenanigans/S4E11/AutogluonModels/ag-20241204_124838/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=8367)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                          model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0          CatBoost_r177_BAG_L1       0.941794   0.939856    accuracy        0.073967       0.110197   58.235724                 0.073967                0.110197          58.235724            1       True         14\n",
      "1          CatBoost_r177_BAG_L2       0.941794   0.940407    accuracy        6.995782      20.139444  573.563994                 0.028517                0.089858          15.607875            2       True         32\n",
      "2           WeightedEnsemble_L2       0.941666   0.940328    accuracy        2.767663       3.981457  316.058785                 0.001403                0.005730           2.606886            2       True         20\n",
      "3         ExtraTreesGini_BAG_L2       0.941666   0.939592    accuracy        7.294904      22.791488  561.745398                 0.327639                2.741902           3.789278            2       True         26\n",
      "4               CatBoost_BAG_L2       0.941602   0.940519    accuracy        7.000961      20.138235  578.639388                 0.033696                0.088649          20.683268            2       True         25\n",
      "5                XGBoost_BAG_L2       0.941538   0.940216    accuracy        7.294691      20.479347  613.963540                 0.327426                0.429762          56.007420            2       True         29\n",
      "6           WeightedEnsemble_L3       0.941538   0.940535    accuracy        7.329574      20.574158  638.865096                 0.001187                0.006161           4.218288            3       True         33\n",
      "7               CatBoost_BAG_L1       0.941410   0.939792    accuracy        0.276678       0.173677  112.492922                 0.276678                0.173677         112.492922            1       True          7\n",
      "8       RandomForestGini_BAG_L2       0.941410   0.940080    accuracy        7.195895      22.524096  574.331777                 0.228630                2.474510          16.375657            2       True         23\n",
      "9         ExtraTreesEntr_BAG_L2       0.941282   0.939672    accuracy        7.257673      22.815431  561.737888                 0.290408                2.765845           3.781769            2       True         27\n",
      "10  NeuralNetFastAI_r191_BAG_L1       0.941090   0.938880    accuracy        1.273186       1.677284   50.403014                 1.273186                1.677284          50.403014            1       True         17\n",
      "11            LightGBMXT_BAG_L2       0.941026   0.939888    accuracy        7.101886      20.488005  561.116163                 0.134621                0.438419           3.160043            2       True         21\n",
      "12      RandomForestEntr_BAG_L2       0.940834   0.939424    accuracy        7.165879      22.585509  573.526843                 0.198614                2.535923          15.570723            2       True         24\n",
      "13               XGBoost_BAG_L1       0.940770   0.939568    accuracy        0.349590       0.794114  114.898394                 0.349590                0.794114         114.898394            1       True         11\n",
      "14       NeuralNetFastAI_BAG_L2       0.940770   0.940359    accuracy        7.916198      21.015718  631.496575                 0.948933                0.966133          73.540456            2       True         28\n",
      "15       NeuralNetFastAI_BAG_L1       0.940706   0.938441    accuracy        1.311883       0.865894   68.904582                 1.311883                0.865894          68.904582            1       True         10\n",
      "16         LightGBMLarge_BAG_L2       0.940706   0.939680    accuracy        7.116188      20.583455  561.820472                 0.148923                0.533869           3.864352            2       True         31\n",
      "17        NeuralNetTorch_BAG_L2       0.940706   0.939248    accuracy        7.723669      20.884447  607.159932                 0.756404                0.834861          49.203812            2       True         30\n",
      "18            LightGBMXT_BAG_L1       0.940578   0.939688    accuracy        0.494885       1.644355    4.638735                 0.494885                1.644355           4.638735            1       True          3\n",
      "19              LightGBM_BAG_L2       0.940578   0.940056    accuracy        7.054505      20.254348  560.385325                 0.087240                0.204762           2.429205            2       True         22\n",
      "20    NeuralNetTorch_r79_BAG_L1       0.940258   0.938952    accuracy        0.535936       0.561166   66.774464                 0.535936                0.561166          66.774464            1       True         15\n",
      "21              LightGBM_BAG_L1       0.940130   0.938968    accuracy        0.249794       1.479741    3.858142                 0.249794                1.479741           3.858142            1       True          4\n",
      "22         LightGBMLarge_BAG_L1       0.939939   0.938928    accuracy        0.351990       1.596653    5.480528                 0.351990                1.596653           5.480528            1       True         13\n",
      "23        NeuralNetTorch_BAG_L1       0.939491   0.938241    accuracy        0.446347       0.435546   45.458849                 0.446347                0.435546          45.458849            1       True         12\n",
      "24      RandomForestEntr_BAG_L1       0.937764   0.935802    accuracy        0.269438       2.447712    6.138732                 0.269438                2.447712           6.138732            1       True          6\n",
      "25      RandomForestGini_BAG_L1       0.937508   0.935650    accuracy        0.256716       2.298495    5.321454                 0.256716                2.298495           5.321454            1       True          5\n",
      "26           CatBoost_r9_BAG_L1       0.937252   0.934547    accuracy        0.046038       0.096480    7.389218                 0.046038                0.096480           7.389218            1       True         18\n",
      "27        ExtraTreesGini_BAG_L1       0.936485   0.934603    accuracy        0.391612       2.639747    3.187688                 0.391612                2.639747           3.187688            1       True          8\n",
      "28        ExtraTreesEntr_BAG_L1       0.936101   0.934651    accuracy        0.480106       2.812332    2.889204                 0.480106                2.812332           2.889204            1       True          9\n",
      "29        KNeighborsDist_BAG_L1       0.849431   0.848512    accuracy        0.036509       0.155805    0.131306                 0.036509                0.155805           0.131306            1       True          2\n",
      "30        KNeighborsUnif_BAG_L1       0.846169   0.846553    accuracy        0.037520       0.148892    0.123903                 0.037520                0.148892           0.123903            1       True          1\n",
      "31          LightGBM_r96_BAG_L1       0.818281   0.818288    accuracy        0.034781       0.051423    0.514720                 0.034781                0.051423           0.514720            1       True         19\n",
      "32         LightGBM_r131_BAG_L1       0.818281   0.818288    accuracy        0.050289       0.060072    1.114541                 0.050289                0.060072           1.114541            1       True         16\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t914s\t = DyStack   runtime |\t2686s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 2686s\n",
      "AutoGluon will save models to \"/Users/haukesteffen/dev/TabularShenanigans/S4E11/AutogluonModels/ag-20241204_124838\"\n",
      "Train Data Rows:    140700\n",
      "Train Data Columns: 23\n",
      "Label Column:       Depression\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5017.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 102.93 MB (2.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 10 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\t\t('int', [])    :  1 | ['id']\n",
      "\t\t('object', []) : 12 | ['Name', 'Gender', 'City', 'Working Professional or Student', 'Profession', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  8 | ['Name', 'City', 'Profession', 'Sleep Duration', 'Dietary Habits', ...]\n",
      "\t\t('float', [])     : 10 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\t\t('int', [])       :  1 | ['id']\n",
      "\t\t('int', ['bool']) :  4 | ['Gender', 'Working Professional or Student', 'suicidal_thoughts', 'family_history']\n",
      "\t0.6s = Fit runtime\n",
      "\t23 features in original data used to generate 23 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 13.56 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2685.37s of the 2685.36s of remaining time.\n",
      "\t0.849\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2685.04s of the 2685.03s of remaining time.\n",
      "\t0.8509\t = Validation score   (accuracy)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2684.72s of the 2684.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.70%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t4.46s\t = Training   runtime\n",
      "\t1.78s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2678.08s of the 2678.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.01%)\n",
      "\t0.9392\t = Validation score   (accuracy)\n",
      "\t3.57s\t = Training   runtime\n",
      "\t1.06s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 2672.54s of the 2672.53s of remaining time.\n",
      "\t0.9363\t = Validation score   (accuracy)\n",
      "\t5.99s\t = Training   runtime\n",
      "\t3.01s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 2663.30s of the 2663.29s of remaining time.\n",
      "\t0.9363\t = Validation score   (accuracy)\n",
      "\t6.47s\t = Training   runtime\n",
      "\t3.02s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2653.57s of the 2653.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.26%)\n",
      "\t0.94\t = Validation score   (accuracy)\n",
      "\t274.06s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 2377.91s of the 2377.90s of remaining time.\n",
      "\t0.935\t = Validation score   (accuracy)\n",
      "\t3.39s\t = Training   runtime\n",
      "\t3.29s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 2370.92s of the 2370.91s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 290 due to low memory. Expected memory usage reduced from 15.51% -> 15.0% of available memory...\n",
      "\t0.9353\t = Validation score   (accuracy)\n",
      "\t3.32s\t = Training   runtime\n",
      "\t3.3s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 2363.97s of the 2363.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.49%)\n",
      "\t0.9393\t = Validation score   (accuracy)\n",
      "\t76.72s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 2284.98s of the 2284.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.10%)\n",
      "\t0.9393\t = Validation score   (accuracy)\n",
      "\t98.38s\t = Training   runtime\n",
      "\t0.82s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2184.40s of the 2184.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.77%)\n",
      "\t0.9387\t = Validation score   (accuracy)\n",
      "\t56.49s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2126.01s of the 2126.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.14%)\n",
      "\t0.9391\t = Validation score   (accuracy)\n",
      "\t5.53s\t = Training   runtime\n",
      "\t2.35s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 2118.05s of the 2118.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.09%)\n",
      "\t0.9399\t = Validation score   (accuracy)\n",
      "\t74.81s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 2041.71s of the 2041.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.80%)\n",
      "\t0.9395\t = Validation score   (accuracy)\n",
      "\t206.6s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1833.32s of the 1833.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.93%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1830.34s of the 1830.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.43%)\n",
      "\t0.9399\t = Validation score   (accuracy)\n",
      "\t235.57s\t = Training   runtime\n",
      "\t1.64s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1592.67s of the 1592.66s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.73%)\n",
      "\t0.9396\t = Validation score   (accuracy)\n",
      "\t382.14s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1208.47s of the 1208.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.84%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 1205.57s of the 1205.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.90%)\n",
      "\t0.9389\t = Validation score   (accuracy)\n",
      "\t101.32s\t = Training   runtime\n",
      "\t0.65s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 1102.43s of the 1102.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.30%)\n",
      "\t0.8183\t = Validation score   (accuracy)\n",
      "\t13.78s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 1086.64s of the 1086.63s of remaining time.\n",
      "\t0.9357\t = Validation score   (accuracy)\n",
      "\t7.57s\t = Training   runtime\n",
      "\t2.98s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 1075.88s of the 1075.87s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.93%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t102.72s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 971.38s of the 971.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.61%)\n",
      "\t0.9381\t = Validation score   (accuracy)\n",
      "\t36.4s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 932.83s of the 932.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.50%)\n",
      "\t0.938\t = Validation score   (accuracy)\n",
      "\t197.85s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 733.05s of the 733.04s of remaining time.\n",
      "\t0.9353\t = Validation score   (accuracy)\n",
      "\t18.53s\t = Training   runtime\n",
      "\t2.83s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 711.49s of the 711.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.35%)\n",
      "\t0.9399\t = Validation score   (accuracy)\n",
      "\t5.96s\t = Training   runtime\n",
      "\t2.63s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 702.77s of the 702.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.85%)\n",
      "\t0.94\t = Validation score   (accuracy)\n",
      "\t186.58s\t = Training   runtime\n",
      "\t1.96s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 514.42s of the 514.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.50%)\n",
      "\t0.9398\t = Validation score   (accuracy)\n",
      "\t177.19s\t = Training   runtime\n",
      "\t1.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 334.85s of the 334.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.58%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t93.19s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 239.74s of the 239.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.86%)\n",
      "\t0.9397\t = Validation score   (accuracy)\n",
      "\t5.69s\t = Training   runtime\n",
      "\t2.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 231.61s of the 231.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.51%)\n",
      "\t0.9385\t = Validation score   (accuracy)\n",
      "\t72.68s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 157.37s of the 157.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.56%)\n",
      "\t0.9394\t = Validation score   (accuracy)\n",
      "\t96.54s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 58.70s of the 58.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.39%)\n",
      "\t0.9372\t = Validation score   (accuracy)\n",
      "\t47.45s\t = Training   runtime\n",
      "\t2.71s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 9.34s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_r30_BAG_L1': 0.28, 'NeuralNetFastAI_r145_BAG_L1': 0.2, 'NeuralNetFastAI_r102_BAG_L1': 0.12, 'NeuralNetFastAI_BAG_L1': 0.08, 'LightGBM_r188_BAG_L1': 0.08, 'LightGBMXT_BAG_L1': 0.04, 'CatBoost_BAG_L1': 0.04, 'CatBoost_r177_BAG_L1': 0.04, 'NeuralNetTorch_r79_BAG_L1': 0.04, 'NeuralNetTorch_r22_BAG_L1': 0.04, 'XGBoost_r89_BAG_L1': 0.04}\n",
      "\t0.9407\t = Validation score   (accuracy)\n",
      "\t4.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2681.4s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1590.3 rows/s (17588 batch size)\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric accuracy | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.9407\n",
      "\tBest Threshold: 0.500\t| val: 0.9407\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/haukesteffen/dev/TabularShenanigans/S4E11/AutogluonModels/ag-20241204_124838\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor(label='Depression').fit(\"train_prep.csv\", presets='best_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: test_prep.csv | Columns = 23 / 23 | Rows = 93800 -> 93800\n"
     ]
    }
   ],
   "source": [
    "predictions = predictor.predict(\"test_prep.csv\")\n",
    "dftest['Depression'] = predictions.to_list()\n",
    "dftest['Depression'].to_csv('submission_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLJar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear algorithm was disabled.\n",
      "AutoML directory: AutoML_5\n",
      "The task is binary_classification with evaluation metric accuracy\n",
      "AutoML will use algorithms: ['Decision Tree', 'Random Forest', 'Extra Trees', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network', 'Nearest Neighbors']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'mix_encoding', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree accuracy 0.909694 trained in 4.17 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "Validation strategy: 10-fold CV Shuffle,Stratify\n",
      "* Step simple_algorithms will try to check up to 3 models\n",
      "1_DecisionTree accuracy 0.910391 trained in 24.05 seconds\n",
      "2_DecisionTree accuracy 0.919744 trained in 24.31 seconds\n",
      "3_DecisionTree accuracy 0.919744 trained in 24.36 seconds\n",
      "* Step default_algorithms will try to check up to 6 models\n",
      "4_Default_LightGBM accuracy 0.939796 trained in 56.56 seconds\n",
      "5_Default_Xgboost accuracy 0.939114 trained in 44.01 seconds\n",
      "6_Default_CatBoost accuracy 0.939446 trained in 62.39 seconds\n",
      "7_Default_NeuralNetwork accuracy 0.937522 trained in 121.08 seconds\n",
      "8_Default_RandomForest accuracy 0.925373 trained in 77.79 seconds\n",
      "9_Default_ExtraTrees accuracy 0.920986 trained in 76.55 seconds\n",
      "* Step not_so_random will try to check up to 54 models\n",
      "19_LightGBM accuracy 0.939796 trained in 54.26 seconds\n",
      "10_Xgboost accuracy 0.939095 trained in 44.57 seconds\n",
      "28_CatBoost accuracy 0.939427 trained in 82.79 seconds\n",
      "37_RandomForest accuracy 0.928993 trained in 95.02 seconds\n",
      "46_ExtraTrees accuracy 0.918038 trained in 75.86 seconds\n",
      "55_NeuralNetwork accuracy 0.936953 trained in 131.89 seconds\n",
      "20_LightGBM accuracy 0.939256 trained in 40.77 seconds\n",
      "11_Xgboost accuracy 0.939275 trained in 43.59 seconds\n",
      "29_CatBoost accuracy 0.939967 trained in 79.83 seconds\n",
      "38_RandomForest accuracy 0.932869 trained in 105.22 seconds\n",
      "47_ExtraTrees accuracy 0.932319 trained in 91.14 seconds\n",
      "56_NeuralNetwork accuracy 0.93684 trained in 126.5 seconds\n",
      "21_LightGBM accuracy 0.939417 trained in 61.9 seconds\n",
      "12_Xgboost accuracy 0.939967 trained in 48.46 seconds\n",
      "30_CatBoost accuracy 0.939919 trained in 74.14 seconds\n",
      "39_RandomForest accuracy 0.917214 trained in 79.58 seconds\n",
      "48_ExtraTrees accuracy 0.912381 trained in 90.41 seconds\n",
      "57_NeuralNetwork accuracy 0.936356 trained in 229.74 seconds\n",
      "22_LightGBM accuracy 0.93936 trained in 56.7 seconds\n",
      "13_Xgboost accuracy 0.939758 trained in 41.17 seconds\n",
      "31_CatBoost accuracy 0.940687 trained in 69.7 seconds\n",
      "40_RandomForest accuracy 0.93032 trained in 97.26 seconds\n",
      "49_ExtraTrees accuracy 0.929685 trained in 88.56 seconds\n",
      "58_NeuralNetwork accuracy 0.937465 trained in 127.49 seconds\n",
      "23_LightGBM accuracy 0.939275 trained in 42.45 seconds\n",
      "14_Xgboost accuracy 0.940327 trained in 51.96 seconds\n",
      "32_CatBoost accuracy 0.939863 trained in 66.43 seconds\n",
      "41_RandomForest accuracy 0.917375 trained in 89.02 seconds\n",
      "50_ExtraTrees accuracy 0.918067 trained in 69.82 seconds\n",
      "59_NeuralNetwork accuracy 0.937247 trained in 120.92 seconds\n",
      "24_LightGBM accuracy 0.939484 trained in 64.92 seconds\n",
      "15_Xgboost accuracy 0.939976 trained in 50.57 seconds\n",
      "33_CatBoost accuracy 0.939578 trained in 65.66 seconds\n",
      "42_RandomForest accuracy 0.930727 trained in 96.51 seconds\n",
      "51_ExtraTrees accuracy 0.932395 trained in 107.23 seconds\n",
      "60_NeuralNetwork accuracy 0.9372 trained in 119.96 seconds\n",
      "25_LightGBM accuracy 0.939484 trained in 53.36 seconds\n",
      "16_Xgboost accuracy 0.939777 trained in 48.29 seconds\n",
      "34_CatBoost accuracy 0.939256 trained in 117.05 seconds\n",
      "43_RandomForest accuracy 0.92779 trained in 120.79 seconds\n",
      "52_ExtraTrees accuracy 0.928842 trained in 99.65 seconds\n",
      "61_NeuralNetwork accuracy 0.937418 trained in 107.95 seconds\n",
      "26_LightGBM accuracy 0.939275 trained in 43.67 seconds\n",
      "17_Xgboost accuracy 0.939578 trained in 43.84 seconds\n",
      "35_CatBoost accuracy 0.94009 trained in 77.33 seconds\n",
      "44_RandomForest accuracy 0.929059 trained in 102.89 seconds\n",
      "53_ExtraTrees accuracy 0.91746 trained in 102.33 seconds\n",
      "62_NeuralNetwork accuracy 0.936915 trained in 137.69 seconds\n",
      "27_LightGBM accuracy 0.939275 trained in 58.67 seconds\n",
      "18_Xgboost accuracy 0.939531 trained in 54.03 seconds\n",
      "36_CatBoost accuracy 0.939294 trained in 115.52 seconds\n",
      "45_RandomForest accuracy 0.930263 trained in 131.48 seconds\n",
      "54_ExtraTrees accuracy 0.927117 trained in 87.83 seconds\n",
      "63_NeuralNetwork accuracy 0.936328 trained in 107.27 seconds\n",
      "* Step mix_encoding will try to check up to 1 model\n",
      "14_Xgboost_categorical_mix accuracy 0.940166 trained in 51.09 seconds\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 10\n",
      "Add Golden Feature: Age_diff_Academic Pressure\n",
      "Add Golden Feature: Age_ratio_Academic Pressure\n",
      "Add Golden Feature: Academic Pressure_ratio_Age\n",
      "Add Golden Feature: Age_diff_Financial Stress\n",
      "Add Golden Feature: Job Satisfaction_sum_Age\n",
      "Add Golden Feature: Age_diff_Work Pressure\n",
      "Add Golden Feature: Age_diff_Work/Study Hours\n",
      "Add Golden Feature: Study Satisfaction_sum_Age\n",
      "Add Golden Feature: CGPA_sum_Age\n",
      "Add Golden Feature: Age_diff_CGPA\n",
      "Created 10 Golden Features in 5.13 seconds.\n",
      "31_CatBoost_GoldenFeatures accuracy 0.940535 trained in 71.91 seconds\n",
      "14_Xgboost_GoldenFeatures accuracy 0.940118 trained in 53.35 seconds\n",
      "14_Xgboost_categorical_mix_GoldenFeatures accuracy 0.939863 trained in 52.52 seconds\n",
      "* Step kmeans_features will try to check up to 3 models\n",
      "31_CatBoost_KMeansFeatures accuracy 0.939654 trained in 66.93 seconds\n",
      "14_Xgboost_KMeansFeatures accuracy 0.939322 trained in 53.71 seconds\n",
      "14_Xgboost_categorical_mix_KMeansFeatures accuracy 0.939683 trained in 56.01 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "31_CatBoost_RandomFeature accuracy 0.940289 trained in 75.9 seconds\n",
      "Drop features ['Gender', 'random_feature']\n",
      "* Step features_selection will try to check up to 6 models\n",
      "31_CatBoost_SelectedFeatures accuracy 0.940251 trained in 66.21 seconds\n",
      "14_Xgboost_SelectedFeatures accuracy 0.940194 trained in 51.2 seconds\n",
      "4_Default_LightGBM_SelectedFeatures accuracy 0.939161 trained in 52.37 seconds\n",
      "7_Default_NeuralNetwork_SelectedFeatures accuracy 0.937503 trained in 130.11 seconds\n",
      "38_RandomForest_SelectedFeatures accuracy 0.932784 trained in 102.24 seconds\n",
      "51_ExtraTrees_SelectedFeatures accuracy 0.93231 trained in 101.52 seconds\n",
      "* Step hill_climbing_1 will try to check up to 28 models\n",
      "64_CatBoost accuracy 0.940355 trained in 71.43 seconds\n",
      "65_CatBoost accuracy 0.940412 trained in 71.08 seconds\n",
      "66_CatBoost_GoldenFeatures accuracy 0.940213 trained in 64.5 seconds\n",
      "67_CatBoost_GoldenFeatures accuracy 0.94009 trained in 67.66 seconds\n",
      "68_Xgboost accuracy 0.94009 trained in 50.62 seconds\n",
      "69_Xgboost accuracy 0.940659 trained in 56.24 seconds\n",
      "70_CatBoost_SelectedFeatures accuracy 0.940317 trained in 71.62 seconds\n",
      "71_CatBoost_SelectedFeatures accuracy 0.939948 trained in 70.71 seconds\n",
      "72_Xgboost_SelectedFeatures accuracy 0.940062 trained in 49.83 seconds\n",
      "73_Xgboost_SelectedFeatures accuracy 0.940147 trained in 50.15 seconds\n",
      "74_Xgboost accuracy 0.939882 trained in 51.91 seconds\n",
      "75_Xgboost accuracy 0.9401 trained in 50.21 seconds\n",
      "76_LightGBM accuracy 0.939891 trained in 56.68 seconds\n",
      "77_LightGBM accuracy 0.939123 trained in 51.97 seconds\n",
      "78_LightGBM accuracy 0.93936 trained in 66.43 seconds\n",
      "79_NeuralNetwork accuracy 0.937569 trained in 116.69 seconds\n",
      "80_NeuralNetwork_SelectedFeatures accuracy 0.937276 trained in 121.04 seconds\n",
      "81_NeuralNetwork_SelectedFeatures accuracy 0.936887 trained in 131.74 seconds\n",
      "82_RandomForest accuracy 0.933239 trained in 117.27 seconds\n",
      "83_RandomForest accuracy 0.932149 trained in 90.72 seconds\n",
      "84_RandomForest_SelectedFeatures accuracy 0.933115 trained in 108.05 seconds\n",
      "85_RandomForest_SelectedFeatures accuracy 0.931997 trained in 107.67 seconds\n",
      "86_ExtraTrees accuracy 0.932035 trained in 112.2 seconds\n",
      "87_ExtraTrees accuracy 0.931931 trained in 122.78 seconds\n",
      "88_ExtraTrees accuracy 0.932443 trained in 89.93 seconds\n",
      "89_ExtraTrees_SelectedFeatures accuracy 0.932367 trained in 112.93 seconds\n",
      "90_RandomForest accuracy 0.931883 trained in 97.65 seconds\n",
      "91_DecisionTree accuracy 0.908733 trained in 30.89 seconds\n",
      "* Step hill_climbing_2 will try to check up to 14 models\n",
      "92_Xgboost accuracy 0.940024 trained in 48.97 seconds\n",
      "93_Xgboost accuracy 0.939796 trained in 49.74 seconds\n",
      "94_Xgboost accuracy 0.94045 trained in 54.61 seconds\n",
      "95_Xgboost accuracy 0.939967 trained in 51.29 seconds\n",
      "96_Xgboost_SelectedFeatures accuracy 0.939957 trained in 49.27 seconds\n",
      "97_Xgboost_SelectedFeatures accuracy 0.939758 trained in 48.16 seconds\n",
      "98_LightGBM accuracy 0.939891 trained in 55.18 seconds\n",
      "99_LightGBM accuracy 0.939796 trained in 59.89 seconds\n",
      "100_LightGBM accuracy 0.939796 trained in 60.28 seconds\n",
      "101_LightGBM accuracy 0.939796 trained in 62.78 seconds\n",
      "102_RandomForest accuracy 0.933523 trained in 94.59 seconds\n",
      "103_RandomForest_SelectedFeatures accuracy 0.933457 trained in 120.05 seconds\n",
      "104_ExtraTrees accuracy 0.932225 trained in 92.98 seconds\n",
      "105_ExtraTrees_SelectedFeatures accuracy 0.932689 trained in 100.61 seconds\n",
      "* Step boost_on_errors will try to check up to 1 model\n",
      "31_CatBoost_BoostOnErrors accuracy 0.940299 trained in 82.64 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble accuracy 0.940744 trained in 66.11 seconds\n",
      "* Step stack will try to check up to 59 models\n",
      "31_CatBoost_Stacked accuracy 0.941218 trained in 71.6 seconds\n",
      "69_Xgboost_Stacked accuracy 0.941237 trained in 56.81 seconds\n",
      "98_LightGBM_Stacked accuracy 0.941284 trained in 60.54 seconds\n",
      "79_NeuralNetwork_Stacked accuracy 0.939645 trained in 177.9 seconds\n",
      "102_RandomForest_Stacked accuracy 0.941805 trained in 680.1 seconds\n",
      "105_ExtraTrees_SelectedFeatures_Stacked accuracy 0.941322 trained in 305.89 seconds\n",
      "31_CatBoost_GoldenFeatures_Stacked accuracy 0.941066 trained in 80.08 seconds\n",
      "94_Xgboost_Stacked accuracy 0.94135 trained in 56.57 seconds\n",
      "76_LightGBM_Stacked accuracy 0.941284 trained in 56.35 seconds\n",
      "7_Default_NeuralNetwork_Stacked accuracy 0.939863 trained in 158.92 seconds\n",
      "103_RandomForest_SelectedFeatures_Stacked accuracy 0.941767 trained in 618.92 seconds\n",
      "88_ExtraTrees_Stacked accuracy 0.941142 trained in 300.31 seconds\n",
      "65_CatBoost_Stacked accuracy 0.941237 trained in 82.59 seconds\n",
      "14_Xgboost_Stacked accuracy 0.941161 trained in 55.35 seconds\n",
      "100_LightGBM_Stacked accuracy 0.941104 trained in 67.15 seconds\n",
      "7_Default_NeuralNetwork_SelectedFeatures_Stacked accuracy 0.939559 trained in 184.5 seconds\n",
      "82_RandomForest_Stacked accuracy 0.941729 trained in 688.77 seconds\n",
      "51_ExtraTrees_Stacked accuracy 0.941161 trained in 324.23 seconds\n",
      "64_CatBoost_Stacked accuracy 0.941237 trained in 65.8 seconds\n",
      "14_Xgboost_SelectedFeatures_Stacked accuracy 0.941483 trained in 59.45 seconds\n",
      "4_Default_LightGBM_Stacked accuracy 0.941104 trained in 69.4 seconds\n",
      "58_NeuralNetwork_Stacked accuracy 0.939711 trained in 212.82 seconds\n",
      "84_RandomForest_SelectedFeatures_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 44.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "89_ExtraTrees_SelectedFeatures_Stacked accuracy 0.941227 trained in 363.53 seconds\n",
      "* Step ensemble_stacked will try to check up to 1 model\n",
      "Ensemble_Stacked accuracy 0.941815 trained in 93.93 seconds\n",
      "AutoML fit time: 14692.06 seconds\n",
      "AutoML best model: Ensemble_Stacked\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'train.csv',\n",
    "    index_col=0\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['Depression']),\n",
    "    df['Depression'],\n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "automl = AutoML(\n",
    "    eval_metric='accuracy',\n",
    "    mode='Compete',\n",
    "    total_time_limit=60*60*4\n",
    ")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
