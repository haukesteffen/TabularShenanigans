{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import optuna\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', index_col=0)\n",
    "df.rename(columns={\n",
    "    'Have you ever had suicidal thoughts ?':'suicidal_thoughts',\n",
    "    'Family History of Mental Illness':'family_history'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['job'] = df.apply(lambda row: 'Student' if row['Working Professional or Student'] == 'Student' else row['Profession'], axis=1)\n",
    "df = df.fillna({'job':'Unemployed'})\n",
    "df['pressure'] = df.apply(lambda row: row['Work Pressure'] if row['Working Professional or Student'] == 'Working Professional' else row['Academic Pressure'], axis=1)\n",
    "df['satisfaction'] = df.apply(lambda row: row['Job Satisfaction'] if row['Working Professional or Student'] == 'Working Professional' else row['Study Satisfaction'], axis=1)\n",
    "degree_mapping = {\n",
    "    'Class 12': 'High School',\n",
    "    'B.Ed': 'Undergraduate',\n",
    "    'B.Arch': 'Undergraduate',\n",
    "    'B.Com': 'Undergraduate',\n",
    "    'B.Pharm': 'Undergraduate',\n",
    "    'BCA': 'Undergraduate',\n",
    "    'BBA': 'Undergraduate',\n",
    "    'BSc': 'Undergraduate',\n",
    "    'B.Tech': 'Undergraduate',\n",
    "    'LLB': 'Undergraduate',\n",
    "    'BHM': 'Undergraduate',\n",
    "    'BA': 'Undergraduate',\n",
    "    'BE': 'Undergraduate',\n",
    "    'MBBS': 'Undergraduate',\n",
    "    'M.Ed': 'Postgraduate',\n",
    "    'MCA': 'Postgraduate',\n",
    "    'MSc': 'Postgraduate',\n",
    "    'LLM': 'Postgraduate',\n",
    "    'M.Pharm': 'Postgraduate',\n",
    "    'M.Tech': 'Postgraduate',\n",
    "    'MBA': 'Postgraduate',\n",
    "    'ME': 'Postgraduate',\n",
    "    'MHM': 'Postgraduate',\n",
    "    'M.Com': 'Postgraduate',\n",
    "    'MA': 'Postgraduate',\n",
    "    'PhD': 'Doctorate',\n",
    "    'MD': 'Doctorate'\n",
    "}\n",
    "df['deg'] = df['Degree'].map(degree_mapping).fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape (112560, 12)\n",
      "Xte shape (28140, 12)\n",
      "ytr shape (112560,)\n",
      "yte shape (28140,)\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    'Gender',\n",
    "    'job',\n",
    "    'Sleep Duration',\n",
    "    'Dietary Habits',\n",
    "    'deg',\n",
    "    'suicidal_thoughts',\n",
    "    'family_history'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'Age',\n",
    "    'pressure',\n",
    "    'satisfaction',\n",
    "    'Work/Study Hours',\n",
    "    'Financial Stress'\n",
    "]\n",
    "\n",
    "target = 'Depression'\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    df[numerical_features + categorical_features],\n",
    "    df[target],\n",
    "    stratify=df['Depression'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'Xtr shape {Xtr.shape}')\n",
    "print(f'Xte shape {Xte.shape}')\n",
    "print(f'ytr shape {ytr.shape}')\n",
    "print(f'yte shape {yte.shape}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM Benchmark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(min_frequency=0.1, handle_unknown='infrequent_if_exist'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear'])\n",
    "    if kernel == 'rbf' or kernel == 'poly':\n",
    "        gamma = trial.suggest_float('gamma', 1e-4, 1e0, log=True)\n",
    "    else:\n",
    "        gamma = 'scale'  # Not used for linear kernel\n",
    "    \n",
    "    pipeline.set_params(\n",
    "        classifier__C=C,\n",
    "        classifier__kernel=kernel,\n",
    "        classifier__gamma=gamma\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(pipeline, Xtr, ytr, cv=3, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(study.best_params)\n",
    "\n",
    "best_params = study.best_params\n",
    "pipeline.set_params(\n",
    "    classifier__C=best_params['C'],\n",
    "    classifier__kernel=best_params['kernel'],\n",
    "    classifier__gamma=best_params['gamma']\n",
    ")\n",
    "\n",
    "pipeline.fit(Xtr, ytr)\n",
    "\n",
    "test_accuracy = pipeline.score(Xte, yte)\n",
    "print(f\"Test accuracy: {test_accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    df[numerical_features + categorical_features],\n",
    "    df[target],\n",
    "    stratify=df['Depression'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'Xtr shape {Xtr.shape}')\n",
    "print(f'Xte shape {Xte.shape}')\n",
    "print(f'ytr shape {ytr.shape}')\n",
    "print(f'yte shape {yte.shape}') \n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(min_frequency=0.1, handle_unknown='infrequent_if_exist'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 1500)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 25)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "\n",
    "    pipeline.set_params(\n",
    "        classifier__n_estimators=n_estimators,\n",
    "        classifier__max_depth=max_depth,\n",
    "        classifier__learning_rate=learning_rate,\n",
    "        classifier__subsample=subsample,\n",
    "        classifier__colsample_bytree=colsample_bytree,\n",
    "        classifier__min_child_weight=min_child_weight\n",
    "    )\n",
    "\n",
    "    scores = cross_val_score(pipeline, Xtr, ytr, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "\n",
    "best_params = study.best_params\n",
    "pipeline.set_params(\n",
    "    classifier__n_estimators=best_params['n_estimators'],\n",
    "    classifier__max_depth=best_params['max_depth'],\n",
    "    classifier__learning_rate=best_params['learning_rate'],\n",
    "    classifier__subsample=best_params['subsample'],\n",
    "    classifier__colsample_bytree=best_params['colsample_bytree'],\n",
    "    classifier__min_child_weight=best_params['min_child_weight']\n",
    ")\n",
    "\n",
    "pipeline.fit(Xtr, ytr)\n",
    "\n",
    "test_accuracy = pipeline.score(Xte, yte)\n",
    "print(f\"Test accuracy: {test_accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TabPFN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9317073170731708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "#X, y = load_breast_cancer(return_X_y=True)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "testdf = df.sample(1024)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    testdf[numerical_features + categorical_features],\n",
    "    testdf[target],\n",
    "    stratify=testdf[target],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# N_ensemble_configurations controls the number of model predictions that are ensembled with feature and class rotations (See our work for details).\n",
    "# When N_ensemble_configurations > #features * #classes, no further averaging is applied.\n",
    "\n",
    "\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(min_frequency=0.1, handle_unknown='infrequent_if_exist'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n",
    "labenc = LabelEncoder()\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "y_train = labenc.fit_transform(y_train)\n",
    "\n",
    "classifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=32)\n",
    "classifier.fit(X_train, y_train, overwrite_warning=True)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "y_eval, p_eval = classifier.predict(X_test, return_winning_probability=True)\n",
    "\n",
    "print('Accuracy', accuracy_score(y_test, y_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AutoGluon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241203_124844\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.2\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.0.0: Mon Aug 12 20:51:54 PDT 2024; root:xnu-11215.1.10~2/RELEASE_ARM64_T6000\n",
      "CPU Count:          8\n",
      "Memory Avail:       4.70 GB / 16.00 GB (29.3%)\n",
      "Disk Space Avail:   203.87 GB / 460.43 GB (44.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Loaded data from: train.csv | Columns = 20 / 20 | Rows = 140700 -> 140700\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (140700 samples, 89.89 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/Users/haukesteffen/dev/TabularShenanigans/S4E11/AutogluonModels/ag-20241203_124844\"\n",
      "Train Data Rows:    140700\n",
      "Train Data Columns: 19\n",
      "Label Column:       Depression\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4767.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 84.66 MB (1.8% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\t\t('int', [])    :  1 | ['id']\n",
      "\t\t('object', []) : 10 | ['Name', 'Gender', 'City', 'Working Professional or Student', 'Profession', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 6 | ['Name', 'City', 'Profession', 'Sleep Duration', 'Dietary Habits', ...]\n",
      "\t\t('float', [])     : 8 | ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', ...]\n",
      "\t\t('int', [])       : 1 | ['id']\n",
      "\t\t('int', ['bool']) : 4 | ['Gender', 'Working Professional or Student', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n",
      "\t0.4s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 11.14 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.017768301350390904, Train Rows: 138200, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.8412\t = Validation score   (accuracy)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.8468\t = Validation score   (accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9428\t = Validation score   (accuracy)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9432\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9408\t = Validation score   (accuracy)\n",
      "\t6.79s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9408\t = Validation score   (accuracy)\n",
      "\t6.23s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9412\t = Validation score   (accuracy)\n",
      "\t5.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 285 due to low memory. Expected memory usage reduced from 15.76% -> 15.0% of available memory...\n",
      "\t0.9416\t = Validation score   (accuracy)\n",
      "\t3.0s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 274 due to low memory. Expected memory usage reduced from 16.37% -> 15.0% of available memory...\n",
      "\t0.9392\t = Validation score   (accuracy)\n",
      "\t2.94s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9472\t = Validation score   (accuracy)\n",
      "\t81.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.942\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\tColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/utils/data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9408\t = Validation score   (accuracy)\n",
      "\t1.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.6, 'RandomForestEntr': 0.2, 'LightGBM': 0.1, 'ExtraTreesGini': 0.1}\n",
      "\t0.9496\t = Validation score   (accuracy)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 114.44s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 24739.1 rows/s (2500 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/haukesteffen/dev/TabularShenanigans/S4E11/AutogluonModels/ag-20241203_124844\")\n",
      "Loaded data from: test.csv | Columns = 19 / 19 | Rows = 93800 -> 93800\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor(label='Depression').fit(\"train.csv\")\n",
    "predictions = predictor.predict(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLJar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear algorithm was disabled.\n",
      "AutoML directory: AutoML_5\n",
      "The task is binary_classification with evaluation metric accuracy\n",
      "AutoML will use algorithms: ['Decision Tree', 'Random Forest', 'Extra Trees', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network', 'Nearest Neighbors']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'mix_encoding', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree accuracy 0.909694 trained in 4.17 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "Validation strategy: 10-fold CV Shuffle,Stratify\n",
      "* Step simple_algorithms will try to check up to 3 models\n",
      "1_DecisionTree accuracy 0.910391 trained in 24.05 seconds\n",
      "2_DecisionTree accuracy 0.919744 trained in 24.31 seconds\n",
      "3_DecisionTree accuracy 0.919744 trained in 24.36 seconds\n",
      "* Step default_algorithms will try to check up to 6 models\n",
      "4_Default_LightGBM accuracy 0.939796 trained in 56.56 seconds\n",
      "5_Default_Xgboost accuracy 0.939114 trained in 44.01 seconds\n",
      "6_Default_CatBoost accuracy 0.939446 trained in 62.39 seconds\n",
      "7_Default_NeuralNetwork accuracy 0.937522 trained in 121.08 seconds\n",
      "8_Default_RandomForest accuracy 0.925373 trained in 77.79 seconds\n",
      "9_Default_ExtraTrees accuracy 0.920986 trained in 76.55 seconds\n",
      "* Step not_so_random will try to check up to 54 models\n",
      "19_LightGBM accuracy 0.939796 trained in 54.26 seconds\n",
      "10_Xgboost accuracy 0.939095 trained in 44.57 seconds\n",
      "28_CatBoost accuracy 0.939427 trained in 82.79 seconds\n",
      "37_RandomForest accuracy 0.928993 trained in 95.02 seconds\n",
      "46_ExtraTrees accuracy 0.918038 trained in 75.86 seconds\n",
      "55_NeuralNetwork accuracy 0.936953 trained in 131.89 seconds\n",
      "20_LightGBM accuracy 0.939256 trained in 40.77 seconds\n",
      "11_Xgboost accuracy 0.939275 trained in 43.59 seconds\n",
      "29_CatBoost accuracy 0.939967 trained in 79.83 seconds\n",
      "38_RandomForest accuracy 0.932869 trained in 105.22 seconds\n",
      "47_ExtraTrees accuracy 0.932319 trained in 91.14 seconds\n",
      "56_NeuralNetwork accuracy 0.93684 trained in 126.5 seconds\n",
      "21_LightGBM accuracy 0.939417 trained in 61.9 seconds\n",
      "12_Xgboost accuracy 0.939967 trained in 48.46 seconds\n",
      "30_CatBoost accuracy 0.939919 trained in 74.14 seconds\n",
      "39_RandomForest accuracy 0.917214 trained in 79.58 seconds\n",
      "48_ExtraTrees accuracy 0.912381 trained in 90.41 seconds\n",
      "57_NeuralNetwork accuracy 0.936356 trained in 229.74 seconds\n",
      "22_LightGBM accuracy 0.93936 trained in 56.7 seconds\n",
      "13_Xgboost accuracy 0.939758 trained in 41.17 seconds\n",
      "31_CatBoost accuracy 0.940687 trained in 69.7 seconds\n",
      "40_RandomForest accuracy 0.93032 trained in 97.26 seconds\n",
      "49_ExtraTrees accuracy 0.929685 trained in 88.56 seconds\n",
      "58_NeuralNetwork accuracy 0.937465 trained in 127.49 seconds\n",
      "23_LightGBM accuracy 0.939275 trained in 42.45 seconds\n",
      "14_Xgboost accuracy 0.940327 trained in 51.96 seconds\n",
      "32_CatBoost accuracy 0.939863 trained in 66.43 seconds\n",
      "41_RandomForest accuracy 0.917375 trained in 89.02 seconds\n",
      "50_ExtraTrees accuracy 0.918067 trained in 69.82 seconds\n",
      "59_NeuralNetwork accuracy 0.937247 trained in 120.92 seconds\n",
      "24_LightGBM accuracy 0.939484 trained in 64.92 seconds\n",
      "15_Xgboost accuracy 0.939976 trained in 50.57 seconds\n",
      "33_CatBoost accuracy 0.939578 trained in 65.66 seconds\n",
      "42_RandomForest accuracy 0.930727 trained in 96.51 seconds\n",
      "51_ExtraTrees accuracy 0.932395 trained in 107.23 seconds\n",
      "60_NeuralNetwork accuracy 0.9372 trained in 119.96 seconds\n",
      "25_LightGBM accuracy 0.939484 trained in 53.36 seconds\n",
      "16_Xgboost accuracy 0.939777 trained in 48.29 seconds\n",
      "34_CatBoost accuracy 0.939256 trained in 117.05 seconds\n",
      "43_RandomForest accuracy 0.92779 trained in 120.79 seconds\n",
      "52_ExtraTrees accuracy 0.928842 trained in 99.65 seconds\n",
      "61_NeuralNetwork accuracy 0.937418 trained in 107.95 seconds\n",
      "26_LightGBM accuracy 0.939275 trained in 43.67 seconds\n",
      "17_Xgboost accuracy 0.939578 trained in 43.84 seconds\n",
      "35_CatBoost accuracy 0.94009 trained in 77.33 seconds\n",
      "44_RandomForest accuracy 0.929059 trained in 102.89 seconds\n",
      "53_ExtraTrees accuracy 0.91746 trained in 102.33 seconds\n",
      "62_NeuralNetwork accuracy 0.936915 trained in 137.69 seconds\n",
      "27_LightGBM accuracy 0.939275 trained in 58.67 seconds\n",
      "18_Xgboost accuracy 0.939531 trained in 54.03 seconds\n",
      "36_CatBoost accuracy 0.939294 trained in 115.52 seconds\n",
      "45_RandomForest accuracy 0.930263 trained in 131.48 seconds\n",
      "54_ExtraTrees accuracy 0.927117 trained in 87.83 seconds\n",
      "63_NeuralNetwork accuracy 0.936328 trained in 107.27 seconds\n",
      "* Step mix_encoding will try to check up to 1 model\n",
      "14_Xgboost_categorical_mix accuracy 0.940166 trained in 51.09 seconds\n",
      "* Step golden_features will try to check up to 3 models\n",
      "None 10\n",
      "Add Golden Feature: Age_diff_Academic Pressure\n",
      "Add Golden Feature: Age_ratio_Academic Pressure\n",
      "Add Golden Feature: Academic Pressure_ratio_Age\n",
      "Add Golden Feature: Age_diff_Financial Stress\n",
      "Add Golden Feature: Job Satisfaction_sum_Age\n",
      "Add Golden Feature: Age_diff_Work Pressure\n",
      "Add Golden Feature: Age_diff_Work/Study Hours\n",
      "Add Golden Feature: Study Satisfaction_sum_Age\n",
      "Add Golden Feature: CGPA_sum_Age\n",
      "Add Golden Feature: Age_diff_CGPA\n",
      "Created 10 Golden Features in 5.13 seconds.\n",
      "31_CatBoost_GoldenFeatures accuracy 0.940535 trained in 71.91 seconds\n",
      "14_Xgboost_GoldenFeatures accuracy 0.940118 trained in 53.35 seconds\n",
      "14_Xgboost_categorical_mix_GoldenFeatures accuracy 0.939863 trained in 52.52 seconds\n",
      "* Step kmeans_features will try to check up to 3 models\n",
      "31_CatBoost_KMeansFeatures accuracy 0.939654 trained in 66.93 seconds\n",
      "14_Xgboost_KMeansFeatures accuracy 0.939322 trained in 53.71 seconds\n",
      "14_Xgboost_categorical_mix_KMeansFeatures accuracy 0.939683 trained in 56.01 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "31_CatBoost_RandomFeature accuracy 0.940289 trained in 75.9 seconds\n",
      "Drop features ['Gender', 'random_feature']\n",
      "* Step features_selection will try to check up to 6 models\n",
      "31_CatBoost_SelectedFeatures accuracy 0.940251 trained in 66.21 seconds\n",
      "14_Xgboost_SelectedFeatures accuracy 0.940194 trained in 51.2 seconds\n",
      "4_Default_LightGBM_SelectedFeatures accuracy 0.939161 trained in 52.37 seconds\n",
      "7_Default_NeuralNetwork_SelectedFeatures accuracy 0.937503 trained in 130.11 seconds\n",
      "38_RandomForest_SelectedFeatures accuracy 0.932784 trained in 102.24 seconds\n",
      "51_ExtraTrees_SelectedFeatures accuracy 0.93231 trained in 101.52 seconds\n",
      "* Step hill_climbing_1 will try to check up to 28 models\n",
      "64_CatBoost accuracy 0.940355 trained in 71.43 seconds\n",
      "65_CatBoost accuracy 0.940412 trained in 71.08 seconds\n",
      "66_CatBoost_GoldenFeatures accuracy 0.940213 trained in 64.5 seconds\n",
      "67_CatBoost_GoldenFeatures accuracy 0.94009 trained in 67.66 seconds\n",
      "68_Xgboost accuracy 0.94009 trained in 50.62 seconds\n",
      "69_Xgboost accuracy 0.940659 trained in 56.24 seconds\n",
      "70_CatBoost_SelectedFeatures accuracy 0.940317 trained in 71.62 seconds\n",
      "71_CatBoost_SelectedFeatures accuracy 0.939948 trained in 70.71 seconds\n",
      "72_Xgboost_SelectedFeatures accuracy 0.940062 trained in 49.83 seconds\n",
      "73_Xgboost_SelectedFeatures accuracy 0.940147 trained in 50.15 seconds\n",
      "74_Xgboost accuracy 0.939882 trained in 51.91 seconds\n",
      "75_Xgboost accuracy 0.9401 trained in 50.21 seconds\n",
      "76_LightGBM accuracy 0.939891 trained in 56.68 seconds\n",
      "77_LightGBM accuracy 0.939123 trained in 51.97 seconds\n",
      "78_LightGBM accuracy 0.93936 trained in 66.43 seconds\n",
      "79_NeuralNetwork accuracy 0.937569 trained in 116.69 seconds\n",
      "80_NeuralNetwork_SelectedFeatures accuracy 0.937276 trained in 121.04 seconds\n",
      "81_NeuralNetwork_SelectedFeatures accuracy 0.936887 trained in 131.74 seconds\n",
      "82_RandomForest accuracy 0.933239 trained in 117.27 seconds\n",
      "83_RandomForest accuracy 0.932149 trained in 90.72 seconds\n",
      "84_RandomForest_SelectedFeatures accuracy 0.933115 trained in 108.05 seconds\n",
      "85_RandomForest_SelectedFeatures accuracy 0.931997 trained in 107.67 seconds\n",
      "86_ExtraTrees accuracy 0.932035 trained in 112.2 seconds\n",
      "87_ExtraTrees accuracy 0.931931 trained in 122.78 seconds\n",
      "88_ExtraTrees accuracy 0.932443 trained in 89.93 seconds\n",
      "89_ExtraTrees_SelectedFeatures accuracy 0.932367 trained in 112.93 seconds\n",
      "90_RandomForest accuracy 0.931883 trained in 97.65 seconds\n",
      "91_DecisionTree accuracy 0.908733 trained in 30.89 seconds\n",
      "* Step hill_climbing_2 will try to check up to 14 models\n",
      "92_Xgboost accuracy 0.940024 trained in 48.97 seconds\n",
      "93_Xgboost accuracy 0.939796 trained in 49.74 seconds\n",
      "94_Xgboost accuracy 0.94045 trained in 54.61 seconds\n",
      "95_Xgboost accuracy 0.939967 trained in 51.29 seconds\n",
      "96_Xgboost_SelectedFeatures accuracy 0.939957 trained in 49.27 seconds\n",
      "97_Xgboost_SelectedFeatures accuracy 0.939758 trained in 48.16 seconds\n",
      "98_LightGBM accuracy 0.939891 trained in 55.18 seconds\n",
      "99_LightGBM accuracy 0.939796 trained in 59.89 seconds\n",
      "100_LightGBM accuracy 0.939796 trained in 60.28 seconds\n",
      "101_LightGBM accuracy 0.939796 trained in 62.78 seconds\n",
      "102_RandomForest accuracy 0.933523 trained in 94.59 seconds\n",
      "103_RandomForest_SelectedFeatures accuracy 0.933457 trained in 120.05 seconds\n",
      "104_ExtraTrees accuracy 0.932225 trained in 92.98 seconds\n",
      "105_ExtraTrees_SelectedFeatures accuracy 0.932689 trained in 100.61 seconds\n",
      "* Step boost_on_errors will try to check up to 1 model\n",
      "31_CatBoost_BoostOnErrors accuracy 0.940299 trained in 82.64 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble accuracy 0.940744 trained in 66.11 seconds\n",
      "* Step stack will try to check up to 59 models\n",
      "31_CatBoost_Stacked accuracy 0.941218 trained in 71.6 seconds\n",
      "69_Xgboost_Stacked accuracy 0.941237 trained in 56.81 seconds\n",
      "98_LightGBM_Stacked accuracy 0.941284 trained in 60.54 seconds\n",
      "79_NeuralNetwork_Stacked accuracy 0.939645 trained in 177.9 seconds\n",
      "102_RandomForest_Stacked accuracy 0.941805 trained in 680.1 seconds\n",
      "105_ExtraTrees_SelectedFeatures_Stacked accuracy 0.941322 trained in 305.89 seconds\n",
      "31_CatBoost_GoldenFeatures_Stacked accuracy 0.941066 trained in 80.08 seconds\n",
      "94_Xgboost_Stacked accuracy 0.94135 trained in 56.57 seconds\n",
      "76_LightGBM_Stacked accuracy 0.941284 trained in 56.35 seconds\n",
      "7_Default_NeuralNetwork_Stacked accuracy 0.939863 trained in 158.92 seconds\n",
      "103_RandomForest_SelectedFeatures_Stacked accuracy 0.941767 trained in 618.92 seconds\n",
      "88_ExtraTrees_Stacked accuracy 0.941142 trained in 300.31 seconds\n",
      "65_CatBoost_Stacked accuracy 0.941237 trained in 82.59 seconds\n",
      "14_Xgboost_Stacked accuracy 0.941161 trained in 55.35 seconds\n",
      "100_LightGBM_Stacked accuracy 0.941104 trained in 67.15 seconds\n",
      "7_Default_NeuralNetwork_SelectedFeatures_Stacked accuracy 0.939559 trained in 184.5 seconds\n",
      "82_RandomForest_Stacked accuracy 0.941729 trained in 688.77 seconds\n",
      "51_ExtraTrees_Stacked accuracy 0.941161 trained in 324.23 seconds\n",
      "64_CatBoost_Stacked accuracy 0.941237 trained in 65.8 seconds\n",
      "14_Xgboost_SelectedFeatures_Stacked accuracy 0.941483 trained in 59.45 seconds\n",
      "4_Default_LightGBM_Stacked accuracy 0.941104 trained in 69.4 seconds\n",
      "58_NeuralNetwork_Stacked accuracy 0.939711 trained in 212.82 seconds\n",
      "84_RandomForest_SelectedFeatures_Stacked not trained. Stop training after the first fold. Time needed to train on the first fold 44.0 seconds. The time estimate for training on all folds is larger than total_time_limit.\n",
      "89_ExtraTrees_SelectedFeatures_Stacked accuracy 0.941227 trained in 363.53 seconds\n",
      "* Step ensemble_stacked will try to check up to 1 model\n",
      "Ensemble_Stacked accuracy 0.941815 trained in 93.93 seconds\n",
      "AutoML fit time: 14692.06 seconds\n",
      "AutoML best model: Ensemble_Stacked\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'train.csv',\n",
    "    index_col=0\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['Depression']),\n",
    "    df['Depression'],\n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "automl = AutoML(\n",
    "    eval_metric='accuracy',\n",
    "    mode='Compete',\n",
    "    total_time_limit=60*60*4\n",
    ")\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
